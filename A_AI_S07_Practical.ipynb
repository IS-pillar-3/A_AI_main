{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0q1MX0J_3jF"
      },
      "source": [
        "#**Applied AI for Health Research**\n",
        "\n",
        "#Practical 7: Interpretability\n",
        "\n",
        "Tutorial by Cher Bass and Emma Robinson. Edited by Mariana da Silva."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we will go through several popular visualization techniques that help interpret deep learning networks.\n",
        "\n",
        "We will cover:\n",
        "1. Filter visualization\n",
        "2. Feature/ activation visualisation with PyTorch hooks\n",
        "3. CNN Layer Visualisation\n",
        "4. Gradient visualisation with Guided backpropagation\n",
        "5. Gradient Class Activation Maps (Grad-CAM)"
      ],
      "metadata": {
        "id": "E81MIzpYZ-xs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing\n",
        "\n",
        "The first thing we need to do is import a package called `visualizations`. "
      ],
      "metadata": {
        "id": "-WXXIN71BfNF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZCm5dknBIg3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "516b6565-a0ec-41f9-ad90-4a0f8e0721b9"
      },
      "source": [
        "!wget -nv https://github.com/IS-pillar-3/datasets/raw/main/visualizations.zip\n",
        "\n",
        "from zipfile import ZipFile \n",
        "with ZipFile(\"visualizations.zip\", \"r\") as zObject:\n",
        "  zObject.extractall(path=\".\")                                   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-31 00:25:49 URL:https://raw.githubusercontent.com/IS-pillar-3/datasets/main/visualizations.zip [560571/560571] -> \"visualizations.zip.1\" [1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4cbbYYZC2pZ"
      },
      "source": [
        "Now import the modules you need from torch and the visualization package by running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN1VScAW_3jM"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "\n",
        "import numpy as np \n",
        "import visualizations\n",
        "from visualizations.src.misc_functions import *\n",
        "from visualizations.src.guided_backprop import GuidedBackprop\n",
        "from visualizations.src.gradcam import GradCam\n",
        "from visualizations.src.deep_dream import DeepDream\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JODrRIKp_3jN"
      },
      "source": [
        "## Exercise 1: Weights Visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the first things you can visualize in your network is your networks weight tensors. These weights kernels reflect the learnt convolutional kernesl which are optimised during training. They can be visualized by calling the weight data inside your network.\n",
        "\n",
        "Let's first load and print a pretrained network using pytorch.models:"
      ],
      "metadata": {
        "id": "MWJBgtpkWIXu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "v8WRwFDW_3jN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c96546ec-fefb-4ec2-f84c-9e149ad96b66"
      },
      "source": [
        "# first load pretrained alxenet model\n",
        "alexnet = models.alexnet(weights='DEFAULT')\n",
        "print(alexnet)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AlexNet(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): ReLU(inplace=True)\n",
            "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Dropout(p=0.5, inplace=False)\n",
            "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5dLZB6S_3jO"
      },
      "source": [
        "We can see from the print output (above) that the network is made up of two sequential containers: 1) `features` (the convolutional layers) and 2) `classifier` (the linear layers).\n",
        "\n",
        "To visualize the convolutional weights for some layer of a specific seqential container (e.g. `container`) of a network (e.g. `net`) we must use the following notation:\n",
        "\n",
        "```python \n",
        "# to return layer\n",
        "layer=net.container[layer_num]\n",
        " # to return weights tensor of a layer\n",
        "weight_tensor = net.container[layer_num].weight.data\n",
        "```\n",
        "\n",
        "**Note that layer id `layer_num` should correspond to a *convolutional* layer (e.g. 0, 3, 6, 10) otherwise there are no weights to be visualized.**\n",
        "\n",
        "We will now define a few functions to help with plotting:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o31HQ8fY_3jP"
      },
      "source": [
        "def plot_filters_single_channel(t,channels_to_plot,kernels_to_plot):\n",
        "    \n",
        "\n",
        "    print('printing {} kernels for each of the first {} filters '.format(kernels_to_plot,channels_to_plot))\n",
        "    #total kernels depth * number of kernels\n",
        "    nplots = channels_to_plot*kernels_to_plot\n",
        "    ncols = 12\n",
        "    \n",
        "    nrows = 1 + nplots//ncols\n",
        "    #convert tensor to numpy image\n",
        "    npimg = np.array(t.numpy(), np.float32)\n",
        "    \n",
        "    count = 0\n",
        "    fig = plt.figure(figsize=(ncols, nrows))\n",
        "    \n",
        "    #looping through all the kernels in each channel\n",
        "    for i in range(kernels_to_plot):\n",
        "        for j in range(channels_to_plot):\n",
        "            count += 1\n",
        "            ax1 = fig.add_subplot(nrows, ncols, count)\n",
        "            npimg = np.array(t[i, j].numpy(), np.float32)\n",
        "            npimg = (npimg - np.mean(npimg)) / np.std(npimg)\n",
        "            npimg = np.minimum(1, np.maximum(0, (npimg + 0.5)))\n",
        "            ax1.imshow(npimg)\n",
        "            ax1.set_title(str(i) + ',' + str(j))\n",
        "            ax1.axis('off')\n",
        "            ax1.set_xticklabels([])\n",
        "            ax1.set_yticklabels([])\n",
        "   \n",
        "    plt.tight_layout()\n",
        "    plt.show()    \n",
        "\n",
        "    \n",
        "def plot_filters_multi_channel(t,kernels_to_plot=60):\n",
        "    \n",
        "    print('printing the first {} 3D filters in RGB '.format(kernels_to_plot))\n",
        "    #get the number of kernals\n",
        "    num_kernels = t.shape[0]    \n",
        "    \n",
        "    #define number of columns for subplots\n",
        "    num_cols = 12\n",
        "    #rows = num of kernels\n",
        "    num_rows = kernels_to_plot // num_cols+1\n",
        "    \n",
        "    #set the figure size\n",
        "    fig = plt.figure(figsize=(num_cols,num_rows))\n",
        "    \n",
        "    #looping through all the kernels\n",
        "    for i in range(kernels_to_plot):\n",
        "        ax1 = fig.add_subplot(num_rows,num_cols,i+1)\n",
        "        \n",
        "        #for each kernel, we convert the tensor to numpy \n",
        "        npimg = np.array(t[i].numpy(), np.float32)\n",
        "        #standardize the numpy image\n",
        "        npimg = (npimg - np.mean(npimg)) / np.std(npimg)\n",
        "        npimg = np.minimum(1, np.maximum(0, (npimg + 0.5)))\n",
        "        npimg = npimg.transpose((1, 2, 0))\n",
        "        ax1.imshow(npimg)\n",
        "        ax1.axis('off')\n",
        "        ax1.set_title(str(i))\n",
        "        ax1.set_xticklabels([])\n",
        "        ax1.set_yticklabels([])\n",
        "        \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6KLRa1x_3jR"
      },
      "source": [
        "We next define our plot weights function, which first extracts the weights of a convolutional filter, and then passes into an appropriate image plotting function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ex 1.1 - Plot the weights of AlexNet\n",
        "\n",
        "Using the generic example given above,  edit the function `plot_weights` function to return the weights tensor for a given convolutional layer:\n",
        "\n",
        "**Task 1.1.1.** Select the convolutional `layer` from alexnets `features` container corresponding to `layer_num`.\n",
        "\n",
        "**Task 1.1.2.** Return the `weight_tensor` for this layer.\n",
        "\n",
        "**Task 1.1.3.** Run this function to return features from  `layer_num = 0`."
      ],
      "metadata": {
        "id": "2Pd-viQnO21k"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QixBmo6u_3jR"
      },
      "source": [
        "def plot_weights(container, layer_num,num_filter=10,num_kernels=30):\n",
        "    '''\n",
        "    plot_weights: a function to plot weights tensors from a network\n",
        "                  this will print in RGB if filter depth is 3 (i.e. first layer)\n",
        "                  else will print each kernel separately\n",
        "    inputs:\n",
        "          container: a sequnetial container for which the convolutional layers of the network are defined\n",
        "          layer_num: choice of layer to visualise\n",
        "\n",
        "    '''\n",
        "    #STUDENTS CODE - REPLACE NONES in FUNCTION BELOW\n",
        "    # 1.1.1. use variable layer_num to select a specific \n",
        "    layer = None\n",
        "  \n",
        "    #checking whether the layer is convolution layer or not \n",
        "    if isinstance(layer, nn.Conv2d):\n",
        "        # 1.1.2. return the weights tensor for this 'layer`\n",
        "        weight_tensor = None\n",
        "        print('This layers learns {} filters each with {} kernels'.format(weight_tensor.shape[0],weight_tensor.shape[1]))\n",
        "        # if the weights tensor has 3 channels the it will plot filters in RGB\n",
        "        if weight_tensor.shape[1]==3:\n",
        "             # labelling each by filer id            \n",
        "             plot_filters_multi_channel(weight_tensor,num_filter)    \n",
        "        else:\n",
        "            # else it will print each filter kernel separately\n",
        "            # labelling each by filer.kernel (the filter id and the kernel id)\n",
        "            plot_filters_single_channel(weight_tensor,num_filter,num_kernels)\n",
        "        \n",
        "    else:\n",
        "        print(\"Can only visualize layers which are convolutional\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.1.3. Call the function to visualise layer 0 (first conv layer)\n",
        "plot_weights(None)"
      ],
      "metadata": {
        "id": "cHUZ2NPZRYnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TZqI8VA_3jR"
      },
      "source": [
        "###Ex 1.2 - Answer the following questions\n",
        "\n",
        "**Task 1.2.1.** What do the weights of your network represent? (Answer in the cell below)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DEHBFHz5R_31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1.2.2.** Layer 3 (the second convolution) has a weights matrix of size `[192, 64, 5, 5]`. What does each dimension represent? How many filters does it learn, and how many kernels does each filter have?\n"
      ],
      "metadata": {
        "id": "mEfB3ufqSuA1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rsMxCOrRTuaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1.2.3** Try plotting the weights for different convolutional layers (changing code cell below). "
      ],
      "metadata": {
        "id": "fr2_2WMJTtKt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "UY0XEN3Q_3jS"
      },
      "source": [
        "# Task 1.2.3 \n",
        "plot_weights(None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "   - Try changing the numbers of filters and/or kernels printed per layer\n",
        "   - What if anything can you interpret from each of these layer?\n",
        "   - Do you think this approach is useful for intrepretation of deep networks? Why do you think that?\n",
        "   "
      ],
      "metadata": {
        "id": "r8a20WbHUvx0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnOnQpB9_3jY"
      },
      "source": [
        "## Exercise 2: Saliency by Occlusion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saliency mapping techniques seek to generate heatmaps which highlight which parts of an image are important for activation.\n",
        "\n",
        "The most simple approach to saliency mapping is to perform occlusion. This works by greying out (setting to 0.5) patches of pixels for an image to see what impact this has on classification. "
      ],
      "metadata": {
        "id": "ALQFFsLgWgiX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use MNIST for this example. Loading train and validation DataLoaders, and generating a similar basic convolutional network to which we used in Session 4:"
      ],
      "metadata": {
        "id": "vE1p64GFM9Y1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wu-GLnPo_3jT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 845,
          "referenced_widgets": [
            "d91a76b8c0b84fb5b4cc000f44682191",
            "662d270e02cf44c989ac84c92de44963",
            "db598f22bc644d3a8e681bf82344bcd3",
            "cc71deb67306448ebe566dc47fd2990e",
            "d841675d6c02402a842081db0542f939",
            "759551c1259441a4b82f0f3f7297fab2",
            "6b5e7fc7ed6a4953935c01fc3c71bde7",
            "aed848a1dccf4b649626b8e45c4286dd",
            "f9bcae726241462691f0ba851d600b5c",
            "4efdc01e76154706919656d489a23862",
            "0eb08a66af78499cb37e8154774f7d11",
            "69672d76d18f4dc3ac04c380fb8a8d3e",
            "ac74fec74f854b5d9ec59ac22ee4bca6",
            "2830b2cab3a948e2b70174d665e9a996",
            "5af4c67764984e469720e13a1e2fe2cf",
            "be00d5569f0443069f872a140a707e78",
            "21889f09014940ca8b2c243c88b71afd",
            "d8044b341e3e4ffc82de75efe4de7ef2",
            "db1e78d0d99043f0926e058dae8696f5",
            "b117cdf74cfe465c92957f02cef2a4e9",
            "d390e16f18654facbb58b65fcd04b99e",
            "3d62f1af0093444c877d12c076e90897",
            "19bc7efc389b440d82236a3b2c685a23",
            "3a1d0dee37cd4971ba9debfcb32ed4e6",
            "20a481ba48ca42ab84a9807afa093904",
            "7951b2cb4da947ea9893530bcacfa87a",
            "8cee60c922ec44428c54e713752c5472",
            "a3704383ac194c749d0c0926a9f52e98",
            "ee0be31000744371955bd388b14e03a6",
            "e517dd6f89d24072bf4a2ea619fa6af1",
            "84f8f4ddbaf14a68919204d19be8996c",
            "2b3b38e7cdac4b5b801768a1f708b2c1",
            "8391cedbd297496faa3c547edc6c2f3c",
            "c0e89a9e3fe1418ea48e3855c6af3a61",
            "94be68cbffd84006a671cee009979411",
            "73fa4b46c92a41c2ae040feb450f3491",
            "620e9d9672dc4aa28d3cee89d9c958b0",
            "d9d9d02513c34b6495ac9bbc21d89492",
            "7fd6a9c8ab0b4feea9f001d2b164a8c6",
            "85691f69260d48f5b6c3ac556fdf8d47",
            "caf03de682c149f8a2c07dbc1f735d38",
            "bd43a025011f498d8af95725370bfe25",
            "f86fe714210d4778b0fcb48f82807164",
            "66c7c223b60741bf81f8a90c0aac69c0",
            "b83ccb6b4fa6433b9d43d65d9f009d47",
            "adb98845ad5d401da4ed97d11a3ed024",
            "34c9e45ae5304908abcb37aece9b7fa1",
            "a44ca3abbbd54e82bf29e2b7ecd25f70",
            "81a8d5aaabe84eaf9f51ccf6fcdf6a53",
            "db72c12adfac456fbafb69487f204efb",
            "9a748c8b44b042d0afdbfc14f7331fb4",
            "67ace1065a444c279fd664783a658e9c",
            "3bafb280ea6842579a08e89d92bc4f19",
            "01ab1a85d45f4d3a95009252c26f26f8",
            "7b864e5d3f824d2bb60bdff68db28dee",
            "2c964caea2db407585ad7140d75ebce9",
            "1a64d3fd1ebd47a988542a212ea48ecf",
            "d2268cff061d4d8996e8c871de895f41",
            "47ec254b5d7c4a06b7ec26d04f2360cd",
            "9d97c2de360642f2a5df008955cea1ef",
            "16dd303b130e4c32bdaa76c069e512d4",
            "3d8a2bd619d442c3af2bbecd7205c840",
            "93ff8bef7531402b88815b4d91aa52b7",
            "8cff69de368e403d8443609b145a0d0a",
            "162ea539b1fe4ced8d49ddcc66909d31",
            "465fd48946ea459782a2c5f7184cf579",
            "4368607f84004da1b931e434face2b4d",
            "d790b8a4a8c245899827266792e7000a",
            "91ca7b23bb0840bf9f862f405775d9fe",
            "e7cff61c065c4cb49d019d6e4ea06eba",
            "bda85b5950d94a288c87f5ca732f889f",
            "c299ab3e3c044c4a964f76604698aeaf",
            "4dbaede352934724b264ea45cd77a826",
            "5626276c5256463d93a2b4ae9a4d3e90",
            "5009224ed3af49bd89405543d3e29955",
            "51903fa2215d410c9b17f907fc951041",
            "d92fcc007d1b4bbc915e0e36c67484fb",
            "6f8ddc229f3d4c46b516bbc9d17363cb",
            "11e2943f3f60489fbb00e3eb8e5a5842",
            "963bbad5e4984720ba44ed4f8ab98d4a",
            "0396f20916544eccab820e3eb9421c67",
            "52c702e060bf4034862491f07db76edb",
            "dcfb4e3c3d604fdb83d5b49b9aa73c34",
            "db52291eb1bb48e79f73bfbccb2a1334",
            "1aa0d5c5a4314d66bcff6172f2ca12d2",
            "5a14ed9668f44f9c9d3842566a03203b",
            "0b38e0ba4fb645d98994d0c6338a422c",
            "d729f1e08cd1445d9131f504ac9cf05d"
          ]
        },
        "outputId": "157629dd-fca4-4689-8772-b7e4361eb292"
      },
      "source": [
        "mnist_train_dataset = datasets.MNIST(root = 'mnist_data/train', download= True, train = True, transform = transforms.ToTensor())\n",
        "mnist_test_dataset = datasets.MNIST(root = 'mnist_data/test', download= True, train = False, transform = transforms.ToTensor())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "       mnist_train_dataset, batch_size= 8, shuffle = True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "       mnist_test_dataset, batch_size = 8, shuffle = True)\n",
        "\n",
        "classes = ('0', '1', '2', '3',\n",
        "          '4', '5', '6', '7', '8', '9')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to mnist_data/train/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/9912422 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d91a76b8c0b84fb5b4cc000f44682191"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist_data/train/MNIST/raw/train-images-idx3-ubyte.gz to mnist_data/train/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to mnist_data/train/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/28881 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "69672d76d18f4dc3ac04c380fb8a8d3e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist_data/train/MNIST/raw/train-labels-idx1-ubyte.gz to mnist_data/train/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to mnist_data/train/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1648877 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "19bc7efc389b440d82236a3b2c685a23"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist_data/train/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist_data/train/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to mnist_data/train/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/4542 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0e89a9e3fe1418ea48e3855c6af3a61"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist_data/train/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist_data/train/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to mnist_data/test/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/9912422 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b83ccb6b4fa6433b9d43d65d9f009d47"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist_data/test/MNIST/raw/train-images-idx3-ubyte.gz to mnist_data/test/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to mnist_data/test/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/28881 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c964caea2db407585ad7140d75ebce9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist_data/test/MNIST/raw/train-labels-idx1-ubyte.gz to mnist_data/test/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to mnist_data/test/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1648877 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4368607f84004da1b931e434face2b4d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist_data/test/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist_data/test/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to mnist_data/test/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/4542 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f8ddc229f3d4c46b516bbc9d17363cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist_data/test/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist_data/test/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hideCode": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e1utuc5_3jT",
        "outputId": "d7cf506c-c6e6-46ce-8789-7b9e8c266f23"
      },
      "source": [
        "class MNIST_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Model, self).__init__()\n",
        "    \n",
        "        self.conv1=nn.Conv2d(1, 10, 3)\n",
        "        self.maxpool1=nn.MaxPool2d(2)\n",
        "        self.dropout1=nn.Dropout2d()\n",
        "        \n",
        "        self.conv2=nn.Conv2d(10, 20, 3)\n",
        "        self.maxpool2=nn.MaxPool2d(2)\n",
        "        self.dropout2=nn.Dropout2d()\n",
        "        \n",
        "        self.lin_blocks = nn.Sequential(\n",
        "            nn.Linear(500, 50),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50, 10),\n",
        "        )\n",
        "        \n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.maxpool2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        \n",
        "        x = x.view(x.size(0),-1)\n",
        "        x = self.lin_blocks(x)\n",
        "\n",
        "        return F.log_softmax(x,dim=1)\n",
        "\n",
        "\n",
        "net = MNIST_Model() \n",
        "print(net)\n",
        "net = net.to(device)\n",
        "\n",
        "loss_fun = nn.CrossEntropyLoss()\n",
        "loss_fun = loss_fun.to(device)\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "epochs = 1\n",
        "for epoch in range(epochs): \n",
        "\n",
        "    # enumerate can be used to output iteration index i, as well as the data \n",
        "    for i, (data, labels) in enumerate(train_loader, 0):\n",
        "        \n",
        "        # load data and labels to device\n",
        "        data = data.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # clear the gradient\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #feed the input and acquire the output from network\n",
        "        outputs = net(data)\n",
        "\n",
        "        # calculating the predicted and the expected loss\n",
        "        loss = loss_fun(outputs, labels)\n",
        "\n",
        "        #compute the gradient\n",
        "        loss.backward()\n",
        "\n",
        "        #update the parameters\n",
        "        optimizer.step()\n",
        "        # \n",
        "\n",
        "        # print statistics\n",
        "        ce_loss = loss.item()\n",
        "        if i % 500 == 0:\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                 (epoch + 1, i + 1, ce_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST_Model(\n",
            "  (conv1): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (dropout1): Dropout2d(p=0.5, inplace=False)\n",
            "  (conv2): Conv2d(10, 20, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (dropout2): Dropout2d(p=0.5, inplace=False)\n",
            "  (lin_blocks): Sequential(\n",
            "    (0): Linear(in_features=500, out_features=50, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "[1,     1] loss: 2.314\n",
            "[1,   501] loss: 2.180\n",
            "[1,  1001] loss: 0.663\n",
            "[1,  1501] loss: 0.600\n",
            "[1,  2001] loss: 0.446\n",
            "[1,  2501] loss: 0.856\n",
            "[1,  3001] loss: 0.442\n",
            "[1,  3501] loss: 0.979\n",
            "[1,  4001] loss: 1.086\n",
            "[1,  4501] loss: 0.289\n",
            "[1,  5001] loss: 0.193\n",
            "[1,  5501] loss: 0.292\n",
            "[1,  6001] loss: 0.148\n",
            "[1,  6501] loss: 0.343\n",
            "[1,  7001] loss: 0.184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOtUHFHA_3jY"
      },
      "source": [
        "def plot_MNIST(images,labels):\n",
        "    rows = 2\n",
        "    columns = 4\n",
        "    classes = ('0', '1', '2', '3',\n",
        "          '4', '5', '6', '7', '8', '9')\n",
        "    # plot y_score - true label (t) vs predicted label (p)\n",
        "    fig2 = plt.figure()\n",
        "    for i in range(8):\n",
        "        fig2.add_subplot(rows, columns, i+1)\n",
        "        plt.title('t: ' + classes[labels[i].cpu()])\n",
        "        img = images[i] / 2 + 0.5     # this is to unnormalize the image\n",
        "        img = torchvision.transforms.ToPILImage()(img.cpu())\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "im_batch, lab_batch=next(iter(test_loader)) # view one batch\n",
        "im_batch = im_batch.to(device)\n",
        "plot_MNIST(im_batch,lab_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ6XsPeJ_3jZ"
      },
      "source": [
        "The first thing we need to do is run inference on the images without occlusion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2znVVgO_3jZ"
      },
      "source": [
        "# running inference on the images without occlusion\n",
        "\n",
        "# pretrained model\n",
        "outputs = net(im_batch)\n",
        "\n",
        "# passing the outputs through softmax to interpret them as probability\n",
        "outputs = nn.functional.softmax(outputs, dim = 1)\n",
        "\n",
        "# assigning the predicted label from the maximum softmax output\n",
        "prob_no_occ, pred = torch.max(outputs.data, 1)\n",
        "\n",
        "# get the first item\n",
        "prob_no_occ = prob_no_occ[0].item()\n",
        "\n",
        "print('Predictions = ', pred,'max prob',prob_no_occ)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7PNKBCe_3jZ"
      },
      "source": [
        "Now, let's look at the effect of zeroing (or greying out) blocks of pixels in our image.\n",
        "\n",
        "Similar to the patch based selection that we saw in lecture 4, we are constrained to select only patches which fit in our image (which means that number of points on which we can start each patch is constrained by the width and height of our patch). This means that the output size will be smaller than the original image size by a factor of: (h-p)/s; where, h is the height (or width), p is the patch size and s is the chosen stride.\n",
        "\n",
        "Let's start create an image with an occluded patch in the centre. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Ex 2.1 - Create a function which occludes a patch from an image\n",
        "\n",
        "**Task 2.1.1.** set all pixels of patch (centred at height: `heigh_centre` and width: `width_centre`, and of size: `patch_size`) to 0.5\n",
        "  - **note** if the patch_size exceeds the dimensions on any axis - just set the available space to zero (otherwise the results of Ex 3.2 will have shape less than the original image). This is equivalent to padding the operation."
      ],
      "metadata": {
        "id": "OORkHGyqW6kS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CXF-qxT_3jZ"
      },
      "source": [
        "# Write a function which occludes a patch from an image\n",
        "from math import ceil\n",
        "\n",
        "def occlude_image(image, height_centre,width_centre,patch_size):\n",
        "    ''' \n",
        "    Creates a copy of the image and occludes a patch \n",
        "    input:\n",
        "    image (Pytorch tensor): image to be occluded\n",
        "    height_centre=centre of patch on height dimension\n",
        "    width_centre= centre of patch on width dimension\n",
        "    patch_size: size of patch\n",
        "    \n",
        "    output: \n",
        "    occluded image\n",
        "    '''\n",
        "    occluded_image = image.detach().clone()\n",
        "    \n",
        "    # estimate the start and end dimensions of the patch\n",
        "    # allow for patches to be centred at all locations irrespective of whether patch fits fully or not i.e. pad\n",
        "    height_start=int(height_centre-ceil(patch_size/2))\n",
        "    width_start=int(width_centre-ceil(patch_size/2))\n",
        "\n",
        "    if height_start <0:\n",
        "      height_start=0\n",
        "    if width_start < 0:\n",
        "      width_start=0\n",
        "\n",
        "    height_end=height_start+patch_size\n",
        "    width_end=width_start+patch_size\n",
        "\n",
        "    if height_end >= occluded_image.shape[1]:\n",
        "      height_end=occluded_image.shape[1]-1\n",
        "    if width_end >= occluded_image.shape[2]:\n",
        "      width_end=occluded_image.shape[2]-1\n",
        "    \n",
        "    # Task 2.1.1 - change pixel values in patch to 0.5\n",
        "    None\n",
        "\n",
        "    return occluded_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27MO5Kk5uoyu"
      },
      "source": [
        "**Task 2.1.2.** Pass the full image through the network and estimate the probability of the network predicting the correct label (return softmax output corresponding to the correct label)\n",
        "\n",
        "**Task 2.1.3** Now occlude the image and make another forward pass; how does occlusion impact the prediction accuracy?\n",
        "\n",
        "**Task 2.1.4** Return label probabilities for this prediction using softmax\n",
        "\n",
        "**Note** \n",
        "- forward pass expects an input tensor of shape $B\\times C\\times H\\times W$ where $B$ represents the batch size, $C$ the number of channels and $H$ and $W$ the height and width (**hint** perhaps use [unsqueeze](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html))\n",
        "-you will need to use `nn.functional.softmax()` on the output from your network. This will return a probability for each label (summing to one over all classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lV8EWK25upB7"
      },
      "source": [
        "true_label=lab_batch[0].numpy()\n",
        "patch_size=20\n",
        "height_center=8\n",
        "width_center=8\n",
        "\n",
        "# Task 2.1.2 pass the image through a forward pass  and estimate the probability of prediction of the True label (2 lines)\n",
        "# remember the network will be expect an input of shape BxCxHxW where B is batch size and C is number of channels\n",
        "# use nn.functional.softmax to return a probability - slice the output for the true label\n",
        "output_full=None\n",
        "full_prob=None\n",
        "\n",
        "# Task 2.1.3 pass the image through the occlusion function and make forward pass (2 lines)\n",
        "occluded_image=None\n",
        "output_occluded=None\n",
        "# Task 2.1.4 return label probabilities for this prediction using softmax - slice the output for the true label\n",
        "occluded_prob=None\n",
        "print('True label: {} original probability: {} occluded probability: {}'.format(true_label, full_prob,occluded_prob)) \n",
        "\n",
        "# plot\n",
        "fig2 = plt.figure(figsize=(15,5))\n",
        "fig2.add_subplot(1, 2, 1)\n",
        "#displaying the image using seaborn heatmap and also setting the maximum value of gradient to probability\n",
        "img = torchvision.transforms.ToPILImage()(occluded_image[0].cpu())\n",
        "plt.imshow(img)\n",
        "fig2.add_subplot(1, 2, 2)\n",
        "img = torchvision.transforms.ToPILImage()(im_batch[0].cpu())\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JerU1b39_3ja"
      },
      "source": [
        "You will see that, not unexpectedly grey out the part of the image which contains the number will dramatically reduce the probability of selecting the correct label.\n",
        "\n",
        "Thus, occlusion presents us a network indepedent way of generating a map of regional saliencies. All we need to do is **occlude patches centered at all feasible locations** in the image; then we can generate a heatmap of label probabilities estimated for each of these patches/locations. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Ex 2.2 - Create occlusion based saliency map\n",
        "\n",
        "Create a function to iterate across an image to generate patch predictions at each feasible location.\n",
        "\n"
      ],
      "metadata": {
        "id": "EheHW6iFXZn9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biwnG-Fd_3jb"
      },
      "source": [
        "#STUDENTS CODE -  Write a custom function to conduct occlusion experiments\n",
        "\n",
        "def occlusion(model, image, label, occ_size = 50,  occ_pixel = 0.5):\n",
        "    '''\n",
        "       function to iterate occlusion mapping for all locations in an image\n",
        "       in order to return a heatmap\n",
        "       input:\n",
        "            model - the trained model\n",
        "            image - the input image to occlude\n",
        "            label - the true image label\n",
        "            occ_size - the patch size\n",
        "            occ_pixel - the value to fill the patch with\n",
        "      \n",
        "      output:\n",
        "           heatmap (torch array) - same shape as image,\n",
        "                                 - value at each grid location =  label probabilities corresponding to result of occluding a patch centred at the corresponding image location\n",
        "\n",
        "    '''\n",
        "    #create a zero image with shape equal to the image\n",
        "    heatmap = torch.zeros((image.shape[1],image.shape[2]))\n",
        "    \n",
        "    # Ex 2.2 - complete occlusion function\n",
        "\n",
        "\n",
        "    return heatmap\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD1WnoWWBgsN"
      },
      "source": [
        "**To do** now run for the test image - you may need to change the patch size to get an interpretable result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoJKn21J_3jb"
      },
      "source": [
        "print(pred[0],pred[0].type)\n",
        "image=im_batch[0]\n",
        "heatmap = occlusion(net, image, lab_batch[0].item(),20, 1)\n",
        "print(torch.max(heatmap))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEt7IAnY_3jb"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "fig2 = plt.figure(figsize=(15,5))\n",
        "fig2.add_subplot(1, 2, 1)\n",
        "#displaying the image using seaborn heatmap and also setting the maximum value of gradient to probability\n",
        "imgplot = sns.heatmap(heatmap.detach().numpy(), xticklabels=False, yticklabels=False, vmax=prob_no_occ)\n",
        "figure = imgplot.get_figure()    \n",
        "fig2.add_subplot(1, 2, 2)\n",
        "img = torchvision.transforms.ToPILImage()(image[0].cpu())\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuV8USNP_3jb"
      },
      "source": [
        "## Exercise 3: Gradient visualization with Guided backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So occlusion can be an effective method for returning saliency maps but for large images it can be computationally costly compute. Therefore several alternative methods have been proposed which instead seek to backpropagate gradients from output neurons to the locations in the image which contribute most significantly to the prediction. \n",
        "\n",
        "The first method we will look at will be Guided Backpropation, which is an extension of DeconvNet to generalise to all convolutional networks (which downsample through strided convolutions). The Deconvnet visualises activations pertaining to specific classes, by pushing gradient activations backwards from through the network through a series of unpooling, relus (to zero negative gradients) and inverse convolutions (implemented by applying transposed filters)\n",
        "\n",
        "For [Guided Backpropagation](https://arxiv.org/pdf/1412.6806.pdf), the outputs of relu operations at each layer are pushed backwards, guided by masks which summarise the forwards and backwards relu. Thus these zero contributions from any locations with negative activations or negative gradients to return locations in the image that contribute positively to the prediction.\n",
        "\n",
        "<figure align=\"center\">\n",
        "<img src=\"https://github.com/IS-pillar-3/miscellaneous/blob/main/guided_backprop.png?raw=True\" alt=\"Drawing\" width=\"800px;\"/>\n",
        "</figure>\n",
        "\n",
        "You can find the implementation of guided backpropagation in the `vizualizations` package folder that you uploaded to run this notebook `/visualizations/src/guided_backprop.py`."
      ],
      "metadata": {
        "id": "TVCrtHk9Xr18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ex 3.1 - first comment each line in `GuidedBackprop.update_relus(self)`\n",
        "\n",
        "To cement your understanding of the Guided Backpropagation method, comment each line of the function `GuidedBackprop.update_relus(self)`. This uses hooks to extract the activations and gradients of each relu layer. These are used to mask the relu gradients and push them back towards the input pixel space.\n"
      ],
      "metadata": {
        "id": "H5knaQGgXytu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNLisyMh_3ji"
      },
      "source": [
        "\n",
        "\n",
        "**Now, run guided backpropagation for a given input image.**\n",
        "\n",
        "The `visualizations` package is specifically designed to visualise the layers of PyTorch pretrained networks. Thus the function `misc_functions.get_example_params()` function loads a pretrained AlexNet and returns a preprocessed image `prep_img`,as well as it's target label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9L0JqUi_3ji"
      },
      "source": [
        "target_example = 0  \n",
        "(original_image, prep_img, target_class, file_name_to_export, pretrained_model) =\\\n",
        "    get_example_params(target_example)\n",
        "\n",
        "im_array=prep_img[0].detach().numpy()\n",
        "print(prep_img.shape,np.moveaxis(im_array, 0, -1).shape)\n",
        "plt.imshow(np.moveaxis(im_array, 0, -1))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbbMgCik_3jj"
      },
      "source": [
        "An instance of the guidedbackprop class can be created using:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwAVsIHF_3jj"
      },
      "source": [
        "GBP = GuidedBackprop(pretrained_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdxfNmac_3jj"
      },
      "source": [
        "Gradients specific to a particular input image, and class may be generated using the method `GP.generate_gradients()`, which has the following steps:\n",
        " 1. go through a forward pass with the image input, and generate an output\n",
        " 2. backprop through the output\n",
        " 3. get the gradients from the backprop\n",
        " \n",
        "**Run:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ym-UsdA_3jj"
      },
      "source": [
        "# Get gradients\n",
        "guided_grads = GBP.generate_gradients(prep_img, target_class)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTg8UTE4_3jj"
      },
      "source": [
        "We can then plot the outputs. Note for RGB matplot lib expects channels on the 3rd axis so we need to reshape:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtT5jETU_3jj"
      },
      "source": [
        "# Convert to grayscale\n",
        "grayscale_guided_grads = convert_to_grayscale(guided_grads)\n",
        "# Positive and negative saliency maps\n",
        "pos_sal, neg_sal = get_positive_negative_saliency(guided_grads)\n",
        "\n",
        "### return normalise gradients and plot\n",
        "guided_grads=normalise_gradient(guided_grads)\n",
        "grayscale_guided_grads=normalise_gradient(grayscale_guided_grads)\n",
        "pos_sal=normalise_gradient(pos_sal)\n",
        "neg_sal=normalise_gradient(neg_sal)\n",
        "\n",
        "fig2 = plt.figure(figsize=(20,10))\n",
        "fig2.add_subplot(1, 4,1)\n",
        "plt.imshow(np.moveaxis(guided_grads, 0, -1))\n",
        "fig2.add_subplot(1, 4,2)\n",
        "plt.imshow(grayscale_guided_grads[0],cmap='gray')\n",
        "fig2.add_subplot(1, 4,3)\n",
        "plt.imshow(np.moveaxis(pos_sal, 0, -1))\n",
        "fig2.add_subplot(1, 4,4)\n",
        "plt.imshow(np.moveaxis(neg_sal, 0, -1))\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfE-hYms_3jl"
      },
      "source": [
        "### Ex 3.2 - Run guided backprop for different inputs\n",
        "You can do this by changing `target_example=` (there's 3 inputs available)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNTKkmE0_3jl"
      },
      "source": [
        "# change target example\n",
        "target_example = None\n",
        "\n",
        "(original_image, prep_img, target_class, file_name_to_export, pretrained_model) =\\\n",
        "    get_example_params(target_example)\n",
        "\n",
        "im_array=prep_img[0].detach().numpy()\n",
        "print(prep_img.shape,np.moveaxis(im_array, 0, -1).shape)\n",
        "plt.imshow(np.moveaxis(im_array, 0, -1))\n",
        "plt.show()\n",
        "\n",
        "# Get gradients \n",
        "guided_grads = GBP.generate_gradients(prep_img, target_class)\n",
        "\n",
        "# Convert to grayscale\n",
        "grayscale_guided_grads = convert_to_grayscale(guided_grads)\n",
        "# Positive and negative saliency maps\n",
        "pos_sal, neg_sal = get_positive_negative_saliency(guided_grads)\n",
        "\n",
        "### return normalise gradients and plot\n",
        "guided_grads=normalise_gradient(guided_grads)\n",
        "grayscale_guided_grads=normalise_gradient(grayscale_guided_grads)\n",
        "pos_sal=normalise_gradient(pos_sal)\n",
        "neg_sal=normalise_gradient(neg_sal)\n",
        "\n",
        "fig2 = plt.figure(figsize=(20,10))\n",
        "fig2.add_subplot(1, 4,1)\n",
        "plt.imshow(np.moveaxis(guided_grads, 0, -1))\n",
        "fig2.add_subplot(1, 4,2)\n",
        "plt.imshow(grayscale_guided_grads[0],cmap='gray')\n",
        "fig2.add_subplot(1, 4,3)\n",
        "plt.imshow(np.moveaxis(pos_sal, 0, -1))\n",
        "fig2.add_subplot(1, 4,4)\n",
        "plt.imshow(np.moveaxis(neg_sal, 0, -1))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbinlavM_3jn"
      },
      "source": [
        "## Exercise 4: Gradient Class Activation Mapping (grad-CAM)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "[Grad-cam](https://arxiv.org/abs/1610.02391) allows extraction of occlusion-like saliency maps in a single pass, by creating a neuron importance score for a given target class. \n",
        "\n",
        "<figure align=\"center\">\n",
        "<img src=\"https://github.com/IS-pillar-3/miscellaneous/blob/main/gradcam.png?raw=True\" alt=\"Drawing\" width=\"800px;\"/>\n",
        "</figure>\n",
        "\n",
        "Specifically, it makes a forward pass, estimates the gradient of output activation ($A^k$), before the softmax and with respect to the target class ($y^c$), clamps this to one and then sets the gradient for all other classes to zero. It then estimates a **neuron importance score** by global average pooling over the spatial dimensions ($i,j$) of the channel:\n",
        "\n",
        "$$\\alpha_k^c=\\frac{1}{Z} \\sum_i \\sum_j \\dfrac{\\partial y^c}{\\partial A^k_{ij}}$$\n",
        "\n",
        "This summarises the importance of a particular featuremap or channel to the prediction.\n",
        "\n",
        "The final $\\mathbb{R}^{u \\times v}$ visualisation is then estimated from a weighted average over all activation map (where weights are given by the neuron importance scores). This is followed by a ReLU to clamp the visualisation to return only positive contributions. \n",
        "\n",
        "$$L^C_{Grad\\_cam}=RELU(\\sum_k \\alpha_k^c A^k)$$"
      ],
      "metadata": {
        "id": "6u5FXdc8YgK1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ex 4.1 - Apply the code\n",
        "\n",
        "Run grad cam on the same examples used for Guided Backprop. What key differences do you observe."
      ],
      "metadata": {
        "id": "UMkFtvVcYmsx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iU3w2h9_3jq"
      },
      "source": [
        "#Get params\n",
        "target_example = 1\n",
        "(original_image, prep_img, target_class, file_name_to_export, pretrained_model) =\\\n",
        "    get_example_params(target_example)\n",
        "\n",
        "# Grad cam\n",
        "grad_cam = None\n",
        "# Generate cam mask\n",
        "cam = None\n",
        "\n",
        "# Create Grayscale activation map\n",
        "heatmap, heatmap_on_image = apply_colormap_on_image(original_image, cam, 'hsv')\n",
        "\n",
        "# plot\n",
        "fig_cam = plt.figure(figsize=(20,10))\n",
        "fig_cam.add_subplot(1, 3,1)\n",
        "plt.imshow(original_image)\n",
        "fig_cam.add_subplot(1, 3,2)\n",
        "plt.imshow(cam)\n",
        "fig_cam.add_subplot(1, 3,3)\n",
        "plt.imshow(heatmap_on_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkPDLmpl_3jq"
      },
      "source": [
        "**Note** `target_example` is a picture of a dog and a cat but the `get_example_params` is hard coded to dog. If you would like to try different pictures the imagenet class label list can be found [here](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a)\n",
        "\n",
        "### Ex 4.2 - Change the class of the image to 'tabby cat'\n",
        "\n",
        "You might also want to chose other images, or vary the target layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUD3wH1M_3jq"
      },
      "source": [
        "# Ex 4.2 - run grad cam for but change class of image to tabby cat\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwFZupKGigZu"
      },
      "source": [
        "## Exercise 5: T-SNE\n",
        "\n",
        "Experiment with t-sne using the [scikit-learn implementation](xhttps://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). In its most basic form this can be done in one line\n",
        "\n",
        "```\n",
        ">>> import numpy as np\n",
        ">>> from sklearn.manifold import TSNE\n",
        ">>> X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
        ">>> X_embedded = TSNE(n_components=2, learning_rate='auto',\n",
        "...                   init='random', perplexity=3).fit_transform(X)\n",
        ">>> X_embedded.shape\n",
        "(4, 2)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "For this exercise we will again use the data from the first session \"prem_vs_termwrois.pkl\". This represents mean vales of three different types of cortical imaging data: cortical thickness, cortical folding and cortical myelination, all averaged within 100 regions of interest ROIS on the surface (300 features in total). There are 101 babies, 50 terms and 51 preterms. \n",
        "\n",
        "<figure align=\"center\">\n",
        "<img src=\"https://github.com/IS-pillar-3/miscellaneous/blob/main/cortical_rois.png?raw=True\" alt=\"Drawing\" width=\"900px;\"/>\n",
        "</figure>\n",
        "\n",
        "**To do**\n",
        "\n",
        "**Ex 5.1.** implement t-sne using scikit learn. Set `n_components=2`; fit the embedding for the dHCP data\n",
        "\n",
        "**Ex 5.2** experiment with changing the perplexity n to the range 5 to 50\n",
        "\n",
        "**Ex 5.3** experiment with changing the metric to other options available through [scipy.spatial.distance.pdista](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html) (to e.g. correlation)\n",
        "\n",
        "**Ex 5.4** In each instance, plot the embedding with the points color coded by label (pre-term vs full-term)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9q6eATzvijjD"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn import manifold\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import copy\n",
        "\n",
        "url      = \"https://raw.githubusercontent.com/IS-pillar-3/datasets/main/prem_vs_termwrois.csv\" \n",
        "download = requests.get(url).content\n",
        "df       = pd.read_csv(io.StringIO(download.decode(\"utf-8\")))\n",
        "\n",
        "# Read the data\n",
        "data = df.values[:,:-2]\n",
        "y = df.values[:,-1]\n",
        "\n",
        "# Ex 5.1\n",
        "\n",
        "# Ex 5.2 vary parameters (e.g. perplexity) and see effect on embeddding\n",
        "\n",
        "# Ex 5.3 vary metrics\n",
        "\n",
        "# Ex 5.4 plot result with different colours for each of the (premature and term baby labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipjPBODnDPl_"
      },
      "source": [
        "**Source references**\n",
        "\n",
        "1. [visualizing-convolution-neural-networks-using-pytorch](https://towardsdatascience.com/visualizing-convolution-neural-networks-using-pytorch-3dfa8443e74e)\n",
        "2. [DeepLearning-PadhAI](https://colab.research.google.com/github/Niranjankumar-c/DeepLearning-PadhAI/blob/master/DeepLearning_Materials/6_VisualizationCNN_Pytorch/CNNVisualisation.ipynb#scrollTo=uQI9jHcP6xfP)"
      ]
    }
  ]
}