{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Applied AI for Health Research\n",
        "\n",
        "#Practical 5: CNN Architectures\n",
        "\n",
        "Tutorial by Cher Bass and Emma Robinson. Edited by Mariana da Silva."
      ],
      "metadata": {
        "id": "wKX7a4vqUCZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by importing the modules and data that we need for the notebook. We start by training and testing on the MNIST dataset, which we have previously used in other Practicals."
      ],
      "metadata": {
        "id": "EvKfcp0GUvKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F # Contains some useful functions like activation \n",
        "                                # functions & convolution operations you can use\n",
        "\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "# This is used to transform the images to Tensor and normalize it\n",
        "transform = transforms.Compose(\n",
        "   [transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])])\n",
        "\n",
        "training = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                       download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(training, batch_size=32,\n",
        "                                         shuffle=True, num_workers=2)\n",
        "\n",
        "testing = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                      download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(testing, batch_size=32,\n",
        "                                        shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('0', '1', '2', '3',\n",
        "          '4', '5', '6', '7', '8', '9')"
      ],
      "metadata": {
        "id": "d0Tu6ZOoV9zG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now set your device to cuda:\n"
      ],
      "metadata": {
        "id": "ih9kD2nlWiNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    \n",
        "print(device) "
      ],
      "metadata": {
        "id": "EeAp1Nd1Wjw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "7561oYG86LeY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##Exercise 1: ResNet"
      ],
      "metadata": {
        "id": "VOXVPjzzU3CI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK-5lOHjZS7F"
      },
      "source": [
        "**ResNet with PyTorch**\n",
        "\n",
        "ResNet was first introduced in 2015 as a way to support training of deeper networks through supporting networks in learning identity mappings during training. It does this through implementation of residual blocks\n",
        "\n",
        "An example of a resnet block (from the original [2015 paper](https://arxiv.org/abs/1512.03385)) is illustrated below (see [image source](t)):\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://drive.google.com/uc?id=1NQ_sLsu0GsXQsVEQ9Rtm5Mnvm2BuucCZ\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
        "<figcaption align = \"centre\"> Fig 1. ResNet 2015 residual block  </figcaption>\n",
        "</figure>\n",
        "\n",
        "Here, input data passes down two paths. In one, it is passed through two convolutional (weights learning) layers; in the other it skips these out to be added to the output of these layers. This shortcut operation is the identity mapping. If there are no gains to be made by learning more weights kernels (the nework is already deep enough); then the network can simply learn to pass the input unchanged through the block (an idenity transform) by pushing these weights kernels to zero.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Using PyTorch implementation**\n",
        "\n",
        "Torchvision offers some default implementations of popular networks\n",
        "\n",
        "For example the following pretrained resnets models can be loaded in Pytorch:\n",
        "```python\n",
        "import torchvision\n",
        "torchvision.models.resnet18(pretrained=True, **kwargs)\n",
        "```\n",
        "\n",
        "You can also load a model that hasn't been pretrained in the following way:\n",
        "```python\n",
        "torchvision.models.resnet18(pretrained=False, **kwargs)\n",
        "```\n",
        "\n",
        "To see more examples, including networks such as ResNet, Alexnet, VGG, Densenet, see [torchvision models](https://pytorch.org/docs/stable/torchvision/models.html) and, for usage, see the official [tutorial](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)\n",
        "\n",
        "However, these pretrained models will not always suit your needs. For example, the resnet models are designed for 3 channel, two-dimensional input  (i.e. for RGB natural channels); this means that you can't use them without adjustment on grayscale images, or on 3D medical data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Coding the residual block**\n",
        "\n",
        "The first thing we need to do is implement a `BasicBlock` class, which will implement a single ResNet (2015) block, which includes the following steps (see Fig 1): \n",
        "\n",
        "1. **(strided) Convolution, followed by batchnorm**, followed by relu: with option to downsample through stride=2 and increase the number of output channels\n",
        "2. **Convolution, followed by batchnorm:** stride 1; input and output channels constant\n",
        "3. **shortcut step**, where the input is first transformed through a strided $1 \\times 1$ convolutional operation to match the dimensions of the output of the residual block and then added to the output of the convolutions. \n",
        "4. **relu**\n",
        "\n",
        "Note, **only the first convolution of each block offers the option of upsampling the channel dimension and downsampling the data through striding**. Further, several residual blocks are typically changed together between downsampling steps (see lilac, green and red groups); therefore downsampling is not implemented for all blocks.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1SVmOrg7uxRowWQNtr5jWmz2re9fLID4Q\" alt=\"Drawing\" style=\"width: 800px;\"/>"
      ],
      "metadata": {
        "id": "x7nxJdiqX6yB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ex 1.1 - Create the Residual  block"
      ],
      "metadata": {
        "id": "j5iYqnJqhbVw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The most challenging bit of coding up a residual block is implementing the reshapeing of the shortcut step. **So let's start by ignoring it to create the main body of the residual block. This will work _provided we maintain input dimensions_**. \n",
        "\n",
        "Let us create a `ResidualBlock` and define (parametrise) the required `Conv2d` and `BatchNorm2d` steps in the constructor `__init__(self, channels1,channels2,res_stride=1)`; here `res_stride` is the intended stride, `channels1` are the number of channels of the incoming activations, and `channels2` is the number of output channels. Note, `res_stride`=1 by default and this should only change if this is intended as a downsampling block;\n",
        "\n",
        "Note, biases are set to `False` in the block as they are instead handled by the batchnorm layer. Also, observe that the Relu layer is implemented in the forward pass function.\n",
        "\n",
        "**Task 1.1.1** Edit (`__init__`) to input\n",
        "\n",
        "1. `self.conv1` a 2D convolution with the power to: a) downsample spatial dimensions (with stride `res_stride` ); and b) upsample channel dimensions (to `channels2`). Set arguments `kernel_size=3, stride=res_stride, padding=1, bias=False`\n",
        "2. ` self.bn1` a 2D batchnorm layer to follow the first convolutional layer. What does it expect for the number of input features (`num_features`)?\n",
        "3. `self.conv2` the second convolutional layer. What should its stride, input and output channel dimensions be given **only the first convolution can change output dimensions**? (set `kernel_size=3, padding=1, bias=False` as before)\n",
        "4. ` self.bn2` a 2D batchnorm layer to follow the second convolutional layer. **Note, a different batch normalisation instance is needed each time as each stores learnable parameters**.\n",
        "\n",
        "**Task 1.1.2** Edit `forward(self, x)` line 35 to **implement the shortcut**. Here the identity mapping `self.shortcut(x)` must be _added_ to the output of the weights layers. \n",
        "\n",
        "For PyTorch documentation, see [nn.Conv2d](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d) and [nn.BatchNorm2d](https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d)\n",
        "\n",
        "**Make sure you understand what all lines of the forward function are doing**. Note, that the output of the first operation is assigned to variable `out` in order to preserve the input `x` for the shortcut (identity) mapping.\n"
      ],
      "metadata": {
        "id": "5hXEhUmVtKeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, channels1,channels2,res_stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.inplanes=channels1\n",
        "\n",
        "        # Task 1.1.1 construct the block \n",
        "        # implement conv1 (which option for reshaping), conv2 (no reshaping) and 2 batchnorm layers to insert between each\n",
        "        self.conv1 = nn.Conv2d(channels1, channels2, kernel_size=3, stride=res_stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(channels2)\n",
        "        self.conv2 = nn.Conv2d(channels2, channels2, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(channels2)  \n",
        "\n",
        "        self.shortcut=nn.Sequential()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        # forward pass: Conv2d > BatchNorm2d > ReLU > Conv2D >  BatchNorm2d > ADD > ReLU\n",
        "        out=self.conv1(x)\n",
        "        out=self.bn1(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        # Task 1.1.3 - implement the shortcut (1 line)\n",
        "        out += self.shortcut(x)\n",
        "\n",
        "        # final ReLu\n",
        "        out = F.relu(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "q3eTlXHyjgtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Ex 1.2 . Perform a test forward pass (_keeping input and output dimensions constant_ )"
      ],
      "metadata": {
        "id": "IftbV5zBkEJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Task 1.2.1.** Instantiate an instance of class ResidualBlock (create a network called `blk`) with input channels = 3 and output channels = 3; leave `res_stride` as default (1). **We have not implemented a shortcut with downsampling yet so running with stride will fail.**\n",
        "\n",
        "**Task 1.2.2.** create a random tensor of size $5 \\times 3 \\times 100 \\times 100$ (which matches expected input dimensions $N,C_{in},H,W$)\n",
        "\n",
        "**Task 1.2.3.** Pass the input through a forward pass and print input and output shape.\n",
        "\n",
        "HINT: look at how this was done in previous training loops. Remember - you don't need to explicitely call the forward function."
      ],
      "metadata": {
        "id": "bl6v9JhItZIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1.2.1\n",
        "blk = ResidualBlock(3,3)\n",
        "\n",
        "\n",
        "# Task 1.2.2\n",
        "data = torch.randint(0, 255, (5,3,100,100)).to(torch.float)\n",
        "\n",
        "\n",
        "# Task 1.2.3\n",
        "output=blk(data)\n",
        "print(data.shape,output.shape)"
      ],
      "metadata": {
        "id": "tuRPZKo2kDpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ex 1.3. Implement the shortcut"
      ],
      "metadata": {
        "id": "St0WvStYmcIm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, lets implement a shortcut with downsampling. \n",
        "\n",
        "Specfically, **if this is a reshaping residual block** (`channels2` $\\ne$ `channels1` and `res_stride` $\\neq$ 1) then **we will also need to reshape the input as it is passed through the shortcut**. Edit the `ResidualBlock` constructor to complete the shortcut function, which will downsample the input as it is passed through the shortcut. \n",
        "\n",
        "**Task 1.3.1. Change `ResidualBlock.__init__()` to implement a `nn.Sequential()` block with two steps:**\n",
        "1. A $1 \\times 1 $ `nn.Conv2d` layer with `stride = res_stride, bias = False`.  This will support changes of spatial dimensions through strided convolutions and changes of feature dimensions through $1 \\times 1 $ convolutions. What should your input and output channels be to make it equivalent to the output of a _reshaping_ residual block?\n",
        "2. Batch normalisation. Think carefully about the input dimension. \n"
      ],
      "metadata": {
        "id": "x3rhd-aatkeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, channels1,channels2,res_stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.inplanes=channels1\n",
        "\n",
        "        self.conv1 = nn.Conv2d(channels1, channels2, kernel_size=3, stride=res_stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(channels2)\n",
        "        self.conv2 = nn.Conv2d(channels2, channels2, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(channels2)\n",
        "\n",
        "        if res_stride != 1 or channels2 != channels1:\n",
        "            # Exercise 1.3 \n",
        "            # create an nn.Sequential() block with one 1x1 conv2D and one batchnorm\n",
        "            self.shortcut=nn.Sequential(\n",
        "                nn.Conv2d(channels1, channels2, kernel_size=1, stride=res_stride, bias=False),\n",
        "                nn.BatchNorm2d(channels2)\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            self.shortcut = nn.Sequential()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        out=self.conv1(x)\n",
        "        out=self.bn1(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "7miHpcJPm64_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1.3.2.** Test the network again, but this time implement `stride = 2` and change the number of output channels to 10.\n",
        "\n",
        "\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "abFRTdo9ptMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blk = ResidualBlock(3,10,2)\n",
        "\n",
        "output=blk(data)\n",
        "\n",
        "print(data.shape,output.shape)"
      ],
      "metadata": {
        "id": "1jUXhkkfpseQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "We now have all the building blocks we need to build a residual network. In what follows we will construct a ResNet with four residual layers. Each layer will contain 2 residual blocks. "
      ],
      "metadata": {
        "id": "_UHBhveRuBpB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Ex 1.4 - Create a Residual Network class\n"
      ],
      "metadata": {
        "id": "GnFhX4ZRuM97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the original paper the network starts with a  convolutional layer with a $7 \\times 7 $ kernel, followed by a batchnorm. However, as we intend to test on the MNIST (which is very small) lets change the $7 \\times 7 $ kernel to a $3 \\times 3 $.\n",
        "\n",
        "We will implement 4 residual layers (or blocks), where the residual block class is passed to the network class as the argument `block`, and output channels and strides for each block are parametrised by lists (also past to the constructor) as `num_features` and `num_strides` respectively.\n",
        "\n",
        "**Task 1.4.1.** \n",
        "- Initialise the network with a 3 × 3 convolutional layer, with input channels = `in_channels`, output channel = `num_features[0]`,  stride = `num_strides[0]`, padding=1 and bias false; \n",
        "- Implement a batch normalisation layer to follow this.\n",
        "\n",
        "**Task 1.4.2.** \n",
        "\n",
        "- Comment the function `_make_layer`. What is each line doing? \n",
        "- Make sure you understand how this is used to create residual blocks in the constructor \n",
        "\n",
        "**Task 1.4.3.** \n",
        "\n",
        "- The penultimate layer of the network is an average pool which averages over spatial dimensions to return a flattened vector of length equal to the number of channels of the tensor passed to it. \n",
        "- The network must output 10 class predictions\n",
        "- Bearing that in mind, implement the final linear layer of the network \n",
        "\n",
        "**hint** if you remain unsure you can always print the shape of all the tensors in the network"
      ],
      "metadata": {
        "id": "eaLKbSN3uZFa"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAryRi7PZS7N"
      },
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_strides, num_features, in_channels, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = num_features[0] # in_planes stores the number channels output from first convolution\n",
        "        \n",
        "        # TASK 1.4.2. replace 'None' to initialise the network with a 3x3 conv and batch norm (2 lines):\n",
        "        self.conv1 = nn.Conv2d(in_channels, num_features[0], kernel_size=3, stride=num_strides[0], padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(num_features[0])\n",
        "\n",
        "        # Creating 4 residual layers - num_blocks per layer is given by input argument num_blocks (which is an array)\n",
        "        self.layer1 = self._make_layer(block, num_features[1], num_blocks, stride=num_strides[1])\n",
        "        self.layer2 = self._make_layer(block, num_features[2], num_blocks, stride=num_strides[2])\n",
        "        self.layer3 = self._make_layer(block, num_features[3], num_blocks, stride=num_strides[3])\n",
        "        self.layer4 = self._make_layer(block, num_features[4], num_blocks, stride=num_strides[4])\n",
        "        \n",
        "        # TASK 1.4.3. create linear layer:\n",
        "        self.linear = nn.Linear(num_features[4], num_classes)\n",
        "\n",
        "    # TASK 1.4.2. comment the function:\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        layers = []\n",
        "        # create initial layer with option of downsampling and increase channels\n",
        "        layers.append(block(self.in_planes, planes, stride))\n",
        "        # then create num_blocks more for each group\n",
        "        for i in np.arange(num_blocks):\n",
        "            layers.append(block(planes, planes))\n",
        "        \n",
        "        # update class attribute in_planes which is keeping track of input channels\n",
        "        self.in_planes = planes \n",
        "              \n",
        "        return nn.Sequential(*layers) # return sequential object comining layers\n",
        "        \n",
        "    def forward(self, x):\n",
        "      # initial convolution and batch norm\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        # residual blocks \n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        \n",
        "        # average pool (flattens spatial dimensions)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)       \n",
        "        out = self.linear(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ex 1.5 - Train on MNIST for classification\n",
        "\n"
      ],
      "metadata": {
        "id": "jTM4JTr40-AQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we have created an instance of our resent class which runs four levels of residual blocks, with 2 blocks in each group"
      ],
      "metadata": {
        "id": "MtgPc0fU1fJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "resnet = ResNet(ResidualBlock,3, [1,1,2,2,2], [64,64,128,256,512], in_channels=1)\n",
        "\n",
        "# see how the network is loaded to the device (GPU)\n",
        "# this allows the optimisation to be run on GPU\n",
        "resnet = resnet.to(device) "
      ],
      "metadata": {
        "id": "xUqwBG0_2SR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1.5.1.** Create a suitable loss function for classification"
      ],
      "metadata": {
        "id": "r5H8Rsvf2ehw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fun = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "ktKal-Rj3qFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1.5.2.** Create an SGD optimiser with momentum, and assign learning rate as 0.001"
      ],
      "metadata": {
        "id": "GRxDp9WU4CeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(resnet.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "3nZDpDNJ2qy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1.5.3.** Complete the training function - don't forget to set runtime to GPU and to push input data and labels (from each batch) to the device."
      ],
      "metadata": {
        "id": "sNONMsVp4DMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1\n",
        "for epoch in range(epochs): \n",
        "\n",
        "    # enumerate can be used to output iteration index i, as well as the data \n",
        "    for i, (data, labels) in enumerate(train_loader, 0):\n",
        "        # TASK 1.5.3. Complete training loop:\n",
        "        data = data.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # clear the gradient\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #feed the input and acquire the output from network\n",
        "        outputs = resnet(data)\n",
        "\n",
        "        #calculating the predicted and the expected loss\n",
        "        loss = loss_fun(outputs, labels)\n",
        "\n",
        "        #compute the gradient\n",
        "        loss.backward()\n",
        "\n",
        "        #update the parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Print statistics of loss tensor\n",
        "        ce_loss = loss.item()\n",
        "        if i % 100 == 0:\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                 (epoch + 1, i + 1, ce_loss))"
      ],
      "metadata": {
        "id": "sKsEdThi25FY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1.5.4.** Test performance of your network by running the validation code in the cells below:"
      ],
      "metadata": {
        "id": "fYaI4Z-245CS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7XU-rO3ZS7Z"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Make an iterator from test_loader\n",
        "test_iterator = iter(test_loader)\n",
        "\n",
        "# Get a batch of testing images\n",
        "images, labels = test_iterator.next()\n",
        "\n",
        "images = images.to(device)\n",
        "labels = labels.to(device)\n",
        "\n",
        "y_score = resnet(images)\n",
        "\n",
        "# Get predicted class from the class probabilities\n",
        "_, y_pred = torch.max(y_score, 1)\n",
        "\n",
        "print('Predicted: ', ' '.join('%5s' % classes[y_pred[j]] for j in range(8)))\n",
        "\n",
        "# Plot true label (t) vs predicted label (p)\n",
        "rows = 2\n",
        "columns = 4\n",
        "fig2 = plt.figure()\n",
        "for i in range(8):\n",
        "    fig2.add_subplot(rows, columns, i+1)\n",
        "    plt.title('t: ' + classes[labels[i].cpu()] + ' p: ' + classes[y_pred[i].cpu()])\n",
        "    img = images[i] / 2 + 0.5     # this is to unnormalize the image\n",
        "    img = torchvision.transforms.ToPILImage()(img.cpu())\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZEbMq6mZS7b"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
        "\n",
        "y_true = labels.data.cpu().numpy()\n",
        "y_pred = y_pred.data.cpu().numpy()\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "precision = precision_score(y_true, y_pred, average='macro')\n",
        "recall = recall_score(y_true, y_pred, average='macro')\n",
        "\n",
        "print('accuracy:', accuracy, ', f1 score:', f1, ', precision:', precision, ', recall:', recall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "AO5NSzmM6Ca-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercise 2: Image segmentation with PyTorch using U-net\n",
        "\n"
      ],
      "metadata": {
        "id": "ofbhNAS952v4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "U-net was first developed in 2015 by [Ronneberger et al.](https://arxiv.org/abs/1505.04597), as a segmentation network for biomedical image analysis.\n",
        "It has been extremely successful, with 9,000+ citations, and many new methods that have used the U-net architecture since.\n",
        "\n",
        "\n",
        "The architecture of U-net is based on the idea of using skip connections (i.e. concatenating) at different levels of the network to retain high, and low level features.\n",
        "\n",
        "Here is the architecture of a U-net:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1zUKKrbcB1BZxJ7-hEYpteCVlVFRJ1nRg\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Dataset**\n",
        "\n",
        "In this tutorial we use a dataset of cortical neurons with their corresponding segmentation binary labels.\n",
        "\n",
        "These images were collected using in-vivo two-photon microscopy from the mouse somatosensory cortex. To generate the 2D images, a max projection was used over the 3D stack. The labels are binary segmentation maps of the axons.\n",
        "\n",
        "Here we will use 100 [64x64] crops during training and validation. Below are some example images [256x256] from the original dataset, taken from [Bass et al 2019](http://proceedings.mlr.press/v102/bass19a.html)\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1YRJev88nBr4aqyaHRU27KWxFaX4JwUJz\" alt=\"Drawing\" style=\"width: 800px;\"/>"
      ],
      "metadata": {
        "id": "dNMFeCDc6cTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the cell below to download the data and auxiliary scripts to your Colab working directory:"
      ],
      "metadata": {
        "id": "ht7u_VH47KGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nv https://github.com/IS-pillar-3/datasets/raw/main/session_5/AxonDataset.py\n",
        "!wget -nv https://github.com/IS-pillar-3/datasets/raw/main/session_5/org64_data_train.npy\n",
        "!wget -nv https://github.com/IS-pillar-3/datasets/raw/main/session_5/org64_mask_train.npy"
      ],
      "metadata": {
        "id": "z9_VLFvo8TRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd /content\n",
        "\n",
        "#load modules\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "import torchvision.utils as vutils\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from AxonDataset import *\n",
        "\n",
        "# Setting parameters\n",
        "timestr = time.strftime(\"%d%m%Y-%H%M\")\n",
        "__location__ = os.path.realpath(\n",
        "    os.path.join(os.getcwd(), os.path.dirname('__file__')))\n",
        "\n",
        "path = __location__\n",
        "\n",
        "results_path = os.path.join(__location__,'results')\n",
        "if not os.path.exists(results_path):\n",
        "    os.makedirs(results_path)\n",
        "\n",
        "print(path)"
      ],
      "metadata": {
        "id": "FJriBd-K8cjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ex 2.1 Creating a dataloader"
      ],
      "metadata": {
        "id": "J5TuujQeRiQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, a custom dataset was created, and we import it from `AxonDataset.py`. \n",
        "\n",
        "Then, rather than create a separate test and train instance, we override the default DataLoader `shuffle` option to instead implement a  `torch.utils.data.sampler.SubsetRandomSampler` to 'sample elements randomly from a given list of indices, without replacement' (see [PyTorch documentation](https://pytorch.org/docs/stable/data.html#torch.utils.data.SubsetRandomSampler)).\n",
        "\n",
        "To do this:\n",
        "1. data is randomly split into 80% train and 20% validation sets\n",
        "2. these lists are passed to class `SubsetRandomSampler` to create a sampling instance for each group\n",
        "3. train and validation DataLoaders are created from the same dataset by passing a different sampler for each class\n",
        "\n",
        "This is a good way of randomly separating your own data, in instances where PyTorch does not provide custom Datasets"
      ],
      "metadata": {
        "id": "rBc4PVppNoun"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.1.1.** Comment the lines of code below to verify you understand how the bespoke samplers are implemented"
      ],
      "metadata": {
        "id": "Wt4hp6WwPRKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#First we create a custom dataset of two photon microscopy images of axons\n",
        "axon_dataset = AxonDataset(data_name='org64', type='train')\n",
        "\n",
        "# We need to further split our training dataset into training and validation sets.\n",
        "# Determine the number of examples in train and validation and create list of all indices\n",
        "indices = list(range(len(axon_dataset)))  \n",
        "# define the split size\n",
        "split = int(len(indices)*0.2)  \n",
        "\n",
        "# Get random list of indices for validation\n",
        "validation_idx = np.random.choice(indices, size=split, replace=False)\n",
        "# training examples are remainder \n",
        "train_idx = list(set(indices) - set(validation_idx))\n",
        "\n",
        "# feed indices into the SubsetRandomSampler to create a sampling instance for each of train and validation\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "validation_sampler = SubsetRandomSampler(validation_idx)\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "# Create a dataloader instance overriding shuffle to pass the bespoke samplers\n",
        "train_loader = torch.utils.data.DataLoader(axon_dataset, batch_size = batch_size,\n",
        "                                           sampler=train_sampler) \n",
        "val_loader = torch.utils.data.DataLoader(axon_dataset, batch_size = batch_size,\n",
        "                                        sampler=validation_sampler) \n"
      ],
      "metadata": {
        "id": "015RBHTIPRb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Building a U-net"
      ],
      "metadata": {
        "id": "bx67SAc6RUUn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVp-bRHWZS74"
      },
      "source": [
        "The original U-net encoder performs downsampling through a $2 \\times 2 $ max pool (however, strided convolutions are equally viable). Thus, in what follows, a single level of encoding can be represented as:\n",
        "\n",
        "```\n",
        " conv1 = self.dconv_down1(x)\n",
        " conv1 = self.dropout(conv1)\n",
        " x = self.maxpool(conv1)\n",
        " ```\n",
        "Here, a dropout layer is inserted between the convolutional layer and the maxpool for regularisation. An alternative approach is to insert a batchnorm between the `nn.Conv2d` and the `nn.ReLU` e.g. [see](https://github.com/milesial/Pytorch-UNet)\n",
        "\n",
        "Next we need to define how we perform an upsample step. This  is performed through use of [`nn.Upsample`](https://pytorch.org/docs/stable/nn.html#torch.nn.Upsample), which interpolates the data to a higher resolution grid. The layer must be created in the consructor (see line 14)  and expects arguments `scale_factor` and (interpolation) `mode`. There are several options for the interpolation mode; we recommend bilinear. In this example we upsample by a `scale_factor` of 2 each time (to match the $2\\times 2$ max pool used during downsampling). \n",
        "        \n",
        "Then, a single level of decoding might may represented as:\n",
        "\n",
        "```\n",
        " deconv4 = self.upsample(conv5)\n",
        " deconv4  = self.dconv_up4(deconv4)\n",
        " deconv4 = self.dropout(deconv4)\n",
        " ```\n",
        " \n",
        "However, we are still missing something vital...\n",
        "\n",
        "**Skip connections**\n",
        "\n",
        "The U-net is a symmetric network with equal numbers of encoding and decoding layers. These form pairs where the spatial dimensions of each encoder/decoder layer in the pair are consistent.\n",
        "\n",
        "A key feature of the U-net is that to support segmentation of sharp boundaries, with preservation of high resolution features, it is necessary to pass features learnt during encoding across the network. The theory is that the early layers, with their small-receptive fields, learn the high-spatial frequency information (i.e. they act as edge detectors and/or texture filters). As the receptive field increases during encoding spatial specicity is lost, but spatial localisation (where class relevant objects broadly are in the image) is gained. In order to import the high spatial frequency information of the early encoding layers into the final decoding layers the *activations* learnt during encoding are directly concatenated onto the upsampled activations of the paired decoding layer.\n",
        "\n",
        "In other words for the first decoding layer (which for a 5-layer U-Net is the layer that directly follows the bottleneck `conv5`) is:\n",
        "\n",
        "```\n",
        " deconv4 = self.upsample(conv5)\n",
        " deconv4 = torch.cat([deconv4, conv4], dim=1)\n",
        " deconv4  = self.dconv_up4(deconv4)\n",
        " deconv4 = self.dropout(deconv4)\n",
        " ```\n",
        " \n",
        " The activations (output) of convolution layer conv (`conv4`) is directly concatenated to the output of `self.upsample` where concatenation is performed on the channel axis (`axis=1`); Thus putting this all together..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s42uHIxZS74"
      },
      "source": [
        "### Ex 2.2 - Creating the U-net Class"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we define a layer double_conv that performs 2 sets of convolution followed by ReLu. This is set up as a nn.Sequential() block."
      ],
      "metadata": {
        "id": "VYs-h0MrUNdH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnZWq_gwZS73"
      },
      "source": [
        "def double_conv(in_channels, out_channels, padding=1):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=padding),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=padding),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then initialise all the different layers in the network in `__init__`:\n",
        "1. `self.dconv_down1` is a double convolutional layer (defined above)\n",
        "2. `self.maxpool` is a max pooling layer that is used to reduce the size of the input, and increase the receptive field\n",
        "3. `self.upsample` is an upsampling layer that is used to increase the size of the input\n",
        "4. `dropout` is a dropout layer that is applied to regularise the training\n",
        "5. `dconv_up4` is also a double convolutional layer- note that it takes in additional channels from previous layers (i.e. the skip connections).\n",
        "\n",
        "\n",
        "**Task 2.2.1.** Following the example for `conv1` complete encoder layers 2, 3 and 4. How many features does each layer have?\n",
        "\n",
        "**Task 2.2.2.** Complete layer `conv5`; this is the bottleneck layer (the bottom of the network) and thus **has no maxpool**.\n",
        "\n",
        "**Task 2.2.3.**  Using the upsampling and skip connection example above implement the decoder layers `deconv4`, `deconv3`, `deconv2`, `deconv1`.\n",
        "\n",
        "**Task 2.2.4.** We are expecting class labels as output; thus the output requires a sigmoid transformation; check you understand what this does?"
      ],
      "metadata": {
        "id": "GKlGUKpbTplN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.dconv_down1 = double_conv(1, 32)\n",
        "        self.dconv_down2 = double_conv(32, 64)\n",
        "        self.dconv_down3 = double_conv(64, 128)\n",
        "        self.dconv_down4 = double_conv(128, 256)\n",
        "        self.dconv_down5 = double_conv(256, 512)\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(2)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
        "        self.dropout = nn.Dropout2d(0.5)\n",
        "        self.dconv_up4 = double_conv(256 + 512, 256)\n",
        "        self.dconv_up3 = double_conv(128 + 256, 128)\n",
        "        self.dconv_up2 = double_conv(128 + 64, 64)\n",
        "        self.dconv_up1 = double_conv(64 + 32, 32)\n",
        "\n",
        "        self.conv_last = nn.Conv2d(32, 1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        ####### ENCODER #######\n",
        "        \n",
        "        # layer 1\n",
        "        conv1 = self.dconv_down1(x)\n",
        "        conv1 = self.dropout(conv1)\n",
        "        x = self.maxpool(conv1)\n",
        "\n",
        "        # TASK 3.2.1. Replace 'nones' to implement encoder layers conv2, conv3 and conv4\n",
        "        # layer 2\n",
        "        conv2 = self.dconv_down2(x)\n",
        "        conv2 = self.dropout(conv2)\n",
        "        x = self.maxpool(conv2)\n",
        "\n",
        "        # layer 3\n",
        "        conv3 = self.dconv_down3(x)\n",
        "        conv3 = self.dropout(conv3)\n",
        "        x = self.maxpool(conv3)\n",
        "\n",
        "        # layer 4\n",
        "        conv4 = self.dconv_down4(x)\n",
        "        conv4 = self.dropout(conv4)\n",
        "        x = self.maxpool(conv4)\n",
        "\n",
        "        # TASK 3.2.2. Replace 'nones' to implement bottleneck (hint: 2 lines)\n",
        "        # layer 5\n",
        "        conv5 = self.dconv_down5(x)\n",
        "        conv5 = self.dropout(conv5) \n",
        "        \n",
        "\n",
        "        ####### DECODER #######\n",
        "\n",
        "        # TASK 3.2.3. Implement the decoding layers\n",
        "        deconv4 = self.upsample(conv5)\n",
        "        deconv4 = torch.cat([deconv4, conv4], dim=1)  \n",
        "        deconv4  = self.dconv_up4(deconv4)\n",
        "        deconv4 = self.dropout(deconv4)\n",
        "\n",
        "        deconv3 = self.upsample(deconv4 )       \n",
        "        deconv3 = torch.cat([deconv3, conv3], dim=1)\n",
        "        deconv3 = self.dconv_up3(deconv3)\n",
        "        deconv3 = self.dropout(deconv3)\n",
        "\n",
        "        deconv2 = self.upsample(deconv3)      \n",
        "        deconv2 = torch.cat([deconv2, conv2], dim=1)\n",
        "        deconv2 = self.dconv_up2(deconv2)\n",
        "        deconv2 = self.dropout(deconv2)\n",
        "       \n",
        "        deconv1 = self.upsample(deconv2)   \n",
        "        deconv1 = torch.cat([deconv1, conv1], dim=1)\n",
        "        deconv1 = self.dconv_up1(deconv1)\n",
        "        deconv1 = self.dropout(deconv1)\n",
        "\n",
        "        out = F.sigmoid(self.conv_last(deconv1)) \n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "cy-Ksk3KUwzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XIjmMWgZS79"
      },
      "source": [
        "### Exercise 2.3 - Create loss function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We next define our loss function - in this case we use Dice loss, a commonly used loss for image segmentation that is essentially a measure of overlap between two samples.\n",
        "\n",
        "Dice is in the range of 0 to 1, where a Dice coefficient of 1 denotes perfect and complete overlap. The Dice coefficient was originally developed for binary data, and can be calculated as:\n",
        "\n",
        "$Dice = \\dfrac{2|A\\cap B|}{|A| + |B|}$\n",
        "\n",
        "where $|A\\cap B|$ represents the common elements between sets $A$ and $B$, and $|A|$ represents the number of elements in set $A$ (and likewise for set $B$).\n",
        "\n",
        "For the case of evaluating a Dice coefficient on predicted segmentation masks, we can get the intersection $|A\\cap B|$ as the element-wise multiplication between the prediction and target mask, and then sum the resulting matrix.\n",
        "\n",
        "**Task 2.3.1.** Implement the calculation of the Dice coeficient in the function below:"
      ],
      "metadata": {
        "id": "Vydl9DOIWgGq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmCqq3dxZS7-"
      },
      "source": [
        "def dice_coeff(pred, target):\n",
        "    smooth = 1. # this is added to avoid errors coming from possible division by zero\n",
        "    epsilon = 10e-8\n",
        "\n",
        "    iflat = pred.contiguous().view(-1)\n",
        "    tflat = target.contiguous().view(-1)\n",
        "    intersection = (iflat * tflat).sum() + smooth\n",
        "\n",
        "    A_sum = torch.sum(iflat) \n",
        "    B_sum = torch.sum(tflat) + smooth\n",
        "\n",
        "    # TASK 2.3.1. replace 'None' to implement the dice coeficient \n",
        "    dice = (2. * intersection + smooth) / (A_sum + B_sum + smooth)\n",
        "    dice = dice.mean(dim=0)\n",
        "    dice = torch.clamp(dice, 0, 1.0-epsilon)\n",
        "\n",
        "    return  dice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An **alternative loss** function would be pixel-wise binary cross entropy (BCE) loss. It would examine each pixel individually, comparing the class predictions (depth-wise pixel vector) to our one-hot encoded target vector.\n",
        "\n",
        "**Task 2.3.2.** Implement the binary cross entropy loss using the PyTorch function"
      ],
      "metadata": {
        "id": "gicwqEZpL6Jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 2.3.2. implement the BCE loss\n",
        "loss_BCE = nn.BCELoss()"
      ],
      "metadata": {
        "id": "QQ_FWnBBL6aE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "S7LQZAbsq1tM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving and loading models\n",
        "For practical reasons training this network from scratch will take too long, and require large computational resources. To save time we initialise the network with a previously trained network by loading the weights in the following way:"
      ],
      "metadata": {
        "id": "gE3MOkQuqHdx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNE51dT4ZS77"
      },
      "source": [
        "# Initialise network - and load weights\n",
        "net = UNet()\n",
        "\n",
        "# This function loads a pretrained network\n",
        "net.load_state_dict(torch.load(path + '/' + 'model.pt', map_location=torch.device(device)))\n",
        "net = net.to(device)\n",
        "\n",
        "# Example how to save a model - check in your results path\n",
        "torch.save(net.state_dict(), path + '/model_save_test.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd6uu5U-Nnsr"
      },
      "source": [
        "In general [PyTorch documentation](https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html#save-the-general-checkpoint), it is advised that you save and load not just network paramters but also the state of the optimiser, current state of the loss and the epoch:\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://drive.google.com/uc?id=1hU498xlA_DbstHSSUwqfm9U9fqtGCZw1\" alt=\"Drawing\" width=\"800px;\"/>\n",
        "</figure>\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://drive.google.com/uc?id=1mQllAgFWxZ9ViaXJmPjeStXJViroBQHi\" alt=\"Drawing\" width=\"800px;\"/>\n",
        "</figure>\n",
        "\n",
        "More details on options for saving and loading are provided [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html). These provide an example of the model and optimiser `state_dict()` where you can see these are python dictionaries with:\n",
        "\n",
        "- **in the case of the model**: keys which store the current state of weight and bias tensors of the model. In more general terms the model dict will store all parameter tensors required to restart the model\n",
        "- **in the case of the optimiser**: this dict stores the hyper-parameters of the optimiser e.g. learning rate, momentum, weight decay etc as well as the current state of the optimiser object.\n",
        "\n",
        "Note, **if saving for inference _only_, it is only necessary to save the `model.state_dict()_**\n",
        "\n",
        "A common PyTorch convention is to save models using either a .pt or .pth file extension (see example of `SAVE_PATH`) above."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ex 2.4 - Training and Evaluation\n",
        "\n",
        "Review the stages of training with:\n",
        "- Network set as `net.train()` for training and `net.eval()` for validation\n",
        "- Clearing of gradients\n",
        "- Loss as `loss = 1 - dice_coeff(pred,target)` where pred is output of forwards pass\n",
        "- Backpropagation and update\n",
        "\n",
        "\n",
        "The results are saved per epoch for both training and validation in the `content/results/` folder, and are saved as the:\n",
        "\n",
        "* real data,\n",
        "* binary labels,\n",
        "* predicted labels.\n",
        "\n",
        "In this example, since we trained on a small sample of the data (100 crops), the results are far from optimal and are likely to overfit to the data."
      ],
      "metadata": {
        "id": "b4oJkMrnPpIN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WU5av-MRZS8C"
      },
      "source": [
        "from distutils.version import LooseVersion\n",
        "epochs = 10\n",
        "save_every = 10\n",
        "all_error = np.zeros(0)\n",
        "all_error_L1 = np.zeros(0)\n",
        "all_error_dice = np.zeros(0)\n",
        "all_dice = np.zeros(0)\n",
        "all_val_dice = np.zeros(1)\n",
        "all_val_error = np.zeros(0)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    ##########\n",
        "    # Train\n",
        "    ##########\n",
        "\n",
        "    # set network to train prior to training loop \n",
        "    net.train() \n",
        "    t0 = time.time()\n",
        "    for i, (data, label) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        label= label.to(device)\n",
        "        \n",
        "        optimizer.zero_grad() \n",
        "\n",
        "        target_real = torch.ones(data.size()[0])\n",
        "        batch_size = data.size()[0]\n",
        "        pred = net(data)\n",
        "\n",
        "        dice_value = dice_coeff(pred, label)\n",
        "        \n",
        "        loss = 1 - dice_value\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        time_elapsed = time.time() - t0\n",
        "        print('[{:d}/{:d}][{:d}/{:d}] Elapsed_time: {:.0f}m{:.0f}s Loss: {:.4f} Dice: {:.4f}'\n",
        "              .format(epoch, epochs, i, len(train_loader), time_elapsed // 60, time_elapsed % 60,\n",
        "                      LooseVersion.item(), dice_value.item()))\n",
        "\n",
        "        if i % save_every == 0:\n",
        "\n",
        "            vutils.save_image(data.data, '%s/epoch_%03d_i_%03d_train_data.png' % (path, epoch, i),\n",
        "                                  normalize=True)\n",
        "            vutils.save_image(label.data, '%s/epoch_%03d_i_%03d_train_label.png' % (path, epoch, i),\n",
        "                                  normalize=True)\n",
        "            vutils.save_image(pred.data, '%s/epoch_%03d_i_%03d_train_pred.png' % (path, epoch, i),\n",
        "                                  normalize=True)\n",
        "            error = loss.item()\n",
        "\n",
        "            all_error = np.append(all_error, error)\n",
        "            all_dice = np.append(all_dice, dice_value.item())\n",
        "\n",
        "    # # Task 2.4.2 - Checkpointing\n",
        "    torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': net.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': err,\n",
        "            }, path+'/model_ch.pt')\n",
        "\n",
        "    # #############\n",
        "    # # Validation\n",
        "    # #############\n",
        "    mean_error = np.zeros(0)\n",
        "    mean_dice = np.zeros(0)\n",
        "    t0 = time.time()\n",
        "\n",
        "    # set network to eval prior to training loop \n",
        "    net.eval()\n",
        "    for i, (data, label) in enumerate(val_loader):\n",
        "        data=data.to(device)\n",
        "        label=label.to(device)     \n",
        "        batch_size = data.size()[0]\n",
        "\n",
        "        pred = net(data)\n",
        "\n",
        "        dice_value = dice_coeff(pred, label)\n",
        "\n",
        "        err = 1 - dice_value\n",
        "\n",
        "        if i == 0:\n",
        "            vutils.save_image(data.data, '%s/epoch_%03d_i_%03d_val_data.png' % (path, epoch, i),\n",
        "                              normalize=True)\n",
        "            vutils.save_image(label.data, '%s/epoch_%03d_i_%03d_val_label.png' % (path, epoch, i),\n",
        "                              normalize=True)\n",
        "            vutils.save_image(pred.data, '%s/epoch_%03d_i_%03d_val_pred.png' % (path, epoch, i),\n",
        "                              normalize=True)\n",
        "\n",
        "        error = err.item()\n",
        "        mean_error = np.append(mean_error, error)\n",
        "        mean_dice = np.append(mean_dice, dice_value.item())\n",
        "\n",
        "    all_val_error = np.append(all_val_error, np.mean(mean_error))\n",
        "    all_val_dice = np.append(all_val_dice, np.mean(mean_dice))\n",
        "\n",
        "    time_elapsed = time.time() - t0\n",
        "\n",
        "    print('Elapsed_time: {:.0f}m{:.0f}s Val dice: {:.4f}'\n",
        "          .format(time_elapsed // 60, time_elapsed % 60, mean_dice.mean()))\n",
        "    \n",
        "    num_it_per_epoch_train = ((train_loader.dataset.x_data.shape[0] * (1 - 0.2)) // \n",
        "                              (save_every * batch_size)) + 1\n",
        "\n",
        "    epochs_train = np.arange(1,all_error.size+1) / num_it_per_epoch_train\n",
        "    epochs_val = np.arange(0,all_val_dice.size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.4.1.** Which operations are implemented only in the training loop and not during validation? Answer in the cell below:"
      ],
      "metadata": {
        "id": "lQGo3Rx-mUSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer.zero_grad(), loss.backward() and optimizer.step()"
      ],
      "metadata": {
        "id": "JG7n-UPhomwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.4.2.** Edit the training/validation code to add checkpointing using torch.save() - i.e. save the model and optimiser state_dict every epoch. "
      ],
      "metadata": {
        "id": "DGoMwTo-nm9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.4.3.** Plot the dice coefficient of the validation set using the plotting fucntion below; Note down your dice validation scores."
      ],
      "metadata": {
        "id": "IvQfZck3m1_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs_val, all_val_dice, label='dice_val')\n",
        "plt.xlabel('epochs')\n",
        "plt.legend()\n",
        "plt.title('Dice score')\n",
        "plt.savefig(path + '/dice_val.png')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "-_NDQDtGnOqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.4.4.** Change the dice loss to a binary cross entropy loss in the code and plot the results again - is dice loss or cross entropy loss better?"
      ],
      "metadata": {
        "id": "rfckAdLqppjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from distutils.version import LooseVersion\n",
        "epochs = 10\n",
        "save_every = 10\n",
        "all_error = np.zeros(0)\n",
        "all_error_L1 = np.zeros(0)\n",
        "all_error_dice = np.zeros(0)\n",
        "all_dice = np.zeros(0)\n",
        "all_val_dice = np.zeros(1)\n",
        "all_val_error = np.zeros(0)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    ##########\n",
        "    # Train\n",
        "    ##########\n",
        "\n",
        "    # set network to train prior to training loop \n",
        "    net.train() \n",
        "    t0 = time.time()\n",
        "    for i, (data, label) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        label= label.to(device)\n",
        "        \n",
        "        optimizer.zero_grad() \n",
        "\n",
        "        target_real = torch.ones(data.size()[0])\n",
        "        batch_size = data.size()[0]\n",
        "        pred = net(data)\n",
        "\n",
        "        dice_value = dice_coeff(pred, label)\n",
        "        \n",
        "        loss = loss_BCE(pred, label)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        time_elapsed = time.time() - t0\n",
        "        print('[{:d}/{:d}][{:d}/{:d}] Elapsed_time: {:.0f}m{:.0f}s Loss: {:.4f} Dice: {:.4f}'\n",
        "              .format(epoch, epochs, i, len(train_loader), time_elapsed // 60, time_elapsed % 60,\n",
        "                      LooseVersion.item(), dice_value.item()))\n",
        "\n",
        "        if i % save_every == 0:\n",
        "\n",
        "            vutils.save_image(data.data, '%s/epoch_%03d_i_%03d_train_data.png' % (path, epoch, i),\n",
        "                                  normalize=True)\n",
        "            vutils.save_image(label.data, '%s/epoch_%03d_i_%03d_train_label.png' % (path, epoch, i),\n",
        "                                  normalize=True)\n",
        "            vutils.save_image(pred.data, '%s/epoch_%03d_i_%03d_train_pred.png' % (path, epoch, i),\n",
        "                                  normalize=True)\n",
        "            error = loss.item()\n",
        "\n",
        "            all_error = np.append(all_error, error)\n",
        "            all_dice = np.append(all_dice, dice_value.item())\n",
        "\n",
        "\n",
        "    # #############\n",
        "    # # Validation\n",
        "    # #############\n",
        "    mean_error = np.zeros(0)\n",
        "    mean_dice = np.zeros(0)\n",
        "    t0 = time.time()\n",
        "\n",
        "    # set network to eval prior to training loop \n",
        "    net.eval()\n",
        "    for i, (data, label) in enumerate(val_loader):\n",
        "        data=data.to(device)\n",
        "        label=label.to(device)     \n",
        "        batch_size = data.size()[0]\n",
        "\n",
        "        pred = net(data)\n",
        "\n",
        "        dice_value = dice_coeff(pred, label)\n",
        "\n",
        "        err = loss_BCE(pred, label)\n",
        "\n",
        "        if i == 0:\n",
        "            vutils.save_image(data.data, '%s/epoch_%03d_i_%03d_val_data.png' % (path, epoch, i),\n",
        "                              normalize=True)\n",
        "            vutils.save_image(label.data, '%s/epoch_%03d_i_%03d_val_label.png' % (path, epoch, i),\n",
        "                              normalize=True)\n",
        "            vutils.save_image(pred.data, '%s/epoch_%03d_i_%03d_val_pred.png' % (path, epoch, i),\n",
        "                              normalize=True)\n",
        "\n",
        "        error = err.item()\n",
        "        mean_error = np.append(mean_error, error)\n",
        "        mean_dice = np.append(mean_dice, dice_value.item())\n",
        "\n",
        "    all_val_error = np.append(all_val_error, np.mean(mean_error))\n",
        "    all_val_dice = np.append(all_val_dice, np.mean(mean_dice))\n",
        "\n",
        "    time_elapsed = time.time() - t0\n",
        "\n",
        "    print('Elapsed_time: {:.0f}m{:.0f}s Val dice: {:.4f}'\n",
        "          .format(time_elapsed // 60, time_elapsed % 60, mean_dice.mean()))\n",
        "    \n",
        "    num_it_per_epoch_train = ((train_loader.dataset.x_data.shape[0] * (1 - 0.2)) // \n",
        "                              (save_every * batch_size)) + 1\n",
        "\n",
        "    epochs_train = np.arange(1,all_error.size+1) / num_it_per_epoch_train\n",
        "    epochs_val = np.arange(0,all_val_dice.size)"
      ],
      "metadata": {
        "id": "s2hd7EMxp6fX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs_val, all_val_dice, label='dice_val')\n",
        "plt.xlabel('epochs')\n",
        "plt.legend()\n",
        "plt.title('Dice score')\n",
        "plt.savefig(path + '/dice_val.png')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "XRPi_m8Yq6Tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.4.5.** Re-load and re-train from your saved model, making sure to load the state dict for the model *and* the optimiser."
      ],
      "metadata": {
        "id": "fR3lRJnIozU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = UNet()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=1e-05, betas=(0.5, 0.999))\n",
        "\n",
        "checkpoint = torch.load(path+'/model_ch.pt')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch_saved = checkpoint['epoch']\n",
        "loss = checkpoint['loss']\n",
        "\n",
        "epochs=20\n",
        "all_error = np.zeros(0)\n",
        "all_error_L1 = np.zeros(0)\n",
        "all_error_dice = np.zeros(0)\n",
        "all_dice = np.zeros(0)\n",
        "all_val_dice = np.zeros(1)\n",
        "all_val_error = np.zeros(0)\n",
        "\n",
        "for epoch in range(epoch_saved, epochs):\n",
        "\n",
        "    ##########\n",
        "    # Train\n",
        "    ##########\n",
        "\n",
        "    # set network to train prior to training loop \n",
        "    net.train() # this will ensure that parameters will be updated during training & that dropout will be used\n",
        "    t0 = time.time()\n",
        "    for i, (data, label) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        label= label.to(device)\n",
        "        \n",
        "        optimizer.zero_grad() \n",
        "\n",
        "        target_real = torch.ones(data.size()[0])\n",
        "        batch_size = data.size()[0]\n",
        "        pred = net(data)\n",
        "        \n",
        "        err = 1- dice_coeff(pred, label) #loss_BCE(pred, label)\n",
        "\n",
        "        dice_value = dice_coeff(pred, label).item()\n",
        "\n",
        "        err.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "\n",
        "        time_elapsed = time.time() - t0\n",
        "        print('[{:d}/{:d}][{:d}/{:d}] Elapsed_time: {:.0f}m{:.0f}s Loss: {:.4f} Dice: {:.4f}'\n",
        "              .format(epoch, epochs, i, len(train_loader), time_elapsed // 60, time_elapsed % 60,\n",
        "                      err.item(), dice_value))\n",
        "\n",
        "        if i % save_every == 0:\n",
        "            # setting your network to eval mode to remove dropout during testing\n",
        "            net.eval()\n",
        "\n",
        "            vutils.save_image(data.data, '%s/epoch_%03d_i_%03d_train_data.png' % (path, epoch, i),\n",
        "                                  normalize=True)\n",
        "            vutils.save_image(label.data, '%s/epoch_%03d_i_%03d_train_label.png' % (path, epoch, i),\n",
        "                                  normalize=True)\n",
        "            vutils.save_image(pred.data, '%s/epoch_%03d_i_%03d_train_pred.png' % (path, epoch, i),\n",
        "                                  normalize=True)\n",
        "\n",
        "            error = err.item()\n",
        "\n",
        "            all_error = np.append(all_error, error)\n",
        "            all_dice = np.append(all_dice, dice_value)\n",
        "\n",
        "   \n",
        "    # #############\n",
        "    # # Validation\n",
        "    # #############\n",
        "    mean_error = np.zeros(0)\n",
        "    mean_dice = np.zeros(0)\n",
        "    t0 = time.time()\n",
        "\n",
        "    # set network to eval prior to training loop \n",
        "    net.eval()\n",
        "    for i, (data, label) in enumerate(val_loader):\n",
        "        data=data.to(device)\n",
        "        label=label.to(device)     \n",
        "        batch_size = data.size()[0]\n",
        "\n",
        "        pred = net(data)\n",
        "        \n",
        "        err = 1-dice_coeff(pred, label)  #loss_BCE(pred, label)\n",
        "        \n",
        "        # compare generated image to data-  metric\n",
        "        dice_value = dice_coeff(pred, label).item()\n",
        "\n",
        "        if i == 0:\n",
        "            vutils.save_image(data.data, '%s/epoch_%03d_i_%03d_val_data.png' % (path, epoch, i),\n",
        "                              normalize=True)\n",
        "            vutils.save_image(label.data, '%s/epoch_%03d_i_%03d_val_label.png' % (path, epoch, i),\n",
        "                              normalize=True)\n",
        "            vutils.save_image(pred.data, '%s/epoch_%03d_i_%03d_val_pred.png' % (path, epoch, i),\n",
        "                              normalize=True)\n",
        "\n",
        "        error = err.item()\n",
        "        mean_error = np.append(mean_error, error)\n",
        "        mean_dice = np.append(mean_dice, dice_value)\n",
        "\n",
        "    all_val_error = np.append(all_val_error, np.mean(mean_error))\n",
        "    all_val_dice = np.append(all_val_dice, np.mean(mean_dice))\n",
        "\n",
        "    time_elapsed = time.time() - t0\n",
        "\n",
        "    print('Elapsed_time: {:.0f}m{:.0f}s Val dice: {:.4f}'\n",
        "          .format(time_elapsed // 60, time_elapsed % 60, mean_dice.mean()))\n",
        "    \n",
        "    \n",
        "    num_it_per_epoch_train = ((train_loader.dataset.x_data.shape[0] * (1 - 0.2)) // (\n",
        "            save_every * batch_size)) + 1\n",
        "    epochs_train = np.arange(1,all_error.size+1) / num_it_per_epoch_train\n",
        "    epochs_val = np.arange(0,all_val_dice.size)"
      ],
      "metadata": {
        "id": "6rOm_eBh2y5V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}