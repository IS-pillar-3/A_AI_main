{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36404fb1",
   "metadata": {},
   "source": [
    "# Matrix operations for a multi-class Softmax classifier with a hidden ReLu layer\n",
    "\n",
    "This notebook illustrates the matrix operations for the forward and backwards passes for a multinomial classifier, using the softmax activation function, the multi-class cross-entropy loss function, and with a hidden layer of ReLu units\n",
    "\n",
    "There is a unit in the output layer for each possible class, the units are \"one-hotted\" to yield 0 if the class is incorrect and 1 if it is correct\n",
    "\n",
    "There is also a quiz question, for you to figure out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04448a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This cell imports functions that are used later on\n",
    "#\n",
    "!wget -nv https://github.com/IS-pillar-3/A_AI_anc/raw/main/A_AI_softmax_relu_loss_01_v01.py\n",
    "import A_AI_softmax_relu_loss_01_v01 as sr\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d95d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Simulate forward and backward pass for Softmax multi-class classifier\n",
    "#\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "#\n",
    "#\n",
    "# Properties of generated training set\n",
    "#\n",
    "no_classes  = 4\n",
    "no_hidden   = 2\n",
    "no_features = 5\n",
    "no_samples  = 10\n",
    "#\n",
    "# Set up X training set, including a bias constant (1) as row 1\n",
    "#\n",
    "X = np.concatenate((np.ones((1, no_samples)), np.random.randn(no_features, no_samples)), axis=0)\n",
    "#\n",
    "#\n",
    "# Set up one-hotted Y's in Y_hot training set\n",
    "#\n",
    "# Use lower case letters as clases\n",
    "#\n",
    "letters = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "classes = list(letters[0:no_classes])\n",
    "#\n",
    "Y = np.random.choice(classes, no_samples)\n",
    "#\n",
    "# One-hot encode Y (from machinelearningmastery.com)\n",
    "#\n",
    "# Fit a LabelEncoder model to encode classes as integers\n",
    "#\n",
    "label_encoder = LabelEncoder()\n",
    "integer_codes = label_encoder.fit_transform(classes)\n",
    "#\n",
    "# Fit a OneHotEncoder model to encode integers as one-hots\n",
    "#\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_codes  = integer_codes.reshape(no_classes, 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_codes)\n",
    "#\n",
    "# Use encoder models to one-hot encode Y\n",
    "#\n",
    "Y_ints = label_encoder.transform(Y)\n",
    "print(Y)\n",
    "print(Y_ints)\n",
    "#\n",
    "Y_ints = Y_ints.reshape(no_samples, 1)\n",
    "Y_hot  = np.array(onehot_encoder.transform(Y_ints))\n",
    "print(Y_hot)\n",
    "#\n",
    "# Set up weights\n",
    "#\n",
    "W1 = np.random.randn(no_hidden, no_features + 1)\n",
    "W2 = np.random.randn(no_classes, no_hidden + 1)\n",
    "#\n",
    "# ReLU activation (could try leaky ReLU later)\n",
    "#\n",
    "Z1 = np.matmul(W1, X)\n",
    "F1 = copy.deepcopy(Z1)\n",
    "F1[F1 < 0] = 0\n",
    "#\n",
    "# Add bias to F1\n",
    "#\n",
    "F1_with_bias = np.concatenate((np.ones((1, no_samples)), F1), axis=0) \n",
    "#\n",
    "# Softmax activation with stabilising adjustment\n",
    "#\n",
    "Z2 = np.matmul(W2, F1_with_bias)\n",
    "#\n",
    "C     = np.max(Z2, axis=0)\n",
    "C_adj = np.atleast_2d(C).repeat(repeats=(no_classes), axis=0)\n",
    "Z2_adj = Z2 - C_adj\n",
    "#\n",
    "eZ2      = np.exp(Z2_adj)\n",
    "eZ2_sums = np.sum(eZ2, axis=0)\n",
    "#\n",
    "print(\"\\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "F2 = eZ2 / eZ2_sums\n",
    "print(F2)\n",
    "#\n",
    "# Confusion matrix and accuracy\n",
    "#\n",
    "P_ints = np.argmax(F2, axis=0)\n",
    "P      = label_encoder.inverse_transform(P_ints)\n",
    "#\n",
    "print(\"\\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "print(P)\n",
    "print(Y)\n",
    "#\n",
    "cm   = confusion_matrix(Y, P)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,  display_labels=classes)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "#\n",
    "print(\"Accuracy:\", accuracy_score(Y, P))\n",
    "#\n",
    "# Gradient from Softmax\n",
    "#\n",
    "G2 = np.matmul((F2 - np.transpose(Y_hot)), np.transpose(F1_with_bias)) / no_samples\n",
    "#\n",
    "print(\"\\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "print(G2)\n",
    "print(G2.shape)\n",
    "#\n",
    "# Gradient from hidden (don't need the bias gradients, hence the [:,1:])\n",
    "#\n",
    "# G1a = del_L / del_F2 * del_F2 / del_Z2 * del_Z2 / del_F1\n",
    "#\n",
    "G1a = np.matmul(np.transpose(F2 - np.transpose(Y_hot)), W2)[:,1:]\n",
    "G1a = np.transpose(G1a)\n",
    "#\n",
    "del_ReLU         = np.zeros(np.shape(G1a))\n",
    "del_ReLU[F1 > 0] = 1\n",
    "#\n",
    "# G1a * del_F1 / del_Z1 * del_Z1 / del_W1\n",
    "\n",
    "# Backpropagation of ReLU is element wise\n",
    "#\n",
    "G1 = np.matmul(np.multiply(G1a, del_ReLU), np.transpose(X))\n",
    "#\n",
    "print(\"\\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "print(G1)\n",
    "print(G1.shape)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee20ade4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Show shapes of principal data structures\n",
    "#\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"Y shape:\", Y.shape)\n",
    "print(\"Y_hot shape:\", Y_hot.shape)\n",
    "print(\"W1 shape:\", W1.shape)\n",
    "print(\"F1 shape:\", F1.shape)\n",
    "print(\"F1_with_bias shape:\", F1_with_bias.shape)\n",
    "print(\"Z1 shape:\", Z1.shape)\n",
    "print(\"Z2 shape:\", Z2.shape)\n",
    "print(\"W2 shape:\", W2.shape)\n",
    "print(\"F2 shape:\", F2.shape)\n",
    "print(\"G1 shape:\", G1.shape)\n",
    "print(\"G2 shape:\", G2.shape)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a6bfce",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "\n",
    "*Can you use the data in the numpy arrays computed above to compute the value of the loss function for each sample?*\n",
    "\n",
    "*You can use the next cell to figure out your answer, and the cell below that to get it checked*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8851bfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Use this cell to work out your answer\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c87cf7",
   "metadata": {},
   "source": [
    "*Change `myL` in the function call, to the variable you have created, containing the loss value for each sample. This is expected to be a numpy array, it can be either a vector of length `no_samples`, or a matrix with shape `(1, no_samples)`. The loss values are expected to be in the same sample sequence as the other sample related data objects*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b506703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Change myL to your variable, and then run the function to check your result\n",
    "#\n",
    "sr.check_softmax_relu_loss(Y_hot, F2, L=myL)\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
