{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36404fb1",
   "metadata": {},
   "source": [
    "# Matrix operations for a multi-class Softmax classifier\n",
    "\n",
    "This notebook illustrates the matrix operations for the forward and backwards passes for a multinomial classifier, using the softmax activation function, and the multi-class cross-entropy loss function\n",
    "\n",
    "There is a unit in the output layer for each possible class, the units are \"one-hotted\" to yield 0 if the class is incorrect and 1 if it is correct\n",
    "\n",
    "There is also a quiz question, for you to figure out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04448a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This cell imports functions that are used later on\n",
    "#\n",
    "!wget -nv https://github.com/IS-pillar-3/A_AI_anc/raw/main/A_AI_softmax_loss_01_v01.py\n",
    "import A_AI_softmax_loss_01_v01 as ml\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d95d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Simulate forward and backward pass for Softmax multi-class classifier\n",
    "#\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "#\n",
    "#\n",
    "# Properties of generated training set\n",
    "#\n",
    "no_classes  = 4\n",
    "no_features = 5\n",
    "no_samples  = 10\n",
    "#\n",
    "# Set up X training set, including a bias constant (1) as row 1\n",
    "#\n",
    "X = np.concatenate((np.ones((1, no_samples)), np.random.randn(no_features, no_samples)), axis=0)\n",
    "#\n",
    "# Set up one-hotted Y's in Y_hot training set\n",
    "#\n",
    "# Use lower case letters as clases\n",
    "#\n",
    "letters = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "classes = list(letters[0:no_classes])\n",
    "#\n",
    "Y = np.random.choice(classes, no_samples)\n",
    "#\n",
    "# One-hot encode Y (from machinelearningmastery.com)\n",
    "#\n",
    "# Fit a LabelEncoder model to encode classes as integers\n",
    "#\n",
    "label_encoder = LabelEncoder()\n",
    "integer_codes = label_encoder.fit_transform(classes)\n",
    "#\n",
    "# Fit a OneHotEncoder model to encode integers as one-hots\n",
    "#\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_codes  = integer_codes.reshape(no_classes, 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_codes)\n",
    "#\n",
    "# Use encocer models to one-hot encode Y\n",
    "#\n",
    "Y_ints = label_encoder.transform(Y)\n",
    "print(Y)\n",
    "print(Y_ints)\n",
    "#\n",
    "Y_ints = Y_ints.reshape(no_samples, 1)\n",
    "Y_hot  = np.array(onehot_encoder.transform(Y_ints))\n",
    "print(Y_hot)\n",
    "#\n",
    "# Set up weights\n",
    "#\n",
    "W = np.random.randn(no_classes, no_features + 1)\n",
    "#\n",
    "# Activation without stablising terms\n",
    "#\n",
    "Z       = np.matmul(W, X)\n",
    "eZ      = np.exp(Z)\n",
    "eZ_sums = np.sum(eZ, axis=0)\n",
    "#\n",
    "print(\"\\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "F = eZ / eZ_sums\n",
    "print(F)\n",
    "#\n",
    "# Activation with stabilising adjustment (as per medium.com)\n",
    "#\n",
    "C     = np.max(Z, axis=0)\n",
    "C_adj = np.atleast_2d(C).repeat(repeats=(no_classes), axis=0)\n",
    "Z_adj = Z - C_adj\n",
    "#\n",
    "eZ      = np.exp(Z_adj)\n",
    "eZ_sums = np.sum(eZ, axis=0)\n",
    "#\n",
    "print(\"\\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "F = eZ / eZ_sums\n",
    "print(F)\n",
    "#\n",
    "# Predictions, confusion matrix and accuracy\n",
    "#\n",
    "P_ints = np.argmax(F, axis=0)\n",
    "P      = label_encoder.inverse_transform(P_ints)\n",
    "#\n",
    "print(\"\\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "print(P)\n",
    "print(Y)\n",
    "#\n",
    "cm   = confusion_matrix(Y, P)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,  display_labels=classes)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "#\n",
    "print(\"Accuracy:\", accuracy_score(Y, P))\n",
    "#\n",
    "# Gradients\n",
    "#\n",
    "G = np.matmul((F - np.transpose(Y_hot)), np.transpose(X)) / no_samples\n",
    "#\n",
    "print(\"\\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "print(G)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee20ade4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Show shapes of principal data structures\n",
    "#\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"Y shape:\", Y.shape)\n",
    "print(\"Y_hot shape:\", Y_hot.shape)\n",
    "print(\"W shape:\", W.shape)\n",
    "print(\"Z shape:\", Z.shape)\n",
    "print(\"eZ shape:\", eZ.shape)\n",
    "print(\"eZ_sums shape:\", eZ_sums.shape)\n",
    "print(\"F shape:\", F.shape)\n",
    "print(\"C shape:\", C.shape)\n",
    "print(\"C_adj shape:\", C_adj.shape)\n",
    "print(\"G shape:\", G.shape)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a6bfce",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "\n",
    "*Can you use the data in the numpy arrays computed above to compute the value of the loss function for each sample?*\n",
    "\n",
    "*You can use the next cell to figure out your answer, and the cell below that to get it checked*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8851bfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Use this cell to work out your answer\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c87cf7",
   "metadata": {},
   "source": [
    "*Change `myL` in the function call, to the variable you have created, containing the loss value for each sample. This is expected to be a numpy array, it can be either a vector of length `no_samples`, or a matrix with shape `(1, no_samples)`. The loss values are expected to be in the same sample sequence as the other sample related data objects*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b506703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Change myL to your variable, and then run the function to check your result\n",
    "#\n",
    "ml.check_softmax_loss(Y_hot, F, L=myL)\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
