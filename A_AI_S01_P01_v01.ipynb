{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied AI for Health Research\n",
    "# Practical 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ikjLYcvCnACs"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yw/217_5q3n5lnf46sz86ph0jtw0000gn/T/ipykernel_88483/3823286883.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrunning_on_colab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "#\n",
    "# I modified this\n",
    "#\n",
    "running_on_colab = False\n",
    "#\n",
    "if running_on_colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9PySGSUUm3K"
   },
   "source": [
    "# Lecture 1: The inner workings of neural networks\n",
    "\n",
    "## The relationship between artifical and biological neurons\n",
    "\n",
    "The basic biological unit of brain computation is a neuronal cell (see figure below). Each cell is essentially an electrical device that receives signals at the dendrites (surrounding the cell body) and, once these exceed a threshold, transmits the signal down the cell axon to synaptic terminals, where it then connects with other cells.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1i3nntiP9pWUCyFNfZGPL-Rvg-8EYXftg\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "Artificial neurons are designed to mimic this process, but only in a limited or constrained way. The idea is that the modulatory effect of the different synaptic connections is modelled through linear multiplication  of input signals $x_i$ with weights $w_i$, plus a bias term which shifts the centre of the prediction from zero. If the final sum is high enough, modelled through an nonlinear activation function $f$, the artificial neuron is allowed to ‘fire’ in response to that pattern of activation. \n",
    "\n",
    "Then, the optimisation process of neural networks involves learning optimal weights and biases to perform a given task. \n",
    "\n",
    "Note, while it can be useful to explain how the design of artifical neurons was motivated by the concept of biological networks, they remain far from a close simulation of the true thing. In truth design of many components of the most common artificial networks used for computer vision today are more inspired by engineering choices than any true goal to replicate human brain function. Biologically inspired neural networks  exist but, to an extent, they are a are a separate and  distinct line of research.\n",
    "\n",
    "For more details, and the source which inspired the neuron diagram, see the explanation from [CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/neural-networks-1/)\n",
    "\n",
    "## A Single Neuron Logistic Regression Classifier\n",
    "\n",
    "To understand how neural networks work, it serves to consider a single neuron as a logistic regression classifier:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1bz_hR949l986NLChyme1tlD4gyg1VoF4\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "Here the line $z =0$ defines a separating hyperplane, where the bias term $w_0$ has shifted this from the origin, and all data points with $z >0$ are assigned to the positive class, and all data points with $z < 0$ are assigned the negative class.\n",
    "\n",
    "The vector $\\mathbf{W}$ runs perpendicular to the line $z =0$ and defines the direction in which data classes are maximally separated when projected onto it. \n",
    "\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1PX-napPbTEAVAhe2zwoukRNbjoAZbk4p\" alt=\"Drawing\" style=\"width: 100px;\"/>\n",
    "\n",
    "## Exercise 1: Implementing a single neuron classifier through logistic regression\n",
    "\n",
    "In this section we will train a classifier to predict if a neonate is preterm based on volume measures of 86 brain volumes. We will code up and train a logistic regression classifier from scratch.\n",
    "\n",
    "### Import the data\n",
    "The data are in the file \"prem_vs_termwrois.pkl\". The final column indicates whether each data set was collected from a term or preterm baby (scanned at term equivalent age). The data represent mean vales of three different types of cortical imaging data: cortical thickness, cortical folding and cortical myelination, all averaged within 100 regions of interest ROIS on the surface. This gives 300 features in total. \n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1ZbAn0R_ihQ4DCe1XyKaHIRZSvUQv3puh\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "There are 101 babies, 50 terms and 51 preterms. The code below loads the file and splits the data randomly into a train and test set. The data is transposed such that the rows reflect features and the columns examples (as expected from the lectures notation). A row of ones is added to each dataset to allow model;ling of the bias term.\n",
    "\n",
    "**To Do** upload the data file to your local Google Drive, update the ```file_path``` accordingly and run the below code cell.\n",
    "\n",
    "**Be sure to understand what each line is doing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "j303w0jSUm3M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of X is  (301, 90) (101, 300)\n",
      "Dimension of y is  (1, 90)\n",
      "Number of features 300\n",
      "Number of examples 90\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# STUDENTS CODE HERE - UPDATE THE PATH TO CORRESPOND TO WHERE YOU HAVE UPLOADED prem_vs_termwrois.pkl TO YOUR DRIVE #\n",
    "if running_on_colab:\n",
    "    file_path = '/content/drive/My Drive/Colab Notebooks/AdvancedML/2021/01_fundamentals/prem_vs_termwrois.pkl'\n",
    "else:\n",
    "    file_path = \"prem_vs_termwrois.pkl\"\n",
    "#\n",
    "# Read the data\n",
    "df = pd.read_pickle(file_path)\n",
    "data = df.values[:,:-2]\n",
    "y = df.values[:,-1]\n",
    "\n",
    "# create a test and train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Create feature matrix\n",
    "X_train = X_train.T\n",
    "X_test=X_test.T\n",
    "\n",
    "#reshape y to (1 x n_T) matrix\n",
    "y_train=np.expand_dims(y_train, axis=0)\n",
    "y_test=np.expand_dims(y_test, axis=0)\n",
    "\n",
    "\n",
    "# add a row of ones for multiplication with bias term\n",
    "X_train = np.concatenate((np.ones((1,X_train.shape[1])),X_train))\n",
    "X_test = np.concatenate((np.ones((1,X_test.shape[1])),X_test))\n",
    "\n",
    "# set variables for numbers of feature and examples to improve readabiity of code\n",
    "n_features=X_train.shape[0]-1\n",
    "n_examples=X_train.shape[1]\n",
    "\n",
    "print('Dimension of X is ', X_train.shape,data.shape)\n",
    "print('Dimension of y is ', y_train.shape)\n",
    "\n",
    "print('Number of features', n_features)\n",
    "print('Number of examples', n_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# I put this cell in so I could see the data\n",
    "#\n",
    "# NB: Need to be a bit careful here since raw data has not been transposed to [p, n]\n",
    "#\n",
    "df.to_csv(\"prem_vs_termwrois.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(301, 90)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# I put this cell in\n",
    "#\n",
    "X_train.shape\n",
    "#\n",
    "# So it comes out as 301, 90 which is [p, n]\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "of0GyW9rUm3Q"
   },
   "source": [
    "### Eyeball the data\n",
    "The following code plots histograms of a single feature (brain volume) for preterms vs terms. Run the code for a few different values of 'feature'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-Ydk_R3yUm3R"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEGCAYAAABvtY4XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAaCElEQVR4nO3de5hU9Z3n8feHawMiJtiZMZJOg+MFEURomCREwSteMjhjHE3YZMdEwjqbeJtoQqI7Ibs7+7g7TpRxfFQmF41RZyJenkTHiMmIJo9GbgI2oqixje1lRWbFSxBp/O4f5zRWN32prqrTVX34vJ6nHk7Vufy+9aP59OFXp35HEYGZmeXPoGoXYGZm2XDAm5nllAPezCynHPBmZjnlgDczy6kh1S6g0P777x+NjY3VLsPMbMBYs2bN6xFR39W6mgr4xsZGVq9eXe0yzMwGDEkvdLfOQzRmZjnlgDczyykHvJlZTtXUGLyZ7X127txJa2sr7777brVLqWl1dXWMGzeOoUOHFr2PA97Mqqq1tZXRo0fT2NiIpGqXU5Migq1bt9La2sr48eOL3s9DNGZWVe+++y5jx451uPdAEmPHju3z/3IyDXhJF0vaKKlZ0m2S6rJsz8wGJod770rpo8wCXtKBwAVAU0QcAQwGPpdVe2Zm1lHWY/BDgBGSdgIjgZczbs/MBrjGRfdW9HgtV5zW4/qtW7dy/PHHA/Dqq68yePBg6uuTL4auXLmSYcOGVbSe/pRZwEfES5KuBH4PbAeWR8TyzttJWggsBGhoaMiqHDOg+/DoLQQsv8aOHcu6desAWLx4Mfvssw+XXHJJr/u1tbUxZEhtX6eS5RDNh4DTgfHAR4FRkr7QebuIWBoRTRHR1P5b08ysmtasWcPs2bOZPn06c+fO5ZVXXgFgzpw5fPvb32b27NksWbKEOXPmcPHFF3PMMccwceJEVq1axRlnnMHBBx/M5ZdfXuV3ke2HrCcAz0fElojYCdwJfCrD9szMyhYRnH/++Sxbtow1a9bw5S9/mcsuu2z3+jfeeIOHHnqIr3/96wAMGzaMhx9+mPPOO4/TTz+da6+9lubmZm688Ua2bt1arbcBZDsG/3vgE5JGkgzRHA94JjEzq2k7duygubmZE088EYBdu3ZxwAEH7F5/9tlnd9h+3rx5AEyePJlJkybt3nbChAm8+OKLjB07tp8q31OWY/CPSVoGrAXagMeBpVm1Z2ZWCRHBpEmTePTRR7tcP2rUqA7Phw8fDsCgQYN2L7c/b2try67QImR6HXxEfCciDouIIyLiixGxI8v2zMzKNXz4cLZs2bI74Hfu3MnGjRurXFVpavsjYDPb61T7iqZBgwaxbNkyLrjgArZt20ZbWxsXXXQRkyZNqmpdpVBEVLuG3ZqamsI3/LAs+TLJ2rNp0yYmTpxY7TIGhK76StKaiGjqanvPRWNmllMOeDOznHLAm5nllAPezCynHPBmZjnlgDczyylfB29mtWXxmAofb1uvmwwePJjJkyfT1tbGxIkTuemmmxg5cmRRh29paeGRRx5h/vz55VZacT6DN7O93ogRI1i3bh3Nzc0MGzaM66+/vsP6Xbt2dbtvS0sLt956a5/a6+l4leSANzMrcPTRR/Pss8+yYsUKjj32WObPn8/kyZPZtWsXl156KTNmzGDKlCnccMMNACxatIhf//rXTJ06lauuuqrb7Tofb8WKFcyePZuzzjqLQw45hEWLFnHLLbcwc+ZMJk+ezHPPPVf2e/EQjZlZqq2tjfvuu4+TTz4ZSO7o1NzczPjx41m6dCljxoxh1apV7Nixg1mzZnHSSSdxxRVXcOWVV3LPPfcAdLtd5+OtWLGC9evXs2nTJj784Q8zYcIEFixYwMqVK1myZAnXXHMNV199dVnvxwFvZnu97du3M3XqVCA5gz/33HN55JFHmDlzJuPHjwdg+fLlbNiwgWXLlgGwbds2nnnmmT1u6dfTdoXHA5gxY8bu6YUPOuig3b8IJk+ezIMPPlj2+3LAm9ler30MvrPCqYEjgmuuuYa5c+d22GbFihUdnve0XXdTDUPH6YYrNdWwx+DNzIowd+5crrvuOnbu3AnA5s2beeeddxg9ejRvvfVWr9tVg8/gzay2FHFZYzUsWLCAlpYWpk2bRkRQX1/P3XffzZQpUxgyZAhHHnkk55xzDhdeeGGX21VDZtMFSzoU+NeClyYAfxsR3X5q4OmCLWueLrj2eLrg4vV1uuAsb9n3NDA1LWAw8BJwV1btmZlZR/01Bn888FxEvNBP7ZmZ7fX6K+A/B9zWT22Z2QBTS3eWq1Wl9FHmAS9pGDAPuL2b9QslrZa0esuWLVmXY2Y1pq6ujq1btzrkexARbN26lbq6uj7t1x9X0ZwCrI2I/9vVyohYCiyF5EPWfqjHzGrIuHHjaG1txSd4Paurq2PcuHF92qc/Av7zeHjGzLoxdOjQDt/utMrJdIhG0kjgRODOLNsxM7M9ZXoGHxF/AMZm2YaZmXXNUxWYmeWUA97MLKcc8GZmOeWANzPLKQe8mVlOOeDNzHLKAW9mllMOeDOznHLAm5nllAPezCynHPBmZjnlgDczyykHvJlZTjngzcxyygFvZpZTDngzs5xywJuZ5VTWt+zbT9IySU9J2iTpk1m2Z2ZmH8j6pttLgF9ExJmShgEjM27PzMxSmQW8pH2BY4BzACLiPeC9rNozM7OOshyimQBsAX4k6XFJ35c0qvNGkhZKWi1p9ZYtWzIsx8xs75JlwA8BpgHXRcRRwDvAos4bRcTSiGiKiKb6+voMyzEz27tkGfCtQGtEPJY+X0YS+GZm1g8yC/iIeBV4UdKh6UvHA09m1Z6ZmXWU9VU05wO3pFfQ/A74UsbtmZlZKtOAj4h1QFOWbZiZWdf8TVYzs5xywJuZ5ZQD3swspxzwZmY55YA3M8spB7yZWU454M3McsoBb2aWUw54M7OccsCbmeWUA97MLKcc8GZmOeWANzPLKQe8mVlOOeDNzHLKAW9mllMOeDOznMr0jk6SWoC3gF1AW0T47k5mZv2kqICXdERENJfYxrER8XqJ+5qZWYmKHaK5XtJKSf9V0n6ZVmRmZhVR1Bl8RHxa0sHAl4HVklYCP4qIB3rbFVguKYAbImJp5w0kLQQWAjQ0NPSpeLNuLR7T5cstdR2fN757az8UY1YdRX/IGhHPAJcD3wRmA/8o6SlJZ/Sw26yImAacAnxV0jFdHHdpRDRFRFN9fX0fyzczs+4UFfCSpki6CtgEHAf8WURMTJev6m6/iHg5/fM14C5gZtkVm5lZUYo9g/8nYC1wZER8NSLWwu4Av7yrHSSNkjS6fRk4CSj1g1ozM+ujYi+TPBXYHhG7ACQNAuoi4g8RcXM3+/wRcJek9nZujYhflFuwmZkVp9iA/yVwAvB2+nwksBz4VHc7RMTvgCPLqs7MzEpW7BBNXUS0hzvp8shsSjIzs0ooNuDfkTSt/Ymk6cD2bEoyM7NKKHaI5iLgdkkvp88PAM7OpiQzM6uEYr/otErSYcChgICnImJnppWZmVlZ+jLZ2AygMd3nKElExI8zqcrMzMpW7GRjNwMHAetIZoaEZBoCB7yZWY0q9gy+CTg8IiLLYszMrHKKvYqmGfjjLAsxM7PKKvYMfn/gyXQWyR3tL0bEvEyqMjOzshUb8IuzLMLMzCqv2MskH5L0ceDgiPilpJHA4GxLMzOzchQ7XfBXgGXADelLBwJ3Z1WUmZmVr9gPWb8KzALehN03//hIVkWZmVn5ig34HRHxXvsTSUNIroM3M7MaVWzAPyTp28AISScCtwM/z64sMzMrV7EBvwjYAjwB/Bfg3+jmTk5mZlYbir2K5n3gn9OHmZkNAMXORfM8XYy5R8SEIvYdDKwGXoqIz/S5QjMzK0lf5qJpVwf8JfDhIve9ENgE7NuHuszMrExFjcFHxNaCx0sRcTVwXG/7SRoHnAZ8v8w6zcysj4odoplW8HQQyRn96CJ2vRr4Rk/bSloILARoaGgophzbmy0eU9HDtdTNT4/bW7vbKtrugFBsX++NfTNAFDtE8w8Fy21AC3BWTztI+gzwWkSskTSnu+0iYimwFKCpqcnX1puZVUixV9EcW8KxZwHzJJ1KMm6/r6SfRMQXSjiWmZn1UbFDNH/T0/qI+F4Xr30L+Fa6/xzgEoe7mVn/6ctVNDOAn6XP/wx4GHgxi6LMzKx8fbnhx7SIeAtA0mLg9ohYUMzOEbECWFFCfWZmVqJipypoAN4reP4e0FjxaszMrGKKPYO/GVgp6S6Sb7T+BfDjzKoyM7OyFXsVzd9Jug84On3pSxHxeHZlmZlZuYodogEYCbwZEUuAVknjM6rJzMwqoNhb9n0H+CbpZY/AUOAnWRVlZmblK/YM/i+AecA7ABHxMsVNVWBmZlVSbMC/FxFBOmWwpFHZlWRmZpVQbMD/VNINwH6SvgL8Et/8w8yspvV6FY0kAf8KHAa8CRwK/G1EPJBxbWZmVoZeAz4iQtLdETEdcKibmQ0QxQ7R/FbSjEwrMTOziir2m6zHAudJaiG5kkYkJ/dTsirMzMzK02PAS2qIiN8Dp/RTPWZmViG9ncHfTTKL5AuS7oiIz/ZHUWZmVr7exuBVsDwhy0LMzKyyegv46GbZzMxqXG9DNEdKepPkTH5EugwffMi6b6bVmZlZyXoM+IgYXOqBJdWR3NZveNrOsoj4TqnHMzOzvin2MslS7ACOi4i3JQ0FfiPpvoj4bYZtmplZKrOATycnezt9OjR9eBzfzKyfZHkGj6TBwBrgT4BrI+KxLrZZCCwEaGhoyLKc6lo8pg/bbsuujlrUl77JiyLfc+O7twLQcsVpFTvmXvfztRfryx2d+iwidkXEVGAcMFPSEV1sszQimiKiqb6+PstyzMz2KpkGfLuIeANYAZzcH+2ZmVmGAS+pXtJ+6fII4ATgqazaMzOzjrIcgz8AuCkdhx8E/DQi7smwPTMzK5DlVTQbgKOyOr6ZmfWsX8bgzcys/zngzcxyygFvZpZTDngzs5xywJuZ5ZQD3swspxzwZmY55YA3M8spB7yZWU454M3McsoBb2aWUw54M7OccsCbmeWUA97MLKcc8GZmOeWANzPLKQe8mVlOZXlP1o9JelDSJkkbJV2YVVtmZranLO/J2gZ8PSLWShoNrJH0QEQ8mWGbZmaWyuwMPiJeiYi16fJbwCbgwKzaMzOzjrI8g99NUiPJDbgf62LdQmAhQENDQ3+UU/sWjylyu21lN9W46N5u17VccVo37fZffbWiu37ao4+K7ZsitdTNT49buWP29HcOPfy9l2sv/Lmptsw/ZJW0D3AHcFFEvNl5fUQsjYimiGiqr6/Puhwzs71GpgEvaShJuN8SEXdm2ZaZmXWU5VU0An4AbIqI72XVjpmZdS3LM/hZwBeB4yStSx+nZtiemZkVyOxD1oj4DaCsjm9mZj3zN1nNzHLKAW9mllMOeDOznHLAm5nllAPezCynHPBmZjnlgDczyykHvJlZTjngzcxyygFvZpZTDngzs5xywJuZ5ZQD3swspxzwZmY55YA3M8spB7yZWU454M3McirLe7L+UNJrkpqzasPMzLqX5Rn8jcDJGR7fzMx6kFnAR8TDwH9kdXwzM+tZZjfdLpakhcBCgIaGhtIPtHhMkdttK72NHjQuurfL11uuOC2T9vqkh75pqetpv+zaHWha6uZ3vWJxv5ZREd2+l3aL+3a87n72d7dXC/8GilXpHKlyLlX9Q9aIWBoRTRHRVF9fX+1yzMxyo+oBb2Zm2XDAm5nlVJaXSd4GPAocKqlV0rlZtWVmZnvK7EPWiPh8Vsc2M7PeeYjGzCynHPBmZjnlgDczyykHvJlZTjngzcxyygFvZpZTDngzs5xywJuZ5ZQD3swspxzwZmY55YA3M8spB7yZWU454M3McsoBb2aWUw54M7OccsCbmeWUA97MLKcyDXhJJ0t6WtKzkhZl2ZaZmXWU5T1ZBwPXAqcAhwOfl3R4Vu2ZmVlHWZ7BzwSejYjfRcR7wL8Ap2fYnpmZFVBEZHNg6Uzg5IhYkD7/IvCnEfG1TtstBBamTw8Fnu5jU/sDr5dZbh64H9wH4D6Ava8PPh4R9V2tGJJho+ritT1+m0TEUmBpyY1IqyOiqdT988L94D4A9wG4DwplOUTTCnys4Pk44OUM2zMzswJZBvwq4GBJ4yUNAz4H/CzD9szMrEBmQzQR0Sbpa8D9wGDghxGxMYOmSh7eyRn3g/sA3AfgPtgtsw9ZzcysuvxNVjOznHLAm5nlVE0HfG9THSjxj+n6DZKmdVo/WNLjku7pv6orq5w+kLSfpGWSnpK0SdIn+7f6yiizDy6WtFFSs6TbJNX1b/WVUUQfHCbpUUk7JF3Sl30HilL7QNLHJD2Y/hvYKOnC/q28iiKiJh8kH8w+B0wAhgHrgcM7bXMqcB/JNfefAB7rtP5vgFuBe6r9fqrRB8BNwIJ0eRiwX7XfU3/2AXAg8DwwIn3+U+Ccar+njPrgI8AM4O+AS/qy70B4lNkHBwDT0uXRwOaB2AelPGr5DL6YqQ5OB34cid8C+0k6AEDSOOA04Pv9WXSFldwHkvYFjgF+ABAR70XEG/1ZfIWU9XNAcqXYCElDgJEMzO9i9NoHEfFaRKwCdvZ13wGi5D6IiFciYm26/BawieSXf+7VcsAfCLxY8LyVPf9SetrmauAbwPtZFdgPyumDCcAW4EfpMNX3JY3KstiMlNwHEfEScCXwe+AVYFtELM+w1qwU0wdZ7FtLKvI+JDUCRwGPVaSqGlfLAV/MVAddbiPpM8BrEbGm8mX1q5L7gOTMdRpwXUQcBbwDDMTx13J+Dj5EcpY3HvgoMErSFypcX38oatqPDPatJWW/D0n7AHcAF0XEmxWpqsbVcsAXM9VBd9vMAuZJaiH5r9xxkn6SXamZKacPWoHWiGg/U1lGEvgDTTl9cALwfERsiYidwJ3ApzKsNSvlTPuRlylDynofkoaShPstEXFnhWurWbUc8MVMdfAz4D+nV1F8guS/4K9ExLciYlxENKb7/XtEDMQzt3L64FXgRUmHptsdDzzZb5VXTsl9QDI08wlJIyWJpA829WfxFVLOtB95mTKk5PeR/t3/ANgUEd/LsMbaU+1PeXt6kFwdsZnk0/PL0tfOA85Ll0VyU5HngCeApi6OMYcBehVNuX0ATAVWAxuAu4EPVfv9VKEPvgs8BTQDNwPDq/1+MuqDPyY5y30TeCNd3re7fQfio9Q+AD5NMpyzAViXPk6t9vvpj4enKjAzy6laHqIxM7MyOODNzHLKAW9mllMOeDOznHLAm5nllAPeyiJpl6R1BY/GEo7x55IOr3x12ZLUImn/Ch1rajoT4sZ0RsyzC9YdJ2ltOiPmTem8Okj6kKS70u1XSjqi0zFvkDQrPfZv07+f1ZJmFmwzpaDdJwbqbJvWNV8maWWR9HZE7FPmMW4k+a7Csj7sMyQi2sppt1zpN6WbIuL1EvbtUL+kQ4CIiGckfRRYA0wkuab7BeD4iNgs6b8DL0TEDyT9PfB2RHxX0mHAtRFxfMEx1wHTSWbavCoi7pN0KvCNiJiT/qJYC3wxItZLGgu8ERG7SusRqzU+g7eKkzRd0kOS1ki6v2CGz69IWiVpvaQ70m+YfgqYB/x9eoZ5kKQVkprSffZPgxRJ50i6XdLPgeWSRkn6YXrMxyXtMUuipDnp8drnxb8l/WZjhzNwSU2SVqTLi9Mz5eXpNmdI+j/pGe4vlHztvd2l6dnzSkl/ku5fn76/VeljVsFxl0paDvy4sM6I2BwRz6TLLwOvAfXAWGBHRGxON30A+Gy6fDjwq3Sfp4BGSX+UtjUR2JyGdZB84QdgDB98xf8kYENErE+PsdXhni8OeCvXiILhmbvS8LsGODMipgM/JJmfG+DOiJgREUeSTBlwbkQ8QvKV80sjYmpEPNdLe58E/ioijgMuI5mGYgZwLMkvia5mzDwKuIgkECeQzFXUm4NIpps+HfgJ8GBETAa2p6+3ezMiZgL/RDKDKcASkjPmGSRhXDhl9XTg9IiY313D6RDKMJJvbL4ODG3/hQecyQdzsqwHzijY5+Mkc7QAnAL8Il2+iKRvXiSZXfNb6euHkEzKdn86BPSNXnvFBpQh1S7ABrztETG1/Uk6DnwE8EB6ojyYZKpegCMk/U9gP2Af4P4S2nsgIv4jXT6JZFK59rv31AEN7DnfzMqIaE3rWwc0Ar/ppZ37ImKnpCfS99Aelk+k+7e7reDPq9LlE4DD0/cPsK+k0enyzyJie3eNpv/buZnkl9j76WufA66SNBxYDrQP7VwBLEnf0xPA4wXr5gJfSpf/Grg4Iu6QdBbJvCwnkPz7/zTJTTL+APxK0pqI+FUvfWMDhAPeKk3Axojo6vaANwJ/no73nkMyT1BX2vjgf5edP/R7p1Nbn42Ip3upaUfB8i4++LnvqZ0dABHxvqSd8cGHVe/T8d9NdLE8CPhk5yBPA7+wfjqt3xe4F7g8khuXkNbwKHB0us1JJGfeRDLl7ZfS10Vy96rnJY0kuXtX+1DMXwHtt6m7nQ/+R9EKPNT+GYKkfyOZcdQBnxMeorFKexqoV3r/V0lDJU1K140GXkmHcf5TwT5vpevatZAMZUAyJNGd+4HzC8bUj+pjrYXtfLaH7XpydsGfj6bLy4GvtW8gaWrnnTpTMkPiXSR3prq907qPpH8OB74JXJ8+3y/dD2AB8HAa+scCDxYc4mVgdrp8HPBMunw/MCX9LGRIus1AnHHUuuGAt4qK5HZqZwL/W9J6kpn72udg/28kd9J5gGSGx3b/QvJh5eOSDiIZJ/5rSY8APV2G+D+AocAGSc3p8774LskQx69JzuxLMVzSYyRnyBenr10ANCm5fPFJkhkPe3MWyS0Wzyn4TKP9F8OlkjaRzIb484j49/T1icBGSU+RjLm3n6UXjr8DfAX4h/Tv438BCwEi4v8B3yOZincdsDYi7u3j+7ca5sskzXJG0lrgTyO5yYntxRzwZmY55SEaM7OccsCbmeWUA97MLKcc8GZmOeWANzPLKQe8mVlO/X+aIkP35lHrUwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#### STUDENT'S CODE HERE ####\n",
    "# try values between 0 and 300\n",
    "#feature = 150\n",
    "feature = 299\n",
    "\n",
    "plt.hist(X_train[feature+1, y_train[0]==0], bins=30)\n",
    "plt.hist(X_train[feature+1, y_train[0]==1], bins=30)\n",
    "plt.xlabel('Feature number {}/86'.format(feature))\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(['Term','Preterm']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QExCDN2aUm3T"
   },
   "source": [
    "### Model\n",
    "\n",
    "Our predictions for a single logistric regression classifier may be written:\n",
    "$$ f= f(z) = \\dfrac{1}{1+e^{-z}} $$\n",
    "\n",
    "Where, for $f$ is the sigmoid function and $$z$$ is the linear transform of $\\mathbf{x}$ by $\\mathbf{w}$:\n",
    "\n",
    "$$z=w_0 + w_1x_1 + w_2x_2 +w_3x_3....+w_m x_m$$\n",
    "\n",
    "Here $w_0$ is the bias term, $w_1,w_2....w_m$ are the weights;, $m$ is the number of features and $\\mathbf{x}$ is a single example (i.e. one column) from our training set $X \\in \\mathbb{R}^{m\\times n}$ .\n",
    "\n",
    "### Implementation of the forward pass\n",
    "\n",
    "We could calculate $f$ in one line of code, but it will come in handy when considering backpropagation later to consider the computation in stages, with each stage consisting of a simple module:\n",
    "\n",
    "$$\n",
    "\\begin{align} \n",
    "\\mathbf{Z} &= \\mathbf{W} \\mathbf{X} \\\\\n",
    "\\mathbf{F}=f(\\mathbf{Z}) &= \\dfrac{1}{1+e^{-\\mathbf{Z}}} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Implemented using vectorisation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNYIH2_LUm3V"
   },
   "source": [
    "### Task 1.1 Initialise $\\mathbf{W}$\n",
    "\n",
    "**To do** Create a matrix of zeros to initialise $\\mathbf{W}$ (note initialisation by zero is ok for a single neuron). \n",
    "\n",
    "- If $\\mathbf{X}$ has shape $(m_{features} \\times n_{examples})$, and we know that $\\mathbf{Z}$ (and thus $\\mathbf{F}$) should return _one_ scalar prediction _per example_, what shape should $\\mathbf{W}$ be? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "mBfjieGBUm3V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(301,)\n"
     ]
    }
   ],
   "source": [
    "#### STUDENT'S CODE HERE ####\n",
    "#\n",
    "# Both work, I suppose the vector version is a bit cleaner - i.e. its a column vector\n",
    "# whereas [1, 301] is more accurate in principal and creates a row vector (i.e. 1 row many columns)\n",
    "#\n",
    "#W = np.zeros([1, 301], dtype=float)\n",
    "W = np.zeros(301, dtype=float)\n",
    "# Answer:\n",
    "print(W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.45318122e-01  4.42462664e-01 -2.44283383e-01 -2.65698001e-01\n",
      " -2.57126901e-01 -3.44011537e-01  8.29151686e-03  3.04658973e-01\n",
      " -2.87009817e-01  4.52241573e-01 -4.17763347e-01 -3.35829910e-02\n",
      "  6.54160177e-02  2.35284320e-01  3.10059555e-01 -4.95418097e-02\n",
      " -1.75625986e-01 -2.57206664e-01 -2.65487306e-01  2.77631188e-01\n",
      "  1.31958482e-02  2.69302673e-01  6.01958106e-02 -3.41887973e-01\n",
      "  1.92822069e-01 -7.96566821e-02 -7.05670176e-02  5.84003259e-02\n",
      "  1.89484943e-01  3.82645344e-01  2.83112874e-01 -3.28666262e-01\n",
      " -3.34172971e-01  2.23095538e-02 -3.70338340e-01  4.86441639e-01\n",
      " -4.82935834e-01  8.35553935e-03  1.80871372e-01 -4.90956315e-01\n",
      "  2.01072502e-01  3.82456602e-02  4.26645495e-01  4.43930498e-01\n",
      " -7.32963877e-02  4.45960281e-01 -2.93106876e-01 -1.30445575e-01\n",
      "  1.21617148e-01 -1.56332285e-01 -6.62325273e-02  4.30661680e-01\n",
      " -8.01745630e-03 -3.70657567e-01  4.41389654e-02  3.05838684e-01\n",
      " -1.35453517e-04 -3.06295726e-01 -1.97559850e-01 -2.68453690e-01\n",
      "  7.27505548e-02  1.88202256e-02 -7.60216287e-02 -1.42803143e-01\n",
      "  1.21262115e-01 -3.10785226e-01  4.70861715e-03 -2.76171243e-01\n",
      " -2.45863124e-01  4.45738372e-01  1.27658973e-01 -2.97415810e-01\n",
      "  1.82984830e-01 -2.06699216e-01 -1.87410942e-01 -1.55597593e-01\n",
      " -3.52254588e-01 -4.92352758e-01 -3.96778322e-01 -1.65852068e-01\n",
      "  3.60072032e-01 -4.91667433e-01  2.73004877e-01  3.96188780e-01\n",
      "  8.04879216e-02  4.30338283e-01 -2.13106132e-01  1.39789032e-01\n",
      "  2.77168768e-01 -1.08111320e-01  1.56679028e-01 -3.55303537e-01\n",
      " -3.42451416e-01  2.40473091e-01  6.13508839e-02  3.20014308e-01\n",
      " -2.18148626e-01  7.85831774e-02  2.06493490e-01 -1.72580245e-02\n",
      "  4.81731328e-02 -3.80495368e-01  3.56908475e-01 -3.22304061e-02\n",
      "  3.43363861e-01 -3.20717104e-01  4.93464416e-01 -7.99672116e-02\n",
      " -2.32466158e-01 -2.42964240e-01  4.99066657e-01 -4.27699327e-01\n",
      " -4.01725345e-01 -2.19551569e-01 -2.18638489e-01  3.33352818e-01\n",
      "  8.42650721e-02  1.64740251e-01 -1.50125899e-01  2.19468505e-01\n",
      "  4.36267371e-02 -1.70037323e-01  2.79565296e-01  3.12148490e-01\n",
      "  1.04760578e-02 -2.17357586e-01 -1.85571209e-01  4.22775859e-01\n",
      " -3.64729782e-01  2.48523701e-01 -4.30991883e-01 -4.91753364e-01\n",
      "  3.09977351e-01  2.07739256e-01 -1.11953537e-01 -2.21742170e-01\n",
      "  2.13593864e-01  1.06425806e-01  4.08033666e-01 -3.32846807e-01\n",
      "  6.61346425e-02  4.96464181e-01 -3.47796822e-01  2.57587446e-01\n",
      "  9.44389772e-02  3.12511822e-01  4.36272568e-01 -2.73328423e-01\n",
      " -2.30002665e-02  2.53761915e-01 -2.02451617e-01 -3.03866105e-01\n",
      "  2.29841592e-01  2.58564122e-01  2.17361699e-01  3.85014997e-01\n",
      "  4.21772771e-01  1.21699454e-01 -1.26914639e-01  2.99319137e-01\n",
      " -1.70548048e-01  4.71984102e-01  2.87734721e-01 -3.66221911e-02\n",
      " -1.16464975e-01 -2.03447276e-01  3.13144365e-01 -3.66262375e-01\n",
      "  1.16696035e-01  9.08332075e-02  9.86290833e-02  1.58581803e-01\n",
      " -4.89483702e-01 -2.39928900e-01 -1.93059624e-01 -2.63120738e-01\n",
      " -8.56195630e-02  1.83920555e-01 -9.62664556e-02  1.35000781e-01\n",
      " -2.94417521e-01 -1.97911696e-01 -3.73664454e-01 -1.74388647e-01\n",
      " -4.94295774e-01  7.88989414e-02  3.06703129e-03  1.22380246e-01\n",
      " -1.48798802e-01 -2.38489981e-01 -1.35402841e-01  3.13789638e-01\n",
      " -3.31762820e-01  2.80862289e-01 -4.38872981e-01  3.21267245e-01\n",
      " -1.24777605e-01 -4.01940631e-01 -1.22199500e-01 -1.06580737e-01\n",
      " -4.91534748e-01  4.71746423e-01  2.60360691e-01 -2.00083464e-01\n",
      "  8.47031690e-03 -1.85043447e-01 -2.45421287e-01 -3.77006442e-01\n",
      "  1.73486147e-01  6.04231606e-02 -1.72369317e-01 -2.47874139e-01\n",
      " -2.22969921e-02 -1.75093194e-01  1.75319919e-01 -3.09264412e-01\n",
      " -4.62627049e-01 -3.29566218e-01 -1.50632931e-01 -2.21992779e-02\n",
      " -2.54565058e-01 -3.06407190e-01  4.05126799e-01 -4.81540467e-01\n",
      "  3.27833861e-01  4.34374240e-01 -4.19489566e-02 -4.18729271e-01\n",
      "  4.44341948e-01  7.52926590e-02 -2.38651034e-02  1.07033118e-02\n",
      " -3.88106437e-01  8.67465608e-02 -9.18230664e-03 -3.31572440e-01\n",
      "  4.33982678e-01 -2.52701595e-01  3.47553841e-01 -1.98152938e-02\n",
      "  2.69042235e-02 -1.79302701e-01  1.01981126e-01  3.87581410e-01\n",
      "  4.80849868e-01 -1.11502236e-01  9.94707005e-02 -2.64171805e-01\n",
      " -4.89734453e-01  2.57939801e-01 -1.73550326e-01 -1.04063828e-01\n",
      " -3.25498085e-01  2.38683409e-01 -1.17592510e-01  1.48482456e-01\n",
      " -2.65592564e-01  4.01855006e-01  1.24588745e-01 -2.17612693e-01\n",
      " -4.63632661e-01 -2.30820307e-01 -2.36725150e-01 -9.09802631e-02\n",
      "  2.70739270e-01  2.06504292e-01  3.52955353e-01  6.57327742e-02\n",
      " -1.37834951e-01 -5.01338466e-03  1.12930286e-01 -5.06862268e-02\n",
      "  4.95949760e-02  3.80641716e-01 -4.55720734e-01  7.32740714e-02\n",
      " -3.33187561e-01 -8.83292172e-02  1.94482193e-01 -1.09219196e-01\n",
      "  3.50749471e-01  2.57233769e-01  3.60548379e-01  1.15683610e-01\n",
      " -2.05088312e-01  2.27717056e-01  2.05328997e-01 -2.05130334e-01\n",
      "  4.57710793e-01 -7.42700834e-02  4.00140847e-01 -3.62422384e-01\n",
      "  1.84815229e-01  7.59839690e-03  2.44455018e-01  4.90615241e-01\n",
      " -2.04660592e-01  4.26016145e-01 -1.17020217e-01 -4.83845086e-01\n",
      " -4.07292100e-01]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# I'm just making it a bit more interesting with non-zero weights\n",
    "#\n",
    "W = np.random.rand(301) - 0.5\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "re2-XSGq7KZZ"
   },
   "source": [
    "### Task 1.2 Estimate $\\mathbf{Z}$: \n",
    "\n",
    "**To do** Write a function $z(w,x)$ that uses vectorisation to linearly transform data matrix $\\mathbf{X}$ using the weights matrix $\\mathbf{W}$.\n",
    "\n",
    "**Hint** implement $\\mathbf{Z} = \\mathbf{W} \\mathbf{X}$; print out the shape - is it what you would expect?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "9ziYkuJOUm3T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90,)\n"
     ]
    }
   ],
   "source": [
    "# task 1.2 complete function to calculate Z (can be done in one line in the return statement)\n",
    "def z(w,x):\n",
    "    #### STUDENT'S CODE HERE ####\n",
    "    return np.matmul(w, x)\n",
    "\n",
    "Z = z(W,X_train)\n",
    "print(Z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JPKqUVuUm3Y"
   },
   "source": [
    "### Task 1.3 Implement Sigmoid function f: \n",
    "\n",
    "**To do** Now write a function to compute $f(\\mathbf{Z})=\\dfrac{1}{1+e^{-\\mathbf{Z}}} $, our logistic regression function:\n",
    "\n",
    "**Hint** don't forget to implement with numpy functions - to support vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08308693 0.07902029 0.10193474 0.07863095 0.08823107 0.07440353\n",
      " 0.15361047 0.09768775 0.0927298  0.12646898 0.129329   0.16020196\n",
      " 0.1320178  0.12446809 0.07905117 0.08344038 0.1082308  0.09059476\n",
      " 0.09573213 0.10990414 0.12086946 0.08952739 0.0692495  0.08357295\n",
      " 0.07381796 0.12946158 0.12628266 0.11815829 0.10094735 0.09803785\n",
      " 0.10991909 0.12360761 0.04733751 0.10994599 0.08816454 0.14016862\n",
      " 0.08792181 0.05296529 0.12437959 0.09123498 0.14383051 0.12178138\n",
      " 0.06643256 0.0811126  0.11108567 0.06086472 0.07254336 0.12315793\n",
      " 0.08841809 0.0731495  0.11069117 0.08618878 0.11887805 0.08222868\n",
      " 0.0972968  0.10436327 0.09005629 0.05917612 0.14301112 0.13289479\n",
      " 0.08641858 0.10688128 0.07060787 0.08203876 0.07345461 0.14922311\n",
      " 0.07100118 0.11790478 0.07793559 0.09000051 0.11111214 0.14239484\n",
      " 0.11015949 0.14142798 0.10454557 0.11202916 0.08677183 0.03256962\n",
      " 0.09105923 0.11208129 0.10151022 0.04834533 0.11406087 0.13193918\n",
      " 0.09029769 0.12121154 0.09880953 0.07572543 0.08323477 0.08496985]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "#\n",
    "print(sigmoid(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "zqLM6S0gUm3Y"
   },
   "outputs": [],
   "source": [
    "# task 1.3 implement sigmoid function with vectorisation (in one line in the return statement)\n",
    "def f(z):\n",
    "  #### STUDENT'S CODE HERE ####\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXCKUvBPUm3a"
   },
   "source": [
    "**To do** Verify your softmax looks right by running this plotting code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "r4SHs64oUm3a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs = np.linspace(-10,10)\n",
    "outputs = f(inputs)\n",
    "plt.plot(inputs, outputs)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('sigmoid(z)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WeqRIOUUm3c"
   },
   "source": [
    "We're now in a position to compute some predictions $\\mathbf{\\hat{y}}$ (**run below code cells**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "0c2TznnyUm3c"
   },
   "outputs": [],
   "source": [
    "y_pred = f(z(W,X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjQBHdE6Um3e"
   },
   "source": [
    "**To do** Are these predictions any good? Let's take a look at the accuracy (run below cell):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Uoj4OTlrUm3g"
   },
   "outputs": [],
   "source": [
    "def accuracy(y, y_pred, threshold = 0.5):\n",
    "    y_pred_thresholded = y_pred > threshold\n",
    "    correct_predictions = np.sum(y==y_pred_thresholded) \n",
    "    total_predictions = y.shape[1]\n",
    "    accuracy = 100 * correct_predictions / total_predictions\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "43qXm4ofUm3i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.77777777777778\n"
     ]
    }
   ],
   "source": [
    "y_pred = f(z(W, X_train))\n",
    "print(accuracy(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DQzn0WRUm3k"
   },
   "source": [
    "Look at the predictions ```y_pred```, what does this initial prediction return and why? **Enter your answer in the box below**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d52gogye6n2O"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aVEpgDgPUm3n"
   },
   "source": [
    "### Task 1.4 Implement Cross Entropy Loss:\n",
    "\n",
    "Accuracy is easy to intepret, but can't be optimised using gradient descent. We need a measure of our prediction quality that can be. A typical loss function used in  classification problems is cross-entropy:\n",
    "\n",
    "$$L(y_i,f(z_i)) = - y_i \\ln(f(z_i)) - (1-y_i) \\ln(1-f(z_i))$$\n",
    "\n",
    "This may be implemented using vectorisation as:\n",
    "\n",
    "$$L(\\mathbf{Y},\\mathbf{F}) = - \\mathbf{Y} \\ln(\\mathbf{F} + \\epsilon) - (1-\\mathbf{Y}) \\ln(1-\\mathbf{F} + \\epsilon)$$\n",
    "\n",
    "This returns a vector of losses $(L_1,L_2....L_n)$ estimated for all training examples n. The $\\epsilon$ is added for numerical stability. We require the total cost estimated as:\n",
    "\n",
    "$$ J(\\mathbf{W})= \\frac{1}{n} \\sum_i L_i(y_i,f(z_i)) $$\n",
    "\n",
    "**To do Implement the Cross-Entropy loss and return the total cost**\n",
    "\n",
    "**hint** using numpy functions for vectorisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjKFxwASUm3o"
   },
   "outputs": [],
   "source": [
    "# task 1.4 implement loss function to calculate cross-entropy loss for all examples and average to return total cost\n",
    "# note the negative sign so that the loss decreases as our predictions get better\n",
    "def loss(y, y_pred):\n",
    "    epsilon = 1e-5\n",
    "    #### STUDENT'S CODE HERE - replace Nones - ####\n",
    "    # note, we must add a small penalty term (epsilon) to prevent calculation of log(0)\n",
    "    L = None\n",
    "    J = None \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ftVHqyS2Um3r"
   },
   "outputs": [],
   "source": [
    "total_loss= loss(y_train,y_pred)\n",
    "print(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbo7wTt_Um3u"
   },
   "source": [
    "## The Computation Graph\n",
    "\n",
    "Now we have our functions for $\\mathbf{L}$ and $\\mathbf{Z}$, and initialised $\\mathbf{W}$, we are finally in a position to compute a forward and backward pass. Computation graphs can help us to this by tracking the order of operations. The computation graph for logistic regression is:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1tWrFwh_lT_RVfYmodkvR_uRP8JOGfOu0\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "We can estimate the backwards pass using the chain rule:\n",
    "\n",
    "> > > > > >  <img src=\"https://drive.google.com/uc?id=14tKMEhXhlP2psxfdxrNWTt8M-LcfZL9m\" alt=\"Drawing\" style=\"width: 100px;\"/>\n",
    "\n",
    "Working backwards from the right side, this determines that to calculate the gradient of the loss with respect to the parameters we need;\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1R3qEtBPHZCwJ_e3uWwWoO5vx050vjGgg\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "And don't forget that the full cost equates to the mean of the loss over all examples $J=\\frac{1}{n_T}\\sum_i L_I$ , $\\dfrac{dJ}{dW}=\\frac{1}{n_T} \\sum_i \\dfrac{dL_i}{dW} $ . All calculations should be vectorised. \n",
    "\n",
    "### Task 1.5 Implement Forward Pass\n",
    "\n",
    "\n",
    "We now have all the components of the forward pass for our logistic regression. Write a full forward pass that takes data, targets and a weight matrix and performs the forward pass, with vectorisation calculating the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zAYzwvT7Um3v"
   },
   "outputs": [],
   "source": [
    "# task 1.5 implement the forwards pass to calculate F and then print loss and accuracy\n",
    "def forward_pass(X, y, W):\n",
    "    #### STUDENT'S CODE HERE - replace Nones to calculate F (loss and accruacy is printed for you) - ####\n",
    "    F = None\n",
    "    print('Loss: {}'.format(loss(y,y_pred)))\n",
    "    print('Accuracy: {}'.format(accuracy(y,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H8wYS0MMUm3x"
   },
   "outputs": [],
   "source": [
    "#perform forward pass\n",
    "forward_pass(X_train,y_train, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UieT9ii0Um3z"
   },
   "source": [
    "### Task 1.6 Implement backwards pass\n",
    "\n",
    "We're now ready to try and adjust our parameters $\\mathbf{W}$ in order to optimise our predictions. To do this we need to calculate the change in our loss function with respect to our parameters, $\\dfrac{\\partial L}{\\partial \\mathbf{W}}$. \n",
    "\n",
    "Recalling our staged calculation of the logistic regression (in vectorised form):\n",
    "\n",
    "$$\n",
    "\\mathbf{Z} = \\mathbf{W} \\mathbf{X} \\\\\n",
    "\\mathbf{F}= \\dfrac{1}{1+e^{- \\mathbf{Z}}} \\\\\n",
    "\\mathbf{L}  =  - \\mathbf{Y} \\ln(\\mathbf{F}) - (1-\\mathbf{Y}) \\ln(1-\\mathbf{F})\n",
    "$$\n",
    "\n",
    "We can write the vectorised gradients for each individual stage (see lecture slides and keats quiz): \n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial L}{\\partial f} = \\dfrac{\\mathbf{F} - \\mathbf{Y}}{\\mathbf{F}(1-\\mathbf{F})}\\\\\n",
    "\\dfrac{\\partial f}{\\partial z} = \\mathbf{F}(1-\\mathbf{F}) \\\\\n",
    "\\dfrac{\\partial z}{\\partial w} = \\mathbf{X}^T\n",
    "$$\n",
    "\n",
    "And compose through the chain rule:\n",
    "\n",
    "$$ \n",
    "\\dfrac{\\partial L}{\\partial w} = \\dfrac{\\partial L}{\\partial f} \\cdot \\dfrac{\\partial f}{\\partial z} \\cdot\\dfrac{\\partial z}{\\partial w} \\\\\n",
    "\\dfrac{\\partial L}{\\partial w} = \\dfrac{\\mathbf{F} - \\mathbf{Y}}{\\mathbf{F}(1-\\mathbf{F})} \\cdot \\mathbf{F}(1-\\mathbf{F}) \\cdot \\mathbf{X}^T\n",
    "$$\n",
    "\n",
    "Which can be simplified by cancelling $ \\mathbf{F}(1-\\mathbf{F})$ terms in both the numerator and the denominator: \n",
    "\n",
    "$$ \\dfrac{\\partial L}{\\partial w} = (\\mathbf{F} - \\mathbf{Y}) \\mathbf{X}^T $$\n",
    "\n",
    "Let's calculate the gradient of our loss, $\\dfrac{\\partial L}{\\partial \\mathbf{W}}$, for a **single** input, $\\mathbf{x}$. \n",
    "\n",
    "**To do** Fill in the calculations of the backward pass in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9TeNvOH1Um3z"
   },
   "outputs": [],
   "source": [
    "W = np.zeros((1,X_train.shape[0]))\n",
    "\n",
    "# select just the first example here\n",
    "x = X_train[:,0]\n",
    "y_single = y[0]\n",
    "print('The true value of y is: {}'.format(y_single))\n",
    "\n",
    "# calculate the forward pass, and store the outputs at each stage\n",
    "Z = z(W,x)\n",
    "F = f(Z)\n",
    "print('Our prediction for y is: {}'.format(F))\n",
    "\n",
    "l = loss(y_single,F)\n",
    "print('The loss is {}'.format(l))\n",
    "\n",
    "#### STUDENT'S CODE HERE - replace Nones - ####\n",
    "# now enter the backwards pass here, \n",
    "#implementing using the equations above:\n",
    "dl_dw = None\n",
    "\n",
    "print(dl_dw.shape,W.shape,x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lgLcN7B1Um31"
   },
   "source": [
    "**To do - run below code cells to check your implementation** \n",
    "\n",
    "We can check this gradient calculation is correct by updating our weights vector and looking at our new prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mI3PalmtUm31"
   },
   "outputs": [],
   "source": [
    "# The gradient is in the direction of increasing loss,\n",
    "# so we subtract the gradient from w.\n",
    "W = W - 0.001 * dl_dw\n",
    "Z = z(W,x)\n",
    "F = f(Z)\n",
    "l = loss(y_single,F)\n",
    "print('Our updated prediction for y is: {}'.format(F))\n",
    "print('The loss is {}'.format(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghCcWZz0Um33"
   },
   "source": [
    "Looks good! We have only updated $\\mathbf{w}$ using information from a single data point. In practice we want to use all the data available. The following implements the gradient calculation for all data points, using vectorisation. Make sure you understand what is happening here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sLNQKa0tUm34"
   },
   "outputs": [],
   "source": [
    "w = np.zeros((1,X_train.shape[0]))\n",
    "\n",
    "# calculate the forward pass, and store the outputs at each stage\n",
    "Z = z(w,X_train)\n",
    "F = f(Z)\n",
    "l = loss(y_train, F)\n",
    "\n",
    "# To do - implement the backward pass\n",
    "dL_dw = np.matmul((F-y_train),X_train.T) \n",
    "\n",
    "print('dL_dw has shape: {}'.format(dL_dw.shape))\n",
    "\n",
    "grad_mean = dL_dw/n_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymGZxeArUm38"
   },
   "source": [
    "### Task 1.7 -  Putting it all together: the training loop\n",
    "\n",
    "We now have everything we need to train a logistic regression classifier using backprop.\n",
    "\n",
    "**To do** Fill out the training loop below referencing the code you have already written (or been given) in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cRHEJ7jyUm39"
   },
   "outputs": [],
   "source": [
    "# initialise w to all zeros\n",
    "w = np.zeros((1,X_train.shape[0]))\n",
    "epsilon=1e-5\n",
    "# Normalise the data matrix (Ignoring the first row of ones which correspond to the bias term and is therefore all ones)\n",
    "X_norm = np.ones_like(X_train)\n",
    "X_norm[1:] = (X_train[1:] -X_train[1:].min(axis=1,keepdims=True) )/ (X_train[1:].max(axis=1,keepdims=True)-X_train[1:].min(axis=1,keepdims=True)+epsilon)\n",
    "# we'll store the loss and accuracy in these lists during training\n",
    "loss_record = []\n",
    "accuracy_record = []\n",
    "\n",
    "num_iterations = 5000\n",
    "learning_rate = 1e-4\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    # forward pass - get predictions\n",
    "    #### STUDENT CODE HERE -replace nones ####\n",
    "    # answer\n",
    "    Z = None\n",
    "    F = None\n",
    "    l = None\n",
    "    \n",
    "    # store the loss/ accuracy at this iteration\n",
    "    accuracy_it=accuracy(y_train,F)\n",
    "    loss_record.append(l)\n",
    "    accuracy_record.append(accuracy_it)\n",
    "    \n",
    "    #backwards pass to get gradients\n",
    "    #### STUDENT CODE HERE - replace nones #### \n",
    "    # answer:\n",
    "    dL_dw = None\n",
    "\n",
    "    grad_mean = None\n",
    "    \n",
    "    # update the \n",
    "    w = w - learning_rate * grad_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETOhNWPEUm3-"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize = (18,5))\n",
    "ax[0].plot(loss_record)\n",
    "ax[1].plot(accuracy_record)\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy');\n",
    "\n",
    "print(np.max(accuracy_record))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ciZE5Ps1Um4A"
   },
   "source": [
    "###  Task 1.8 Now Testing on left out set\n",
    "\n",
    "**To do** test the performance of your logistic regression on your left out test set by running below code cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kykn9UOcUm4A"
   },
   "outputs": [],
   "source": [
    "# centre X\n",
    "X_test_norm= np.ones_like(X_test)\n",
    "X_test_norm[1:] = (X_test[1:] -X_test[1:].mean(axis=1,keepdims=True)) / (X_test[1:].max(axis=1,keepdims=True) - X_test[1:].min(axis=1,keepdims=True) +epsilon)\n",
    "\n",
    "Z_test = z(w,X_test_norm)\n",
    "F_test = f(Z_test)\n",
    "l = loss(y_test,F_test)\n",
    "\n",
    "print(l,accuracy(y_test,F_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCivWhgHUm4C"
   },
   "source": [
    "## Exercise 2 - The multi-layer perceptron (MLP)\n",
    "\n",
    "We Now want to extend this model to create a single hidden layer neural network:\n",
    "\n",
    ">  > > >  <img src=\"https://drive.google.com/uc?id=1-6-7Md_WFhe728yyDMfMfkjXavvl8jj9\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "The forward pass through such a network may be written as\n",
    "\n",
    "$$ \\hat{y} = f_2 \\left( \\mathbf{W_2} f_1 \\left(\\mathbf{W_1}\\mathbf{X}\\right) \\right) $$\n",
    "\n",
    "where $f_2$ is a non-linear activation function for the hidden layer (we use ReLu), \n",
    "\n",
    "$$ \\text{Relu}(x) = \\text{max}(0,x)$$\n",
    "\n",
    "$f_1$ is  a non-linear activation function for the output layer (we use sigmoid for classification) and  $\\mathbf{W_1}$ and $\\mathbf{W_2}$ are the weights matrices for each layer. The generic shapes of each matrix are demonstrated in the figure \n",
    "\n",
    "**In this toy example we ask you to instead create a network with 5 hidden neurons** \n",
    "\n",
    "\n",
    "**Question** Given the shape of our input data, and the fact that we are still seeking the solution to a binary classification what are the number of input and output units for this problem (answer below)? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFblPZjYUm4C"
   },
   "source": [
    "**Students Answer here**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seymm6tzUm4C"
   },
   "source": [
    "We now go about implementing our simple network from scratch with gradient descent based optimisation\n",
    "\n",
    "### The forward pass\n",
    "\n",
    "Once again, we can write the forward pass as a staged computation:\n",
    "\n",
    "$$\n",
    "\\mathbf{Z}_1 = \\mathbf{W}_1 \\mathbf{X} \\\\\n",
    "\\mathbf{F}_1 = \\text{max}(0,\\mathbf{Z_1}) \\\\\n",
    "\\mathbf{Z}_2 = \\mathbf{W}_2 \\mathbf{F}_1 \\\\\n",
    "\\mathbf{F}_2 = \\dfrac{1}{1+e^{- \\mathbf{Z_2}}} \\\\\n",
    "\\mathbf{L}  =  - \\mathbf{Y} \\ln(\\mathbf{F_2}) - (1-\\mathbf{Y}) \\ln(1-\\mathbf{F_2})\n",
    "$$\n",
    "\n",
    "we give you the code for the ReLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FrrGMfaTUm4D"
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return x * (x>=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9dDY7_tUm4E"
   },
   "source": [
    "Let's implement the forward pass. \n",
    "\n",
    "\n",
    "### Task 2.1 Implement a forward pass of the MLP below: \n",
    "\n",
    "Use the vectorised expressions detailed above and ```np.random.randn``` to generate weights initialised with small random numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gxJKRMvBUm4F"
   },
   "outputs": [],
   "source": [
    "#### STUDENTS CODE HERE - replace nones ####\n",
    "# Answer\n",
    "W1 = None\n",
    "W2 = None\n",
    "Z1 = None\n",
    "F1 = None\n",
    "Z2 = None\n",
    "F2 = None # recall f is the sigmoid function\n",
    "l = loss(y_train,F2) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38DgGYSlUm4G"
   },
   "source": [
    "### The backwards pass\n",
    "\n",
    "The vectorised gradients of our MLP computation graph are, in reverse order, as follows:\n",
    "\n",
    "$$\\frac{\\delta L}{\\delta \\mathbf{F}_2}=\\frac{\\mathbf{F}_2-\\mathbf{Y}}{\\mathbf{F}_2(1-\\mathbf{F}_2)} \\\\\n",
    "\\frac{\\delta  \\mathbf{F}_2}{\\delta  \\mathbf{Z}_2}=\\mathbf{F}_2(1-\\mathbf{F}_2) \\\\\n",
    "\\frac{\\delta  \\mathbf{Z}_2}{\\delta  \\mathbf{W}_2}=\\mathbf{X} \\\\\n",
    "\\frac{\\delta  \\mathbf{Z}_2}{\\delta  \\mathbf{F}_1}=\\mathbf{W}^T_2\\\\\n",
    "\\frac{\\delta  \\mathbf{F}_1}{\\delta  \\mathbf{Z}_1}=1(\\mathbf{Z}_1 >0)\\\\\n",
    "\\frac{\\delta  \\mathbf{Z}_1}{\\delta  \\mathbf{W}_1}=\\mathbf{X}\\\\\n",
    "$$\n",
    "\n",
    "\n",
    "Combining these together using the chain rule we get (from lecture)\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1xG9N-o0UYP836Ehr4A6FrBuq4sgsYqqW\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "\n",
    "**Task** implement the backward pass of the MLP in numpy code, and copy in the forward pass from above.\n",
    "\n",
    "**Hint** carefully consider the order in which the stages are combined (covered in the lecture). Check the dimensions of the outputs are as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1Pqke5MUm4H"
   },
   "outputs": [],
   "source": [
    "epsilon = 1e-5\n",
    "\n",
    "# norm X\n",
    "X_norm = np.ones_like(X_train)\n",
    "X_norm[1:] = (X_train[1:] -X_train[1:].min(axis=1,keepdims=True) )/ (X_train[1:].max(axis=1,keepdims=True)-X_train[1:].min(axis=1,keepdims=True)+epsilon)\n",
    "\n",
    "# initialise w1, w2\n",
    "W1 = np.random.randn(5,X_train.shape[0])\n",
    "W2 = np.random.randn(1,5)\n",
    "\n",
    "# we'll store the loss and accuracy in these lists during training\n",
    "loss_record_mlp = []\n",
    "accuracy_record_mlp = []\n",
    "\n",
    "num_iterations = 2000\n",
    "learning_rate = 1e-2\n",
    "\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    # forward pass - get predictions\n",
    "    \n",
    "    #### STUDENT CODE HERE - replace Nones copying code from above ####\n",
    "    Z1 = None\n",
    "    F1 = None\n",
    "    Z2 = None\n",
    "    F2 = None  # recall f is the sigmoid function\n",
    "    l = loss(y_train,F2) \n",
    "\n",
    "    # store the loss/ accuracy at this iteration\n",
    "    loss_record_mlp.append(l)\n",
    "    accuracy_record_mlp.append(accuracy(y_train,F2))\n",
    "\n",
    "    \n",
    "    #backwards pass to get gradients\n",
    "    #### STUDENT CODE HERE - replace Nones  ####\n",
    "\n",
    "    dL_dW2=None\n",
    "    dL_df1=None \n",
    "    df2_dZ1  = 1.0 *(Z1> 0)\n",
    "    \n",
    "    dL_dZ1=None\n",
    "    dL_dW1 = None\n",
    "    dJ_dW2=(1/W2.shape[0])*dL_dW2 \n",
    "    dJ_dW1=(1/W1.shape[0])*dL_dW1 \n",
    "\n",
    "    # update the weights\n",
    "    W2 = W2 - learning_rate * dJ_dW2    \n",
    "    W1 = W1 - learning_rate * dJ_dW1\n",
    "    \n",
    "# plot loss and accuracy    \n",
    "fig, ax = plt.subplots(1,2, figsize = (18,5))\n",
    "ax[0].plot(loss_record_mlp)\n",
    "ax[1].plot(accuracy_record_mlp)\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy');\n",
    "\n",
    "print(accuracy(y_train,F2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEAXgVTyUm4I"
   },
   "source": [
    "### Testing the performance of the MLP\n",
    "\n",
    "**To do** test the performance of your logistic regression by running the code on your left out test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65itxO2xUm4J"
   },
   "outputs": [],
   "source": [
    "Z1_test = np.matmul(W1,X_test_norm)\n",
    "F1_test = relu(Z1_test)\n",
    "Z2_test = np.matmul(W2,F1_test)\n",
    "F2_test = f(Z2_test) \n",
    "l = loss(y_test,F2_test) \n",
    "\n",
    "print(l,accuracy(y_test,F2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bK8ZiUE6Um4R"
   },
   "source": [
    "## Homework\n",
    "\n",
    "1. Using multiclass data implement a softmax multi-class classifier as \n",
    "    a) a single neuron\n",
    "    b) an MLP\n",
    "2. Try using a tanh or leaky relu in place of the relu function in the MLP classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ACmMAgoAUm4R"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "1.1-fundamentals.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
