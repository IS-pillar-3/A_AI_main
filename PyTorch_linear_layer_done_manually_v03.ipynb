{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c313273",
   "metadata": {},
   "source": [
    "# Emulation of torch.nn.linear #\n",
    "\n",
    "This notebook executes a `torch.nn.linear` forward pass, repeats the same operation using matrix multiplication with a bias vector, **B**, separate from **X** and **W**, then again with the bias components integrated into **X** and **W**\n",
    "\n",
    "You can see the same results for each of the 3 approaches\n",
    "\n",
    "Thus we can directly relate **Z = W.X** from the theory sessions to **Z = X.**t(**W**), which is what PyTorch does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66800c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Torch - Z = X.t(W) + B\n",
    "#\n",
    "# Note that B is the vector of bias weights. The bias unit constants are implicitly 1\n",
    "#\n",
    "# It's not uncommon for the bias unit constants to be represented by a variable of all\n",
    "# 1's added to X, rather than a separate vector\n",
    "#\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "#\n",
    "# Define linear layer\n",
    "#\n",
    "linear_layer = nn.Linear(3, 6)\n",
    "#\n",
    "# Multiply X by 2 to distinguish from the bias, which is internal to Torch\n",
    "#\n",
    "X_torch = torch.ones(5, 3) * 2\n",
    "#\n",
    "# Forward pass\n",
    "#\n",
    "Z_torch = linear_layer(X_torch)\n",
    "#\n",
    "print(\"X:\")\n",
    "print(X_torch)\n",
    "print()\n",
    "#\n",
    "print(\"Bias:\")\n",
    "print(linear_layer.bias)\n",
    "print()\n",
    "#\n",
    "print(\"W:\")\n",
    "print(linear_layer.weight)\n",
    "print()\n",
    "#\n",
    "print(\"Z:\")\n",
    "print(Z_torch)\n",
    "print()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c638428",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Matrix multiplication - Z = X.t(W) + B\n",
    "#\n",
    "# Get data from Torch\n",
    "#\n",
    "X = X_torch.numpy()\n",
    "W = linear_layer.weight.detach().numpy()\n",
    "B = linear_layer.bias.detach().numpy()\n",
    "#\n",
    "print(\"X:\")\n",
    "print(X)\n",
    "print()\n",
    "#\n",
    "print(\"W:\")\n",
    "print(W)\n",
    "print()\n",
    "#\n",
    "print(\"B:\")\n",
    "print(B)\n",
    "print()\n",
    "#\n",
    "# Linear sum\n",
    "#\n",
    "Z = np.matmul(X, np.transpose(W)) + B\n",
    "#\n",
    "print(\"Z:\")\n",
    "print(Z)\n",
    "print()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975665c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Matrix multiplication - Z = X.t(W), where X contains the bias constants and W contains the bias weights\n",
    "#\n",
    "#\n",
    "X_with_bias = np.concatenate((np.ones((5, 1)), X), axis=1)\n",
    "W_with_bias = np.concatenate((B.reshape(6, 1), W), axis=1)\n",
    "Z_with_bias = np.matmul(X_with_bias, np.transpose(W_with_bias))\n",
    "#\n",
    "print(\"X_with_bias:\")\n",
    "print(X_with_bias)\n",
    "print()\n",
    "#\n",
    "print(\"W_with_bias\")\n",
    "print(W_with_bias)\n",
    "print()\n",
    "#\n",
    "print(\"Z_with_bias\")\n",
    "print(Z_with_bias)\n",
    "print()\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
