{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c313273",
   "metadata": {},
   "source": [
    "# Emulation of torch.nn.linear #\n",
    "\n",
    "This notebook executes a `torch.nn.linear` forward pass, repeats the same operation using matrix multiplication with a bias vector, **B**, separate from **X** and **W**, then again with the bias components integrated into **X** and **W**\n",
    "\n",
    "You can see the same results for each of the 3 approaches\n",
    "\n",
    "Thus we can directly relate **Z = W.X** from the theory sessions to **Z = X.**t(**W**), which is what PyTorch does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66800c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "\n",
      "Bias:\n",
      "Parameter containing:\n",
      "tensor([ 0.4651, -0.0523,  0.4757, -0.4737, -0.2291, -0.4987],\n",
      "       requires_grad=True)\n",
      "\n",
      "W:\n",
      "Parameter containing:\n",
      "tensor([[-0.1085,  0.0158, -0.3217],\n",
      "        [-0.1135, -0.2007, -0.2412],\n",
      "        [ 0.1758,  0.1456,  0.3941],\n",
      "        [ 0.3927,  0.4872,  0.4312],\n",
      "        [-0.3661, -0.3290, -0.5376],\n",
      "        [ 0.3550, -0.0855, -0.5272]], requires_grad=True)\n",
      "\n",
      "Z:\n",
      "tensor([[-0.3639, -1.1630,  1.9065,  2.1483, -2.6946, -1.0142],\n",
      "        [-0.3639, -1.1630,  1.9065,  2.1483, -2.6946, -1.0142],\n",
      "        [-0.3639, -1.1630,  1.9065,  2.1483, -2.6946, -1.0142],\n",
      "        [-0.3639, -1.1630,  1.9065,  2.1483, -2.6946, -1.0142],\n",
      "        [-0.3639, -1.1630,  1.9065,  2.1483, -2.6946, -1.0142]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Torch - Z = X.t(W) + B\n",
    "#\n",
    "# Note that B is the vector of bias weights. The bias unit constants are implicitly 1\n",
    "#\n",
    "# It's not uncommon for the bias unit constants to be represented by a variable of all\n",
    "# 1's added to X, rather than a separate vector\n",
    "#\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "#\n",
    "# Define linear layer\n",
    "#\n",
    "linear_layer = nn.Linear(3, 6)\n",
    "#\n",
    "# Multiply X by 2 to distinguish from the bias, which is internal to Torch\n",
    "#\n",
    "X_torch = torch.ones(5, 3) * 2\n",
    "#\n",
    "# Forward pass\n",
    "#\n",
    "Z_torch = linear_layer(X_torch)\n",
    "#\n",
    "print(\"X:\")\n",
    "print(X_torch)\n",
    "print()\n",
    "#\n",
    "print(\"Bias:\")\n",
    "print(linear_layer.bias)\n",
    "print()\n",
    "#\n",
    "print(\"W:\")\n",
    "print(linear_layer.weight)\n",
    "print()\n",
    "#\n",
    "print(\"Z:\")\n",
    "print(Z_torch)\n",
    "print()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c638428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "[[2. 2. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 2. 2.]]\n",
      "\n",
      "W:\n",
      "[[-0.10854128  0.01580048 -0.321738  ]\n",
      " [-0.1135062  -0.20068005 -0.24117315]\n",
      " [ 0.17577547  0.1455571   0.39408076]\n",
      " [ 0.39267427  0.48715985  0.43115246]\n",
      " [-0.36613792 -0.32896414 -0.53762877]\n",
      " [ 0.354953   -0.08546546 -0.5272183 ]]\n",
      "\n",
      "B:\n",
      "[ 0.46506703 -0.052306    0.47567463 -0.4736848  -0.2290915  -0.49872625]\n",
      "\n",
      "Z:\n",
      "[[-0.3638906 -1.1630247  1.9065013  2.1482882 -2.6945531 -1.0141878]\n",
      " [-0.3638906 -1.1630247  1.9065013  2.1482882 -2.6945531 -1.0141878]\n",
      " [-0.3638906 -1.1630247  1.9065013  2.1482882 -2.6945531 -1.0141878]\n",
      " [-0.3638906 -1.1630247  1.9065013  2.1482882 -2.6945531 -1.0141878]\n",
      " [-0.3638906 -1.1630247  1.9065013  2.1482882 -2.6945531 -1.0141878]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Matrix multiplication - Z = X.t(W) + B\n",
    "#\n",
    "# Get data from Torch\n",
    "#\n",
    "X = X_torch.numpy()\n",
    "W = linear_layer.weight.detach().numpy()\n",
    "B = linear_layer.bias.detach().numpy()\n",
    "#\n",
    "print(\"X:\")\n",
    "print(X)\n",
    "print()\n",
    "#\n",
    "print(\"W:\")\n",
    "print(W)\n",
    "print()\n",
    "#\n",
    "print(\"B:\")\n",
    "print(B)\n",
    "print()\n",
    "#\n",
    "# Linear sum\n",
    "#\n",
    "Z = np.matmul(X, np.transpose(W)) + B\n",
    "#\n",
    "print(\"Z:\")\n",
    "print(Z)\n",
    "print()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcfd61e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_with_bias:\n",
      "[[1. 2. 2. 2.]\n",
      " [1. 2. 2. 2.]\n",
      " [1. 2. 2. 2.]\n",
      " [1. 2. 2. 2.]\n",
      " [1. 2. 2. 2.]]\n",
      "\n",
      "W_with_bias\n",
      "[[ 0.46506703 -0.10854128  0.01580048 -0.321738  ]\n",
      " [-0.052306   -0.1135062  -0.20068005 -0.24117315]\n",
      " [ 0.47567463  0.17577547  0.1455571   0.39408076]\n",
      " [-0.4736848   0.39267427  0.48715985  0.43115246]\n",
      " [-0.2290915  -0.36613792 -0.32896414 -0.53762877]\n",
      " [-0.49872625  0.354953   -0.08546546 -0.5272183 ]]\n",
      "\n",
      "Z_with_bias\n",
      "[[-0.36389059 -1.16302478  1.90650129  2.14828837 -2.69455317 -1.01418775]\n",
      " [-0.36389059 -1.16302478  1.90650129  2.14828837 -2.69455317 -1.01418775]\n",
      " [-0.36389059 -1.16302478  1.90650129  2.14828837 -2.69455317 -1.01418775]\n",
      " [-0.36389059 -1.16302478  1.90650129  2.14828837 -2.69455317 -1.01418775]\n",
      " [-0.36389059 -1.16302478  1.90650129  2.14828837 -2.69455317 -1.01418775]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Matrix multiplication - Z = X.t(W), where X contains the bias constants and W contains the bias weights\n",
    "#\n",
    "#\n",
    "X_with_bias = np.concatenate((np.ones((5, 1)), X), axis=1)\n",
    "W_with_bias = np.concatenate((B.reshape(6, 1), W), axis=1)\n",
    "Z_with_bias = np.matmul(X_with_bias, np.transpose(W_with_bias))\n",
    "#\n",
    "print(\"X_with_bias:\")\n",
    "print(X_with_bias)\n",
    "print()\n",
    "#\n",
    "print(\"W_with_bias\")\n",
    "print(W_with_bias)\n",
    "print()\n",
    "#\n",
    "print(\"Z_with_bias\")\n",
    "print(Z_with_bias)\n",
    "print()\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
