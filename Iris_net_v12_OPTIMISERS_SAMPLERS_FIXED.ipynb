{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "809ed905",
   "metadata": {},
   "source": [
    "## Optimisers example using Iris dataset ##\n",
    "\n",
    "Use this script to experiment with different optimizer settings using the well known Iris dataset\n",
    "\n",
    "Run the first code cell to do all the set up. Then you can try various optimisers (and other settings) in the code cell after that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601d7833",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# GET DATA & LIBRARIES\n",
    "#\n",
    "# Get dataset\n",
    "#\n",
    "!wget -nv https://github.com/IS-pillar-3/datasets/raw/main/iris.csv\n",
    "#\n",
    "# Libraries\n",
    "#\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "#\n",
    "# GPU\n",
    "#\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#\n",
    "# Make seed quite random\n",
    "#\n",
    "random.seed(datetime.now().microsecond)\n",
    "#\n",
    "# DATA SET CLASS\n",
    "#\n",
    "class IrisDataset(torch.utils.data.Dataset):\n",
    "    #\n",
    "    def __init__(self, data_path):\n",
    "        #\n",
    "        # Get the data, shuffle it and encode the classes\n",
    "        #\n",
    "        self.iris_df = pd.read_csv(data_path)\n",
    "        self.iris_df = self.iris_df.sample(frac=1).reset_index(drop=True)\n",
    "        #\n",
    "        le = LabelEncoder()\n",
    "        self.iris_df[\"class\"] = le.fit_transform(self.iris_df[\"class\"])\n",
    "    #\n",
    "    #\n",
    "    def __len__(self):\n",
    "        #\n",
    "        # Rows in dataframe\n",
    "        #\n",
    "        return self.iris_df.shape[0]\n",
    "    #\n",
    "    #\n",
    "    def __getitem__(self, idx):\n",
    "        #\n",
    "        # Return row\n",
    "        #\n",
    "        x = np.array(self.iris_df.iloc[idx, 0:4], dtype=np.float32)\n",
    "        y = np.array(self.iris_df.iloc[idx, 4])\n",
    "        #\n",
    "        return torch.from_numpy(x), torch.from_numpy(y)\n",
    "        #\n",
    "    #\n",
    "#\n",
    "#\n",
    "# Test cases\n",
    "#\n",
    "iris_dset = IrisDataset(\"./iris.csv\")     \n",
    "#\n",
    "print(iris_dset.__len__())\n",
    "#\n",
    "x, y = iris_dset.__getitem__(0)\n",
    "print(x, y)\n",
    "#\n",
    "#\n",
    "x, y = iris_dset.__getitem__(12)\n",
    "print(x, y)\n",
    "#\n",
    "# DATA LOADERS\n",
    "#\n",
    "# Creating data loaders for training, test and validation split\n",
    "#\n",
    "# Thanks to: https://stackoverflow.com/questions/50544730/\n",
    "#            how-do-i-split-a-custom-dataset-into-training-and-test-datasets\n",
    "#\n",
    "# vali split is portion of train after split from test\n",
    "#\n",
    "test_split = 0.2\n",
    "vali_split = 0.2\n",
    "#\n",
    "no_rows = len(iris_dset)\n",
    "idxs    = list(range(no_rows))\n",
    "#\n",
    "# Shuffle and then split indices\n",
    "#\n",
    "np.random.shuffle(idxs)\n",
    "#\n",
    "test_size  = int(np.floor(test_split * no_rows))\n",
    "train_size = no_rows - test_size\n",
    "temp_rows  = idxs[0:train_size]\n",
    "test_rows  = idxs[train_size:]\n",
    "#\n",
    "vali_size  = int(np.floor(vali_split * train_size))\n",
    "train_size = train_size - vali_size\n",
    "train_rows = temp_rows[0:train_size]\n",
    "vali_rows  = temp_rows[train_size:]\n",
    "#\n",
    "print(train_size, test_size, vali_size)\n",
    "#\n",
    "# Set up samplers and loaders\n",
    "#\n",
    "train_sampler     = torch.utils.data.SubsetRandomSampler(train_rows)\n",
    "iris_train_loader = torch.utils.data.DataLoader(iris_dset, batch_size=10, sampler=train_sampler)\n",
    "#\n",
    "batch_nos = []\n",
    "for batch_number, (x, y) in enumerate(iris_train_loader):\n",
    "        batch_nos.append(batch_number)\n",
    "#\n",
    "print(\"Train batches\")\n",
    "print(batch_nos)\n",
    "#\n",
    "if test_size > 0:\n",
    "    test_sampler     = torch.utils.data.SubsetRandomSampler(test_rows)\n",
    "    iris_test_loader = torch.utils.data.DataLoader(iris_dset, batch_size=test_size, sampler=test_sampler)\n",
    "    #\n",
    "    batch_nos = []\n",
    "    for batch_number, (x_test, y_test) in enumerate(iris_test_loader):\n",
    "        batch_nos.append(batch_number)\n",
    "    #\n",
    "    print(\"Test batches\")\n",
    "    print(batch_nos)\n",
    "#\n",
    "if vali_size > 0:\n",
    "    vali_sampler      = torch.utils.data.SubsetRandomSampler(vali_rows)\n",
    "    iris_vali_loader  = torch.utils.data.DataLoader(iris_dset, batch_size=vali_size, sampler=vali_sampler)\n",
    "    #\n",
    "    batch_nos = []\n",
    "    for batch_number, (x_vali, y_vali) in enumerate(iris_vali_loader):\n",
    "        batch_nos.append(batch_number)\n",
    "    #\n",
    "    print(\"Vali batches\")\n",
    "    print(batch_nos)\n",
    "#\n",
    "# MODEL\n",
    "#\n",
    "# Class that defines the model structure\n",
    "#\n",
    "class Model(nn.Module):\n",
    "    #\n",
    "    # Class defining network structure\n",
    "    #\n",
    "    def __init__(self):\n",
    "        #\n",
    "        # Start parent class\n",
    "        #\n",
    "        super(Model, self).__init__()\n",
    "        #\n",
    "        # This network is (input=4, ReLU=5, Softmax=5)\n",
    "        #\n",
    "        # Here we have the layers that have learned parameters\n",
    "        #\n",
    "        # NB: default for linear layer is bias=True\n",
    "        #\n",
    "        self.fc1 = nn.Linear(4, 5)\n",
    "        self.fc2 = nn.Linear(5, 3)\n",
    "        #\n",
    "    #\n",
    "    #\n",
    "    def forward(self, x):\n",
    "        #\n",
    "        # Here we have the sequence of operations. \n",
    "        #\n",
    "        # Note that Softmax is handled by cross entropy loss\n",
    "        #\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        #\n",
    "        return x\n",
    "    #\n",
    "#\n",
    "# Instantiate the model\n",
    "#\n",
    "model = Model()\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d8f9e6",
   "metadata": {},
   "source": [
    "### Model training ###\n",
    "\n",
    "Vary the optimizer (and other parameters) and see how this affects the training and validation plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95654b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Train the model\n",
    "#\n",
    "# Define optimizer and loss functions\n",
    "#\n",
    "#\n",
    "# Define SGD optimizer\n",
    "#\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, nesterov=True)\n",
    "#\n",
    "# RMSprop optimizer\n",
    "#\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.005, alpha=0.99, eps=1e-08, weight_decay=0,\n",
    "                                momentum=0.9, centered=False)\n",
    "#\n",
    "# Define Adam optimizer\n",
    "#\n",
    "ooptimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.999), eps=1e-08, weight_decay=0, lr=0.005)\n",
    "#\n",
    "# Define loss function\n",
    "#\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "#\n",
    "# Function to get validation loss  \n",
    "#\n",
    "def validate(model, x_vali, y_vali):\n",
    "    #\n",
    "    # x_vali, y_vali = iris_loader.get_vali()\n",
    "    #\n",
    "    model.eval()\n",
    "    #\n",
    "    output = model(x_vali)  # only forward pass - NO gradients!!\n",
    "    #\n",
    "    loss = loss_function(output, y_vali)\n",
    "    #\n",
    "    return loss.data.item()\n",
    "#\n",
    "#\n",
    "# Train function\n",
    "#\n",
    "def train(model, iris_train_loader, epoch, verbose, report_batches):\n",
    "    #\n",
    "    # Train one epoch\n",
    "    #\n",
    "    model.train()\n",
    "    #\n",
    "    losses = []\n",
    "    #\n",
    "    for batch_number, (x, y) in enumerate(iris_train_loader):\n",
    "        #\n",
    "        # Copy data to GPU if available\n",
    "        #\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        #\n",
    "        optimizer.zero_grad() \n",
    "        #\n",
    "        output = model(x)\n",
    "        #\n",
    "        loss = loss_function(output, y)\n",
    "        #\n",
    "        loss.backward()\n",
    "        #\n",
    "        optimizer.step()\n",
    "        #\n",
    "        if batch_number % report_batches == 0:\n",
    "            if verbose:\n",
    "                print(\"Epoch\", epoch, \", batch_number\", batch_number, \", loss.data.item()\",\n",
    "                      loss.data.item())\n",
    "            #\n",
    "            losses.append(loss.data.item())\n",
    "        #\n",
    "    #\n",
    "    return losses\n",
    "#\n",
    "# Run the epochs\n",
    "#\n",
    "epoch_losses = []\n",
    "vali_losses  = []\n",
    "#\n",
    "for epoch in range(500):\n",
    "    epoch_losses.append(train(model, iris_train_loader, epoch, False, 10)[-1])\n",
    "    #\n",
    "    vali_losses.append(validate(model, x_vali, y_vali))\n",
    "#\n",
    "# Plot loss functions\n",
    "#\n",
    "plt.plot(epoch_losses, color=\"red\")\n",
    "plt.plot(vali_losses, color=\"blue\")\n",
    "plt.xlabel(\"Epoch number\")\n",
    "plt.ylabel(\"Loss\")\n",
    "#\n",
    "red_patch  = mpatches.Patch(color='red',  label=\"train\")\n",
    "blue_patch = mpatches.Patch(color='blue', label=\"vali\")\n",
    "plt.legend(handles=[red_patch, blue_patch], loc=\"upper left\")\n",
    "#\n",
    "plt.show()\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
