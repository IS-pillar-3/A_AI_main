{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29bJmnhYOpc3"
   },
   "source": [
    "# Optimisers and Loss Functions\n",
    "\n",
    "Tutorial by Mark Graham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisers\n",
    "\n",
    "### Exercise 1.1 Gradient descent\n",
    "\n",
    "In this section we'll dig into optimisers in more detail. So far, we have considered simple gradient descent. In gradient descent, the update rule is :\n",
    "\n",
    "$$ \\mathbf{W}_{t+1} = \\mathbf{W}_{t}  - \\eta \\left.\\dfrac{\\partial J}{\\partial \\mathbf{W}}\\right|_{w_{t}} $$\n",
    "\n",
    "where $J$ is our cost function, $\\mathbf{W}$ our parameters and $\\eta$ our learning rate.\n",
    "\n",
    "Let's implement gradient descent. \n",
    "Consider the following cost function for a simple two-parameter system:\n",
    "\n",
    "$$ J = 40w_1^2 + 10 w_2^2 + 40 w_2$$\n",
    "\n",
    "*Remember that the **cost** function refers to the mean loss over the entire training set and the **loss** function gives the cost of an individual observation*\n",
    "\n",
    "**1.1.1 Implement the loss function for the above expression in, the cell below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LpUgOUdUOpc-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#\n",
    "# Loss function\n",
    "#\n",
    "def loss(w1, w2):\n",
    "    #\n",
    "    # ### STUDENT'S ANSWER HERE ####\n",
    "    #\n",
    "    return None\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cy82Fm7COpc_"
   },
   "source": [
    "Because our cost function is simple, we can calculate its minimum analytically.\n",
    "\n",
    "**1.1.2 Estimate the analytic mimina of a convex function**\n",
    "\n",
    "Calculate the partial derivatives analytically $\\dfrac{\\partial J}{\\partial w_1}$ and $\\dfrac{\\partial J}{\\partial w_2}$ so as to complete function below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uX1xC2WcOpc_"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Gradient of loss function\n",
    "#\n",
    "def calculate_gradient(w1, w2):\n",
    "    #\n",
    "    # ### STUDENT'S ANSWER HERE ####\n",
    "    # \n",
    "    grad_w1 = None\n",
    "    grad_w2 = None\n",
    "    #\n",
    "    return (grad_w1, grad_w2)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEV3SbXCOpc_"
   },
   "source": [
    "#### 1.1.3. Estimate the coordinates of the minima\n",
    "\n",
    "Using your knowledge of differention, and the partial derivatives estimated above, calculate the minima of the cost function $ J_{min}$, together with the parameters at which the loss is minimised: $ w_{1min}, w_{2min} $. \n",
    "\n",
    "Pass these values into  `plot_surface` function below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3lH0ZX9AOpc_"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Generate data for a convex function, calculate minima and plot\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "#\n",
    "# Generate evenly spaced values\n",
    "#\n",
    "w1 = np.linspace(-5, 5, 30)\n",
    "w2 = np.linspace(-5, 5, 30)\n",
    "#\n",
    "W1, W2 = np.meshgrid(w1, w2)\n",
    "#\n",
    "# Compute value of loss function for each parameter combination\n",
    "#\n",
    "losses = loss(W1, W2)\n",
    "#\n",
    "# ### STUDENT'S ANSWER HERE ####\n",
    "#\n",
    "w_1min = None\n",
    "w_2min = None\n",
    "L_min  = None\n",
    "#\n",
    "# Do plot\n",
    "#\n",
    "def plot_surface(W1, W2, loss_coords, minima):\n",
    "    #\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax  = plt.axes(projection='3d')\n",
    "    #\n",
    "    ax.contour3D(W1, W2, losses, 50, cmap='binary')\n",
    "    ax.set_xlabel('w1')\n",
    "    ax.set_ylabel('w2')\n",
    "    ax.set_zlabel('loss');\n",
    "    #\n",
    "    # plot the minimum\n",
    "    #\n",
    "    ax.scatter3D(minima[0], minima[1], minima[2], loss_coords, c='green', s=100)\n",
    "    #\n",
    "    return fig, ax\n",
    "#\n",
    "fig, ax = plot_surface(W1, W2, losses, [w_1min, w_2min, L_min])\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUlyXgkIOpdB"
   },
   "source": [
    "Finally, we need a way of updating our parameters so we can use of the gradient. \n",
    "\n",
    "**1.1.4 Implement the gradient descent update rule, in the class outlined in the next cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qNUUCHO1OpdB"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Class for carrying out gradient descent\n",
    "#\n",
    "class gradient_descent():\n",
    "    #\n",
    "    def __init__(self, learning_rate):\n",
    "        #\n",
    "        # We can store parameters of the descent algorithm here.\n",
    "        #\n",
    "        self.lr = learning_rate\n",
    "    #\n",
    "    def __call__(self, w1, w2, w1_grad, w2_grad):\n",
    "        #\n",
    "        # Our actual computation happens here.\n",
    "        #\n",
    "        # ### STUDENT'S ANSWER HERE ####\n",
    "        #\n",
    "        # Update the weights\n",
    "        #\n",
    "        w1_updated = None\n",
    "        w2_updated = None\n",
    "        #\n",
    "        return w1_updated, w2_updated\n",
    "    #\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aem_2CfcOpdB"
   },
   "source": [
    "We're now ready to perform gradient descent\n",
    "\n",
    "**Run the following cell, which calls your functions** `loss_gradient` **and** `gradient_descent` **:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hj2ir6kYOpdC"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Plot the path of gradient descent\n",
    "#\n",
    "def plot_coords(w1_coords, w2_coords, loss_coords, axis):\n",
    "    #\n",
    "    ax.plot3D(w1_coords, w2_coords, loss_coords, c='red')\n",
    "    ax.scatter3D(w1_coords, w2_coords, loss_coords, c='red', s=100)\n",
    "    ax.set_xlim(-5, 5)\n",
    "    ax.set_ylim(-5, 5);\n",
    "    ax.set_zlim(-100, 1500)\n",
    "#\n",
    "# Start point\n",
    "#\n",
    "w1 = 4 * -1\n",
    "w2 = 4\n",
    "#\n",
    "num_epochs = 100\n",
    "lr         = 0.01\n",
    "#\n",
    "# Instantiate the gradient_descent optimiser\n",
    "#\n",
    "grad_descent = gradient_descent(learning_rate=lr)\n",
    "#\n",
    "# Create empty lists to hold the coordinates\n",
    "#\n",
    "w1_coords, w2_coords, loss_coords = [], [], []\n",
    "#\n",
    "# Do epochs\n",
    "#\n",
    "for epoch in range(num_epochs):\n",
    "    #\n",
    "    w1_coords.append(w1)\n",
    "    w2_coords.append(w2)\n",
    "    #\n",
    "    Loss = loss(w1,w2)\n",
    "    #\n",
    "    loss_coords.append(Loss)\n",
    "    #\n",
    "    w1_grad, w2_grad =  calculate_gradient(w1, w2)\n",
    "    #\n",
    "    w1, w2 = grad_descent(w1, w2, w1_grad, w2_grad)\n",
    "    #\n",
    "    if (abs(w1_grad) > 0.01) or (abs(w2_grad) > 0.01):\n",
    "        print('epoch {} , loss {:.3f}, w1 gradient: {:.3f} w2 gradient: {:.3f}'.format(epoch,Loss,w1_grad, w2_grad))\n",
    "    else:\n",
    "        break\n",
    "    #\n",
    "#\n",
    "# Do plot\n",
    "#\n",
    "fig, ax = plot_surface(W1, W2, losses, [w_1min, w_2min, L_min])\n",
    "plot_coords(w1_coords, w2_coords, loss_coords, ax)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obvAsZ7XOpdC"
   },
   "source": [
    "It should work pretty well. Lets investigate some things:\n",
    "- How sensitive is our solution to the learning rate? Try out different values in the above cell, both larger and smaller. What do you notice?\n",
    "- Estimate how long it takes for out solution to converge, by altering the number of epochs at a fixed lr=0.003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Gradient descent with momentum ###\n",
    "\n",
    "Momentum adds 'memory' to our gradient descent, averaging the gradient at the current step with gradients from previous steps. The update equations gradient descent with momentum are:\n",
    "\n",
    "$$ \\mathbf{z}_{t+1} = \\beta   \\mathbf{z}_{t} +  \\eta \\left.\\dfrac{\\partial J}{\\partial \\mathbf{W}}\\right|_{\\mathbf{W}_{t}} \\\\\n",
    "\\mathbf{W}_{t+1} = \\mathbf{W}_{t}  -  \\mathbf{z}_{t+1} $$\n",
    "\n",
    "where $\\beta$ is the momentum parameter and $\\eta$ the learning rate. \n",
    "\n",
    "**1.2.1 Implement the momentum update  in the cell below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "arto2GygOpdC"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Class for carrying out gradient descent with momentum\n",
    "#\n",
    "class gradient_descent_momentum():\n",
    "    #\n",
    "    def __init__(self, momentum, learning_rate):\n",
    "        #\n",
    "        # As well as the parameters momentum and lr, we need to store z_1 and z_2 between update steps\n",
    "        #\n",
    "        self.momentum = momentum\n",
    "        self.lr       = learning_rate\n",
    "        #\n",
    "        self.z_1 = 0\n",
    "        self.z_2 = 0\n",
    "    #    \n",
    "    def __call__(self, w1, w2, w1_grad, w2_grad):\n",
    "        #\n",
    "        ### STUDENT CODE HERE####\n",
    "        #\n",
    "        # Using the above formula implement the momentum update\n",
    "        #\n",
    "        self.z_1 = None\n",
    "        self.z_2 = None\n",
    "        #\n",
    "        # Update the weights \n",
    "        #\n",
    "        w1_updated = None\n",
    "        w2_updated = None\n",
    "        #\n",
    "        return w1_updated, w2_updated \n",
    "    #\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2RBu2alOpdD"
   },
   "source": [
    "The cell below calls the momentum update. **Run it**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p13JS3UOOpdD"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Run the gradient descent with momentum\n",
    "#\n",
    "w1 = 4 * -1\n",
    "w2 = 4\n",
    "#\n",
    "num_epochs = 100\n",
    "lr         = 0.003\n",
    "momentum   = 0.6\n",
    "#\n",
    "gradient_descent_mom = gradient_descent_momentum(momentum=momentum, learning_rate=lr)\n",
    "#\n",
    "w1_coords, w2_coords, loss_coords = [], [], []\n",
    "#\n",
    "# Do epochs\n",
    "#\n",
    "for epoch in range(num_epochs):\n",
    "    #\n",
    "    w1_coords.append(w1)\n",
    "    w2_coords.append(w2)\n",
    "    #\n",
    "    Loss = loss(w1,w2)\n",
    "    #\n",
    "    loss_coords.append(Loss)\n",
    "    #\n",
    "    w1_grad, w2_grad = calculate_gradient(w1, w2)\n",
    "    #\n",
    "    w1, w2 = gradient_descent_mom(w1,w2, w1_grad, w2_grad)\n",
    "    #\n",
    "    if (abs(w1_grad) > 0.01) or (abs(w2_grad) > 0.01):\n",
    "        print('epoch {} , loss {:.3f}, w1 gradient: {:.3f} w2 gradient: {:.3f}'.format(epoch, Loss, w1_grad, w2_grad))\n",
    "    else:\n",
    "        break\n",
    "    #\n",
    "#\n",
    "# Do plot\n",
    "#\n",
    "fig, ax = plot_surface(W1, W2, losses, [w_1min, w_2min, L_min])\n",
    "plot_coords(w1_coords, w2_coords, loss_coords, ax)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VlQWqbA-OpdE"
   },
   "source": [
    "Now play with the momentum parameters:\n",
    "1. What do you notice about the convergence speed of momentum compared to gradient descent? \n",
    "2. Vary the learning rate. What do you notice about higher learning rates, compared to gradient descent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81GZHppqOpdE"
   },
   "source": [
    "### Exercise 1.3: RMSProp\n",
    "\n",
    "RMSProp also keeps a 'memory', but here it uses this memory to moderate the learning rate for each parameter independently, so that smaller steps are taken in directions with larger gradients. The update equations are:\n",
    "\n",
    "$$ \\mathbf{v}_{t+1} = \\beta   \\mathbf{v}_{t} +  (1-\\beta) \\left( \\left.\\dfrac{\\partial J}{\\partial \\mathbf{W}}\\right|_{w_{t}}\\right)^2 \\\\\n",
    "\\mathbf{W}_{t+1} = \\mathbf{W}_{t}  - \\dfrac{\\eta}{\\sqrt{\\mathbf{v}_{t+1} + \\epsilon}} \\circ  \\left.\\dfrac{\\partial J}{\\partial \\mathbf{W}}\\right|_{w_{t}} $$\n",
    "\n",
    "The update looks complicated, but compare with the gradient descent update. They're the same, except the learning rate $\\eta$ is divided by a scalar that is calculated at each update step. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3.1. Implement the RMSProp update step in the cell below**\n",
    "\n",
    "As we have only 2 parameters, implement the updates for each one separately by:\n",
    "\n",
    "- estimating `self.v1` and `self.v2` correspending to $\\mathbf{v}_{t+1}$ for each parameter. This implements an exponential average of the square of the gradient with respect to each parameter\n",
    "- estimate `lr_1` and `lr_2` the learning rate correction ($\\dfrac{\\eta}{\\sqrt{\\mathbf{v}_{t+1} + \\epsilon}}$) for each parameter \n",
    "- update weights for each parameter (`w1_updated`, `w2_updated`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_9Xw_NVcOpdE"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Class for RMSprop optimizer\n",
    "#\n",
    "class RMSProp():\n",
    "    #\n",
    "    def __init__(self, momentum, learning_rate):\n",
    "        #\n",
    "        self.momentum = momentum\n",
    "        self.lr       = learning_rate\n",
    "        self.v_1      = 0\n",
    "        self.v_2      = 0\n",
    "        self.epsilon  = 1e-5\n",
    "    #   \n",
    "    def __call__(self, w1, w2, w1_grad, w2_grad):\n",
    "        #\n",
    "        # ## STUDENT CODE HERE ####\n",
    "        #\n",
    "        # Estimate self.v1 and self.v2 as moving averages of the square of the gradient\n",
    "        #\n",
    "        self.v_1 = None\n",
    "        self.v_2 = None\n",
    "        #\n",
    "        # Implement the learning rate update for each parameter\n",
    "        #\n",
    "        lr_1 = None\n",
    "        lr_2 = None\n",
    "        #\n",
    "        # Implement the parameter update\n",
    "        #\n",
    "        w1_updated = None\n",
    "        w2_updated = None\n",
    "        #\n",
    "        return w1_updated, w2_updated    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1aF_dOPOpdE"
   },
   "source": [
    "Let's use RMSProp - **run the cell below**, which calls your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBS9QLp5OpdF"
   },
   "outputs": [],
   "source": [
    "# \n",
    "# Run the RMSprop gradient descent\n",
    "#\n",
    "w1 = 4 * -1\n",
    "w2 = 4\n",
    "\n",
    "num_epochs = 30\n",
    "lr         = 0.5\n",
    "momentum   = 0.9\n",
    "#\n",
    "rmsprop = RMSProp(momentum=momentum, learning_rate=lr)\n",
    "#\n",
    "w1_coords, w2_coords, loss_coords = [], [], []\n",
    "#\n",
    "# Do epochs\n",
    "#\n",
    "for epoch in range(num_epochs):\n",
    "    #\n",
    "    w1_coords.append(w1)\n",
    "    w2_coords.append(w2)\n",
    "    #\n",
    "    Loss = loss(w1, w2)\n",
    "    #\n",
    "    loss_coords.append(Loss)\n",
    "    #\n",
    "    w1_grad, w2_grad =  calculate_gradient(w1 ,w2)\n",
    "    #\n",
    "    w1, w2 = rmsprop(w1, w2, w1_grad, w2_grad)\n",
    "    #\n",
    "    if (abs(w1_grad) > 0.01) or (abs(w2_grad) > 0.01):\n",
    "        print('epoch {} , loss {:.3f}, w1 gradient: {:.3f} w2 gradient: {:.3f}'.format(epoch, Loss, w1_grad, w2_grad))\n",
    "    else:\n",
    "        break\n",
    "    #\n",
    "#\n",
    "# Do plot\n",
    "#\n",
    "fig, ax = plot_surface(W1, W2, losses, [w_1min, w_2min, L_min])\n",
    "plot_coords(w1_coords, w2_coords, loss_coords, ax)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bejIanFOpdF"
   },
   "source": [
    "Play with the learning rate and momentum. What do you notice about the learning rate needed compared to previous update rules? What about the speed of convergence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtOZ5I8FOpdF"
   },
   "source": [
    "### Stochastic gradient descent\n",
    "\n",
    "In order to investigate optimisers here, we've been analysing a simple, quadratic loss function with parameters that are known to us. This made it straightforward to calculate both the loss and its gradient for any set of parameters, $w_1,w_2$. However, in a practical ML application we won't have this nice functional form for the loss. For a simple two parameter regression problem, the cost function would take the form:\n",
    "\n",
    "$$ J = \\frac{1}{N} \\sum_{i=1}^{N } (y_i - x_{1i}w_1 - x_{2i}w_2)^2$$\n",
    "\n",
    "We can see the cost will still be quadratic in our two parameters $w_1,w_2$ but will depend on our $N$ training data points $\\{\\mathbf{x_i}, y_i\\}$. At each iteration we will need to run our model over the full dataset to calculate the cost and gradient\n",
    "\n",
    "If $N$ is very large, or we have a large model with lots of parameters (e.g. a neural network) it can be time-consuming and memory-intensive to run through the full dataset to get the gradients for our next parameters update. In practice, we calculate the cost and gradients for a randomly chosen subset of the data at each iteration, approximating the cost and the gradients at that point. This has the effect of giving us noisy gradient updates - we don't necessarily take the optimal step at each iteration, but sometimes this can help us avoid or jump out of local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Stochastic gradient descent for real data\n",
    "\n",
    "Let's repeat our MLP training loop from session 1, this time using SGD\n",
    "\n",
    "*This network consists of a hidden ReLU layer, and an output layer containing 1 sigmoid unit*\n",
    "\n",
    "First let's create custom Dataset and Dataloader classes for our brain image data. Here, any preprocessing to be run on the whole data set should be run *only once*, and thus should go in the `__init__` function. \n",
    "\n",
    "**1.4.1 Complete the `__len__` and `__getitem__` methods** \n",
    "Check that the class returns the number of items and feature bvector lengths that you expect\n",
    "\n",
    "**Don't forget to upload the data to colab and edit the path to match where you load it to**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WtnMrj-NOpdG"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Dataset class for brain scan data\n",
    "#\n",
    "import io\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "#\n",
    "class PretermDataset(Dataset):\n",
    "    # \n",
    "    def __init__(self): \n",
    "        #\n",
    "        # Get the data\n",
    "        #\n",
    "        url      = \"https://raw.githubusercontent.com/IS-pillar-3/datasets/main/prem_vs_termwrois.csv\" \n",
    "        download = requests.get(url).content\n",
    "        df       = pd.read_csv(io.StringIO(download.decode(\"utf-8\")))\n",
    "        #\n",
    "        data = df.values[:, :-2].T\n",
    "        y    = df.values[: , -1]\n",
    "        #\n",
    "        # Bias terms\n",
    "        # \n",
    "        X = np.concatenate((np.ones((1, data.shape[1])), data))\n",
    "        #\n",
    "        # Normalise to ~ N(0, 1)\n",
    "        #\n",
    "        epsilon       = 1e-5\n",
    "        X_centred     = np.ones_like(X)\n",
    "        X_centred[1:] = (X[1:] - X[1:].mean(axis=1, keepdims=True)) / (X[1:].std(axis=1, keepdims=True) + epsilon)\n",
    "        #               \n",
    "        self.X = X_centred\n",
    "        self.y = y\n",
    "    #\n",
    "    def __len__(self):\n",
    "        #\n",
    "        # ### STUDENTS COMPLETE ###\n",
    "        #\n",
    "        return None\n",
    "    #\n",
    "    def __getitem__(self, idx):\n",
    "        #\n",
    "        # ### STUDENTS COMPLETE ### \n",
    "        #\n",
    "        # Return a sample from your data set as a tuple \n",
    "        #\n",
    "        sample = None\n",
    "        #\n",
    "        return sample\n",
    "#\n",
    "#\n",
    "dataset = PretermDataset()\n",
    "print('Number of entries in dataset: {}'.format(len(dataset)))\n",
    "#\n",
    "x, y = dataset[5]\n",
    "print('Shape of one item in x: {}'.format(x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRqoWMVjOpdG"
   },
   "source": [
    "**1.4.2 Create a DataLoader class to iterate through your DataSet class**\n",
    "\n",
    "*How must we specify the* `DataLoader` *object in order to ensure random (i.e. Stochastic) selection of training observations?*\n",
    "\n",
    "Initially set `batch_size` to equal the total number of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hIbpGavyOpdG"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Create a DataLoader\n",
    "#\n",
    "# ### STUDENTS TO COMPLETE ###\n",
    "#\n",
    "# Propose a batch size and instantiate at default PyTorch dataloader for this dataset\n",
    "#\n",
    "batch_size = None  \n",
    "dataloader = None  \n",
    "#\n",
    "# Instantiate optimiser\n",
    "#\n",
    "learning_rate = 0.01\n",
    "optimiser     = gradient_descent_momentum(0.9, learning_rate)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DViDffIDOpdH"
   },
   "source": [
    "**1.4.3 Implement the training loop**\n",
    "\n",
    "Create the training loop\n",
    "\n",
    "*This network consists of a hidden ReLU layer, and an output layer containing 1 sigmoid unit*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ucUpzbmAOpdH",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Train the model\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "#\n",
    "# ReLU activation\n",
    "#\n",
    "def relu(x):\n",
    "    return x * (x >= 0)\n",
    "#\n",
    "# Sigmoidal axtivation\n",
    "#\n",
    "def f(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "#\n",
    "# Calculate cost\n",
    "#\n",
    "def cost(y, y_pred):\n",
    "    #\n",
    "    epsilon = 1e-5\n",
    "    #\n",
    "    # Add epsilon to avoid log(0)\n",
    "    #\n",
    "    L = - y * np.log(y_pred + epsilon) - (1 - y) * np.log(1 - y_pred + epsilon) \n",
    "    #\n",
    "    J = torch.mean(L)\n",
    "    #\n",
    "    return J\n",
    "#\n",
    "# Perfomance metrics\n",
    "#\n",
    "def accuracy(y, y_pred, threshold = 0.5):\n",
    "    #\n",
    "    y_pred_thresholded  = y_pred > threshold\n",
    "    correct_predictions = torch.sum(y == y_pred_thresholded)  \n",
    "    total_predictions   = len(y)\n",
    "    #\n",
    "    accuracy = 100 * correct_predictions / total_predictions\n",
    "    #\n",
    "    return accuracy\n",
    "#\n",
    "# Initialise weights\n",
    "#\n",
    "W1 = torch.randn(5, 302)\n",
    "W2 = torch.randn(1, 5)\n",
    "#\n",
    "cost_record_mlp     = []\n",
    "accuracy_record_mlp = []\n",
    "#\n",
    "num_epochs    = 40\n",
    "learning_rate = 1\n",
    "#\n",
    "# Training loop\n",
    "#\n",
    "for i in range(num_epochs):\n",
    "    # STUDENTS CODE - implement training loop\n",
    "    # as before implement the forwards and backwards pass with update\n",
    "    # but this time use a dataloader to iterate\n",
    "    #\n",
    "    for batch_number, (data, labels) in enumerate(dataloader, 0):\n",
    "        #\n",
    "        # ### STUDENTS CODE ###\n",
    "        #\n",
    "        # FORWARD PASS\n",
    "        #\n",
    "        # Multiply input with hidden layer weights and apply ReLU\n",
    "        #\n",
    "        #Z1 = None\n",
    "        #F1 = None\n",
    "        Z1 = torch.matmul(W1, data.T)\n",
    "        F1 = relu(Z1)\n",
    "        #\n",
    "        # Multiply hidden layer weights with output from hidden layer and apply sigmoid\n",
    "        #\n",
    "        Z2 = torch.matmul(W2, F1)\n",
    "        F2 = f(Z2)  \n",
    "        #Z2 = None\n",
    "        #F2 = None\n",
    "        #\n",
    "        # Get the cost and accuracy and store in designated lists\n",
    "        #\n",
    "        #cost_val = None\n",
    "        #cost_record_mlp.append(None)\n",
    "        #accuracy_record_mlp.append(None)\n",
    "        #\n",
    "        cost_val = cost(labels, F2)\n",
    "        #\n",
    "        cost_record_mlp.append(cost_val)\n",
    "        accuracy_record_mlp.append(accuracy(labels, F2))\n",
    "        #\n",
    "        # BACKWARD PASS\n",
    "        #\n",
    "        # Hint: use the chain rule to get del_J / del_W1 and del_J / del_W2 \n",
    "        #\n",
    "        # Output layer deltas\n",
    "        #\n",
    "        dL_dW2 = torch.matmul(F2 - labels, F1.T) \n",
    "        dJ_dW2 = (1 / W2.shape[0]) * dL_dW2\n",
    "        #dL_dW2 = None\n",
    "        #dL_dW2 = None\n",
    "        #\n",
    "        # Hidden layer deltas\n",
    "        #\n",
    "        dL_df1  = torch.matmul((F2 - labels).T, W2)  \n",
    "        df1_dZ1 = 1.0 * (Z1 > 0)\n",
    "        dL_dZ1  = torch.multiply(dL_df1.T, df1_dZ1)\n",
    "        dL_dW1  = torch.matmul(dL_dZ1, data)\n",
    "        dJ_dW1  = (1/ W1.shape[0]) * dL_dW1 \n",
    "        #dL_df1  = None\n",
    "        #df1_dZ1 = None\n",
    "        #dL_dZ1  = None\n",
    "        #dL_dW1  = None\n",
    "        #dJ_dW1  = None\n",
    "        #\n",
    "        # Update the weights\n",
    "        #\n",
    "        W1, W2=optimiser(W1, W2, dJ_dW1, dJ_dW2)\n",
    "      #\n",
    "    #\n",
    "#\n",
    "# Do plots\n",
    "#\n",
    "print('Training with a batch size of {}'.format(batch_size))\n",
    "#\n",
    "fig, ax = plt.subplots(1, 2, figsize=(18, 5))\n",
    "#\n",
    "ax[0].plot(cost_record_mlp)\n",
    "ax[1].plot(accuracy_record_mlp)\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Cost')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy');\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5R8grYYkUZUe"
   },
   "source": [
    "**1.4.4 Investigate the influence of batch size**\n",
    "\n",
    "For a batch size of 101 we have (non-stochastic) gradient descent, since the entire training set has 101 entries. Try\n",
    "reducing the batch size to implement **stochastic** gradient descent - what do you notice about the training curves?\n",
    "\n",
    "Try implementing the different optimiser functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flfgsFjuOpdH"
   },
   "source": [
    "# Loss functions\n",
    "\n",
    "### Exercise 5: Generalised dice overlap ###\n",
    "\n",
    "The GDL can be used for multiclass segmentation - the full paper is [here](https://arxiv.org/pdf/1707.03237.pdf)\n",
    "\n",
    "Let's implement it.\n",
    "\n",
    "First let's load in the data: a 2D slice from a T1 image, segmented into 7 classes. We have both a ground truth label and a label predicted by a partially trained neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJEdiRf9OpdH"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Get and load data\n",
    "#\n",
    "!wget -nv https://github.com/IS-pillar-3/datasets/raw/main/images.npz\n",
    "#\n",
    "file  = np.load(\"images.npz\")\n",
    "#\n",
    "image        = file[\"arr_0\"]\n",
    "ground_truth = file[\"arr_1\"]\n",
    "pred         = file[\"arr_2\"]\n",
    "#\n",
    "# Label numerical equivalents\n",
    "#\n",
    "labels = {0: \"Background\",\n",
    "          1: \"CSF\",\n",
    "          2: \"Basal Ganglia\",\n",
    "          3: \"Cortex\",\n",
    "          4: \"Brainstem\",\n",
    "          5: \"Cerebellum\",\n",
    "          6: \"White matter\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0uqPh9vOpdH"
   },
   "source": [
    "Run the following cell to plot the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-NZh2fIOpdH"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Plot data\n",
    "#\n",
    "import seaborn as sns\n",
    "from matplotlib import colors\n",
    "from matplotlib.lines import Line2D\n",
    "#\n",
    "rgb_values   = sns.color_palette(\"Set3\", 7)\n",
    "cmap         = colors.ListedColormap(rgb_values, N=7)\n",
    "custom_lines = [Line2D([0], [0], color=rgb_values[i], lw=4) for i in range(7)]\n",
    "#\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "#\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(image,cmap='gray'); plt.axis('off'); plt.title('Image')\n",
    "#\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(ground_truth,cmap=cmap, vmin=0, vmax=6); plt.axis('off');plt.title('Ground Truth');\n",
    "#\n",
    "plt.subplot(1, 3, 3)\n",
    "#\n",
    "plt.imshow(pred,cmap=cmap, vmin=0, vmax=6)\n",
    "plt.axis('off')\n",
    "plt.title('Prediction');\n",
    "plt.legend(custom_lines, labels.values(),loc='best', fontsize='medium');\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64rM434-OpdI"
   },
   "source": [
    "The GDL can be expressed as \n",
    "\n",
    "$$\n",
    "\\mathrm{GDL}=1-2 \\frac{\\sum_{l=1}^{K} w_{l} \\sum_{n} y_{l n} \\hat{y}_{l n}}{\\sum_{l=1}^{K} w_{l} \\sum_{n}( y_{l n}+\\hat{y}_{l n})}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $y$ is the true segmentation map\n",
    "- $\\hat{y}$ the predicted class label\n",
    "- $w_l$ is a weight for each class (out of $K$ in total)\n",
    "- $l$ index the class\n",
    "- $n$ indexes each pixel in the image \n",
    "\n",
    "The class weight is estimated from:\n",
    "$$1 /\\left(\\sum_{n=1}^{N} y_{l n}\\right)^{2}$$ \n",
    "\n",
    "Which gives higher weight to classes with fewer examples.\n",
    "\n",
    "The first stage is to one-hot encode the segmentation maps, transforming them from a $WxH$ array to a $CxWxH$ array where each channel contains a binary segmentation mask for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.5.1 Implement a function to one hot encode the segmentation maps**\n",
    "\n",
    "**hint** you will need to loop over all classes and, for each class $i$,  create a binary segmentation (of dimensions equal to the original image) with values 1 (where voxel belongs to class $i$) and 0 (where it does not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jAJdFfPsOpdI"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# One hot encode the segmentation maps\n",
    "#\n",
    "def one_hot_encode(mask, num_classes):\n",
    "    #\n",
    "    # Initialise an empty mask for one hot encoding, with shape (num_classes, image_width,image_depth\n",
    "    #\n",
    "    mask_encoded = np.zeros((num_classes,mask.shape[0], mask.shape[1]))\n",
    "    #\n",
    "    # ### STUDENT'S CODE HERE ###\n",
    "    #\n",
    "    # Code a loop to fill mask_encoded - for each class we expect 1's only in the location of that region\n",
    "    # \n",
    "    return mask_encoded\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbDUNLdbOpdI"
   },
   "source": [
    "Check the encoding makes sense. Run the following cell to one-hot encode and plot each class seperately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R0FXWdgjOpdI"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# One-hot encode each class\n",
    "#\n",
    "ground_truth_encoded = one_hot_encode(ground_truth, num_classes=7)\n",
    "prediction_encoded   = one_hot_encode(pred, num_classes=7)\n",
    "#\n",
    "plt.figure(figsize=(20, 5))\n",
    "for i in range(7):\n",
    "    plt.subplot(2, 7, i + 1)\n",
    "    plt.imshow(ground_truth_encoded[i, :, :]); plt.axis('off')\n",
    "    plt.title(labels[i])\n",
    "    plt.subplot(2, 7, i + 8)\n",
    "    plt.imshow(prediction_encoded[i, :, :])\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amziYmKCOpdI"
   },
   "source": [
    "### 1.5.2.  Implement the GDL: \n",
    "\n",
    "The function can be implemented without looping over all voxels, provided you make use of numpy vectorisation, complete the below function to estimate:\n",
    "\n",
    "a) the numerator $\\sum_{l=1}^{2} w_{l} \\sum_{n} y_{l n} \\hat{y}_{l n}$\n",
    "\n",
    "b) the denominator $\\sum_{l=1}^{2} w_{l} \\sum_{n}( y_{l n}+\\hat{y}_{l n})$\n",
    "\n",
    "c) the complete GDL \n",
    "\n",
    "We suggest a correction for division by zero by setting `weight=epsilon` in these circumstances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qIgSPwOhOpdJ"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Function to apply Generalised Dice Overlap\n",
    "#\n",
    "def gdl(truth, prediction):\n",
    "    #\n",
    "    num_classes = truth.shape[0]\n",
    "    numerator   = 0\n",
    "    denominator = 0\n",
    "    #\n",
    "    # ### STUDENT CODE HERE ###\n",
    "    #\n",
    "    return 1 -  2 * np.divide(numerator, denominator, where = denominator != 0) \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21SWIXFtOpdJ"
   },
   "source": [
    "Let's get the loss value for our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJivsWCHOpdJ"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Get loss value for example\n",
    "#\n",
    "gdl(ground_truth_encoded, prediction_encoded)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yVCC9CYOpdJ"
   },
   "source": [
    "And sanity check: do we get a loss of 0 when our ground truth and prediction exactly match?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IL3Xkj6OOpdJ",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Check for loss of 0 when exact match\n",
    "#\n",
    "gdl(ground_truth_encoded, ground_truth_encoded)\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "3_1_Loss_functions_and_optimisers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
